[
  {
    "case_id": "T3-J1-L1-0081",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Acceptance rate. \nBut when the metric is broken down by Applicant preparedness level (high/low), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
    "claim": "A city or institution releases a headline metric claiming Group A outperforms Group B on Acceptance rate",
    "variables": {
      "X": {
        "name": "Group (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Acceptance rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Applicant preparedness level (high/low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was Applicant preparedness level (high/low) determined before Group (A vs. B) was chosen, and could Applicant preparedness level (high/low) have influenced the choice of Group (A vs. B) before Acceptance rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Acceptance rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Applicant preparedness level (high/low): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Acceptance rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.",
      "C": "Answer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Acceptance rate by the key strata (e.g., Applicant preparedness level (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata.",
    "causal_structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
    "key_insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-81",
    "_original_title": "Scholarship Acceptance Rates by Major",
    "_questions": "Does the higher overall Acceptance rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
    "_expected_analysis": "Identify Simpson’s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Acceptance rate in every subgroup.\nConclusion: The “Group A is better” interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required."
  },
  {
    "case_id": "T3-J1-L1-0082",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Completion rate. \nBut when the metric is broken down by Student schedule flexibility (high/low), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
    "claim": "A city or institution releases a headline metric claiming Group A outperforms Group B on Completion rate",
    "variables": {
      "X": {
        "name": "Group (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Completion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Student schedule flexibility (high/low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was Student schedule flexibility (high/low) determined before Group (A vs. B) was chosen, and could Student schedule flexibility (high/low) have influenced the choice of Group (A vs. B) before Completion rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Completion rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Student schedule flexibility (high/low): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Completion rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.",
      "C": "Answer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Completion rate by the key strata (e.g., Student schedule flexibility (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata.",
    "causal_structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
    "key_insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-82",
    "_original_title": "Library Program Participation",
    "_questions": "Does the higher overall Completion rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
    "_expected_analysis": "Identify Simpson’s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Completion rate in every subgroup.\nConclusion: The “Group A is better” interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required."
  },
  {
    "case_id": "T3-J1-L1-0083",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Recycling compliance rate. \nBut when the metric is broken down by Housing type (single-family/apartment), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
    "claim": "A city or institution releases a headline metric claiming Group A outperforms Group B on Recycling compliance rate",
    "variables": {
      "X": {
        "name": "Group (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Recycling compliance rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Housing type (single-family/apartment)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was Housing type (single-family/apartment) determined before Group (A vs. B) was chosen, and could Housing type (single-family/apartment) have influenced the choice of Group (A vs. B) before Recycling compliance rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Recycling compliance rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Housing type (single-family/apartment): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Recycling compliance rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.",
      "C": "Answer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Recycling compliance rate by the key strata (e.g., Housing type (single-family/apartment) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata.",
    "causal_structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
    "key_insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-83",
    "_original_title": "Neighborhood Recycling Compliance",
    "_questions": "Does the higher overall Recycling compliance rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
    "_expected_analysis": "Identify Simpson’s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Recycling compliance rate in every subgroup.\nConclusion: The “Group A is better” interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required."
  },
  {
    "case_id": "T3-J1-L1-0084",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "An organization reports a very positive statistic for Reported donation intent based only on observations from a subset of people.\nThe subset is formed by Survey response that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
    "claim": "An organization reports a very positive statistic for Reported donation intent based only on observations from a subset of people",
    "variables": {
      "X": {
        "name": "Survey response",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported donation intent",
        "role": "Outcome"
      },
      "Z": {
        "name": "Underlying true outcome (positive/negative)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "At what point were units selected into the observed sample—before or after Reported donation intent occurred—and is selection related to Underlying true outcome (positive/negative) or Reported donation intent?",
    "conditional_answers": {
      "A": "Answer if Survey response is randomly assigned: A difference in Reported donation intent across Survey response groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Survey response vs not-Survey response difference in Reported donation intent is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Survey response–Reported donation intent differences may reflect selection rather than effect.",
    "causal_structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
    "key_insight": "Observed outcomes reflect who is observed, not the full population.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-84",
    "_original_title": "The Alumni Donation Email Survey",
    "_questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
    "_expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Reported donation intent statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L1-0085",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "An organization reports a very positive statistic for Average star rating based only on observations from a subset of people.\nThe subset is formed by Who leaves reviews that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
    "claim": "An organization reports a very positive statistic for Average star rating based only on observations from a subset of people",
    "variables": {
      "X": {
        "name": "Who leaves reviews",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Average star rating",
        "role": "Outcome"
      },
      "Z": {
        "name": "Underlying true outcome (positive/negative)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "At what point were units selected into the observed sample—before or after Average star rating occurred—and is selection related to Underlying true outcome (positive/negative) or Average star rating?",
    "conditional_answers": {
      "A": "Answer if Who leaves reviews is randomly assigned: A difference in Average star rating across Who leaves reviews groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Who leaves reviews vs not-Who leaves reviews difference in Average star rating is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Who leaves reviews–Average star rating differences may reflect selection rather than effect.",
    "causal_structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
    "key_insight": "Observed outcomes reflect who is observed, not the full population.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-85",
    "_original_title": "The 'Top Reviewer' Restaurant Ratings",
    "_questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
    "_expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Average star rating statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L1-0086",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Psychology",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "An organization reports a very positive statistic for Reported improvement based only on observations from a subset of people.\nThe subset is formed by Who shares outcomes that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
    "claim": "An organization reports a very positive statistic for Reported improvement based only on observations from a subset of people",
    "variables": {
      "X": {
        "name": "Who shares outcomes",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported improvement",
        "role": "Outcome"
      },
      "Z": {
        "name": "Underlying true outcome (positive/negative)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "At what point were units selected into the observed sample—before or after Reported improvement occurred—and is selection related to Underlying true outcome (positive/negative) or Reported improvement?",
    "conditional_answers": {
      "A": "Answer if Who shares outcomes is randomly assigned: A difference in Reported improvement across Who shares outcomes groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Who shares outcomes vs not-Who shares outcomes difference in Reported improvement is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Who shares outcomes–Reported improvement differences may reflect selection rather than effect.",
    "causal_structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
    "key_insight": "Observed outcomes reflect who is observed, not the full population.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-86",
    "_original_title": "The Parenting App Success Stories",
    "_questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
    "_expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Reported improvement statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L1-0087",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "A report finds that regions/organizations with higher County average broadband access also have higher Student grades on average.\nA commentator concludes that any individual inside a region with higher County average broadband access will have higher Student grades.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
    "claim": "any individual inside a region with higher County average broadband access will have higher Student grades",
    "variables": {
      "X": {
        "name": "County average broadband access (group-level)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Student grades (individual-level)",
        "role": "Outcome"
      },
      "Z": {
        "name": "Individual-level characteristics",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is County average broadband access (group-level) measured at an aggregate level while Student grades (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Student grades (individual-level)?",
    "conditional_answers": {
      "A": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.",
      "B": "Answer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.",
      "C": "Answer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how County average broadband access (group-level) relates to Student grades (individual-level) for a person.",
    "causal_structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
    "key_insight": "You cannot infer individual behavior reliably from group-level correlations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-87",
    "_original_title": "County-Level Broadband and Grades",
    "_questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
    "_expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID."
  },
  {
    "case_id": "T3-J1-L1-0088",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "A report finds that regions/organizations with higher State unemployment rate also have higher Individual criminal behavior on average.\nA commentator concludes that any individual inside a region with higher State unemployment rate will have higher Individual criminal behavior.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
    "claim": "any individual inside a region with higher State unemployment rate will have higher Individual criminal behavior",
    "variables": {
      "X": {
        "name": "State unemployment rate (group-level)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Individual criminal behavior (individual-level)",
        "role": "Outcome"
      },
      "Z": {
        "name": "Individual-level characteristics",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is State unemployment rate (group-level) measured at an aggregate level while Individual criminal behavior (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Individual criminal behavior (individual-level)?",
    "conditional_answers": {
      "A": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.",
      "B": "Answer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.",
      "C": "Answer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how State unemployment rate (group-level) relates to Individual criminal behavior (individual-level) for a person.",
    "causal_structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
    "key_insight": "You cannot infer individual behavior reliably from group-level correlations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-88",
    "_original_title": "State-Level Unemployment and Crime",
    "_questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
    "_expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID."
  },
  {
    "case_id": "T3-J1-L1-0089",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "A report finds that regions/organizations with higher Company diversity percentage also have higher Individual satisfaction on average.\nA commentator concludes that any individual inside a region with higher Company diversity percentage will have higher Individual satisfaction.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
    "claim": "any individual inside a region with higher Company diversity percentage will have higher Individual satisfaction",
    "variables": {
      "X": {
        "name": "Company diversity percentage (group-level)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Individual satisfaction (individual-level)",
        "role": "Outcome"
      },
      "Z": {
        "name": "Individual-level characteristics",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is Company diversity percentage (group-level) measured at an aggregate level while Individual satisfaction (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Individual satisfaction (individual-level)?",
    "conditional_answers": {
      "A": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.",
      "B": "Answer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.",
      "C": "Answer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Company diversity percentage (group-level) relates to Individual satisfaction (individual-level) for a person.",
    "causal_structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
    "key_insight": "You cannot infer individual behavior reliably from group-level correlations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-89",
    "_original_title": "Company-Level Diversity and Satisfaction",
    "_questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
    "_expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0090",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Program A vs Program B) to improve test pass rate.\nA pilot dataset reports that, overall, intervention A has a lower test pass rate than intervention B.\nBut when the pilot results are stratified by baseline preparedness (high/low), intervention A has a higher test pass rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Program A vs Program B) to improve test pass rate",
    "variables": {
      "X": {
        "name": "Intervention choice (Program A vs Program B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "test pass rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline preparedness (high/low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline preparedness (high/low) determined before Intervention choice (Program A vs Program B) was chosen, and could baseline preparedness (high/low) have influenced the choice of Intervention choice (Program A vs Program B) before test pass rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Program A vs Program B) on test pass rate may be reversed because the mix of subgroups differs between Intervention choice (Program A vs Program B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by baseline preparedness (high/low): Use the within-stratum differences (or a standardized effect). If Intervention choice (Program A vs Program B) improves test pass rate in each stratum, prefer Intervention choice (Program A vs Program B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Program A vs Program B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report test pass rate by the key strata (e.g., baseline preparedness (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Program A vs Program B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-90",
    "_original_title": "Choosing Between Two Tutoring Programs",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize test pass rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves test pass rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0091",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Platform A vs Platform B) to improve resolution rate within 7 days.\nA pilot dataset reports that, overall, intervention A has a lower resolution rate within 7 days than intervention B.\nBut when the pilot results are stratified by case complexity (simple/complex), intervention A has a higher resolution rate within 7 days in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Platform A vs Platform B) to improve resolution rate within 7 days",
    "variables": {
      "X": {
        "name": "Intervention choice (Platform A vs Platform B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "resolution rate within 7 days",
        "role": "Outcome"
      },
      "Z": {
        "name": "case complexity (simple/complex)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was case complexity (simple/complex) determined before Intervention choice (Platform A vs Platform B) was chosen, and could case complexity (simple/complex) have influenced the choice of Intervention choice (Platform A vs Platform B) before resolution rate within 7 days was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Platform A vs Platform B) on resolution rate within 7 days may be reversed because the mix of subgroups differs between Intervention choice (Platform A vs Platform B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by case complexity (simple/complex): Use the within-stratum differences (or a standardized effect). If Intervention choice (Platform A vs Platform B) improves resolution rate within 7 days in each stratum, prefer Intervention choice (Platform A vs Platform B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Platform A vs Platform B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report resolution rate within 7 days by the key strata (e.g., case complexity (simple/complex) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Platform A vs Platform B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-91",
    "_original_title": "Comparing Two Telehealth Platforms",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize resolution rate within 7 days? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves resolution rate within 7 days in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0092",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Message A vs Message B) to improve purchase conversion rate.\nA pilot dataset reports that, overall, intervention A has a lower purchase conversion rate than intervention B.\nBut when the pilot results are stratified by customer segment (new/returning), intervention A has a higher purchase conversion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Message A vs Message B) to improve purchase conversion rate",
    "variables": {
      "X": {
        "name": "Intervention choice (Message A vs Message B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "purchase conversion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "customer segment (new/returning)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was customer segment (new/returning) determined before Intervention choice (Message A vs Message B) was chosen, and could customer segment (new/returning) have influenced the choice of Intervention choice (Message A vs Message B) before purchase conversion rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Message A vs Message B) on purchase conversion rate may be reversed because the mix of subgroups differs between Intervention choice (Message A vs Message B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by customer segment (new/returning): Use the within-stratum differences (or a standardized effect). If Intervention choice (Message A vs Message B) improves purchase conversion rate in each stratum, prefer Intervention choice (Message A vs Message B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Message A vs Message B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report purchase conversion rate by the key strata (e.g., customer segment (new/returning) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Message A vs Message B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-92",
    "_original_title": "Evaluating Two Marketing Messages",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize purchase conversion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves purchase conversion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0093",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Policy A vs Policy B) to improve promotion rate.\nA pilot dataset reports that, overall, intervention A has a lower promotion rate than intervention B.\nBut when the pilot results are stratified by department (sales/engineering), intervention A has a higher promotion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Policy A vs Policy B) to improve promotion rate",
    "variables": {
      "X": {
        "name": "Intervention choice (Policy A vs Policy B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "promotion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "department (sales/engineering)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was department (sales/engineering) determined before Intervention choice (Policy A vs Policy B) was chosen, and could department (sales/engineering) have influenced the choice of Intervention choice (Policy A vs Policy B) before promotion rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Policy A vs Policy B) on promotion rate may be reversed because the mix of subgroups differs between Intervention choice (Policy A vs Policy B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by department (sales/engineering): Use the within-stratum differences (or a standardized effect). If Intervention choice (Policy A vs Policy B) improves promotion rate in each stratum, prefer Intervention choice (Policy A vs Policy B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Policy A vs Policy B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report promotion rate by the key strata (e.g., department (sales/engineering) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Policy A vs Policy B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-93",
    "_original_title": "Promotion Policy Across Departments",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize promotion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves promotion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0094",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Route A vs Route B) to improve on-time delivery rate.\nA pilot dataset reports that, overall, intervention A has a lower on-time delivery rate than intervention B.\nBut when the pilot results are stratified by traffic day type (normal/event), intervention A has a higher on-time delivery rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Route A vs Route B) to improve on-time delivery rate",
    "variables": {
      "X": {
        "name": "Intervention choice (Route A vs Route B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "on-time delivery rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "traffic day type (normal/event)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was traffic day type (normal/event) determined before Intervention choice (Route A vs Route B) was chosen, and could traffic day type (normal/event) have influenced the choice of Intervention choice (Route A vs Route B) before on-time delivery rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Route A vs Route B) on on-time delivery rate may be reversed because the mix of subgroups differs between Intervention choice (Route A vs Route B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by traffic day type (normal/event): Use the within-stratum differences (or a standardized effect). If Intervention choice (Route A vs Route B) improves on-time delivery rate in each stratum, prefer Intervention choice (Route A vs Route B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Route A vs Route B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report on-time delivery rate by the key strata (e.g., traffic day type (normal/event) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Route A vs Route B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-94",
    "_original_title": "Route Choice for Delivery Drivers",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize on-time delivery rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves on-time delivery rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0095",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Format A vs Format B) to improve course completion rate.\nA pilot dataset reports that, overall, intervention A has a lower course completion rate than intervention B.\nBut when the pilot results are stratified by student work hours (low/high), intervention A has a higher course completion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Format A vs Format B) to improve course completion rate",
    "variables": {
      "X": {
        "name": "Intervention choice (Format A vs Format B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "course completion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "student work hours (low/high)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was student work hours (low/high) determined before Intervention choice (Format A vs Format B) was chosen, and could student work hours (low/high) have influenced the choice of Intervention choice (Format A vs Format B) before course completion rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Format A vs Format B) on course completion rate may be reversed because the mix of subgroups differs between Intervention choice (Format A vs Format B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by student work hours (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Format A vs Format B) improves course completion rate in each stratum, prefer Intervention choice (Format A vs Format B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Format A vs Format B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report course completion rate by the key strata (e.g., student work hours (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Format A vs Format B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-95",
    "_original_title": "Online Course Format Change",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize course completion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves course completion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0096",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Outreach A vs Outreach B) to improve vaccination uptake.\nA pilot dataset reports that, overall, intervention A has a lower vaccination uptake than intervention B.\nBut when the pilot results are stratified by neighborhood access level (low/high), intervention A has a higher vaccination uptake in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Outreach A vs Outreach B) to improve vaccination uptake",
    "variables": {
      "X": {
        "name": "Intervention choice (Outreach A vs Outreach B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "vaccination uptake",
        "role": "Outcome"
      },
      "Z": {
        "name": "neighborhood access level (low/high)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was neighborhood access level (low/high) determined before Intervention choice (Outreach A vs Outreach B) was chosen, and could neighborhood access level (low/high) have influenced the choice of Intervention choice (Outreach A vs Outreach B) before vaccination uptake was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Outreach A vs Outreach B) on vaccination uptake may be reversed because the mix of subgroups differs between Intervention choice (Outreach A vs Outreach B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by neighborhood access level (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Outreach A vs Outreach B) improves vaccination uptake in each stratum, prefer Intervention choice (Outreach A vs Outreach B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Outreach A vs Outreach B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report vaccination uptake by the key strata (e.g., neighborhood access level (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Outreach A vs Outreach B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-96",
    "_original_title": "Vaccination Outreach Strategy",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize vaccination uptake? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves vaccination uptake in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0097",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Tool A vs Tool B) to improve offer rate.\nA pilot dataset reports that, overall, intervention A has a lower offer rate than intervention B.\nBut when the pilot results are stratified by applicant experience (junior/senior), intervention A has a higher offer rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Tool A vs Tool B) to improve offer rate",
    "variables": {
      "X": {
        "name": "Intervention choice (Tool A vs Tool B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "offer rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "applicant experience (junior/senior)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was applicant experience (junior/senior) determined before Intervention choice (Tool A vs Tool B) was chosen, and could applicant experience (junior/senior) have influenced the choice of Intervention choice (Tool A vs Tool B) before offer rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Tool A vs Tool B) on offer rate may be reversed because the mix of subgroups differs between Intervention choice (Tool A vs Tool B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by applicant experience (junior/senior): Use the within-stratum differences (or a standardized effect). If Intervention choice (Tool A vs Tool B) improves offer rate in each stratum, prefer Intervention choice (Tool A vs Tool B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Tool A vs Tool B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report offer rate by the key strata (e.g., applicant experience (junior/senior) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Tool A vs Tool B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-97",
    "_original_title": "Job Interview Screen Tool",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize offer rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves offer rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0098",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A manager must choose between two interventions (Rule A vs Rule B) to improve satisfaction score.\nA pilot dataset reports that, overall, intervention A has a lower satisfaction score than intervention B.\nBut when the pilot results are stratified by ticket severity (low/high), intervention A has a higher satisfaction score in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "claim": "A manager must choose between two interventions (Rule A vs Rule B) to improve satisfaction score",
    "variables": {
      "X": {
        "name": "Intervention choice (Rule A vs Rule B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "satisfaction score",
        "role": "Outcome"
      },
      "Z": {
        "name": "ticket severity (low/high)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was ticket severity (low/high) determined before Intervention choice (Rule A vs Rule B) was chosen, and could ticket severity (low/high) have influenced the choice of Intervention choice (Rule A vs Rule B) before satisfaction score was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Rule A vs Rule B) on satisfaction score may be reversed because the mix of subgroups differs between Intervention choice (Rule A vs Rule B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by ticket severity (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Rule A vs Rule B) improves satisfaction score in each stratum, prefer Intervention choice (Rule A vs Rule B) even if the aggregate looks worse.",
      "C": "Answer if Intervention choice (Rule A vs Rule B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report satisfaction score by the key strata (e.g., ticket severity (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Rule A vs Rule B) is unevenly applied across strata.",
    "causal_structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
    "key_insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-98",
    "_original_title": "Customer Support Triage Rule",
    "_questions": "Based on the pilot, should the manager roll out intervention A or B to maximize satisfaction score? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "_expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves satisfaction score in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect."
  },
  {
    "case_id": "T3-J1-L2-0099",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: New housing permits.\nAfter the change, the reported metric average neighborhood income improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: New housing permits",
    "variables": {
      "X": {
        "name": "Intervention (New housing permits)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average neighborhood income)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (New housing permits) change who is included in the denominator before Reported metric (average neighborhood income) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average neighborhood income) after changing Intervention (New housing permits) can reflect a real outcome shift.",
      "B": "Answer if Intervention (New housing permits) changes who is counted: The aggregate Reported metric (average neighborhood income) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average neighborhood income) may be moving because the denominator/population changed after Intervention (New housing permits). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-99",
    "_original_title": "Gentrification and Average Income",
    "_questions": "Does the change in average neighborhood income prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0100",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Opening AP enrollment.\nAfter the change, the reported metric average AP exam score improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Opening AP enrollment",
    "variables": {
      "X": {
        "name": "Intervention (Opening AP enrollment)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average AP exam score)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Opening AP enrollment) change who is included in the denominator before Reported metric (average AP exam score) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average AP exam score) after changing Intervention (Opening AP enrollment) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Opening AP enrollment) changes who is counted: The aggregate Reported metric (average AP exam score) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average AP exam score) may be moving because the denominator/population changed after Intervention (Opening AP enrollment). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-100",
    "_original_title": "Expanded Advanced-Placement Access",
    "_questions": "Does the change in average AP exam score prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0101",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Merging with a contractor-heavy firm.\nAfter the change, the reported metric percent women in 'full-time staff' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Merging with a contractor-heavy firm",
    "variables": {
      "X": {
        "name": "Intervention (Merging with a contractor-heavy firm)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (percent women in 'full-time staff')",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Merging with a contractor-heavy firm) change who is included in the denominator before Reported metric (percent women in 'full-time staff') was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (percent women in 'full-time staff') after changing Intervention (Merging with a contractor-heavy firm) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Merging with a contractor-heavy firm) changes who is counted: The aggregate Reported metric (percent women in 'full-time staff') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (percent women in 'full-time staff') may be moving because the denominator/population changed after Intervention (Merging with a contractor-heavy firm). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-101",
    "_original_title": "Corporate Merger and Diversity Metrics",
    "_questions": "Does the change in percent women in 'full-time staff' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0102",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Launching a transit line.\nAfter the change, the reported metric average commute time of 'drivers' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Launching a transit line",
    "variables": {
      "X": {
        "name": "Intervention (Launching a transit line)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average commute time of 'drivers')",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Launching a transit line) change who is included in the denominator before Reported metric (average commute time of 'drivers') was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average commute time of 'drivers') after changing Intervention (Launching a transit line) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Launching a transit line) changes who is counted: The aggregate Reported metric (average commute time of 'drivers') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average commute time of 'drivers') may be moving because the denominator/population changed after Intervention (Launching a transit line). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-102",
    "_original_title": "New Transit Line and Commute Times",
    "_questions": "Does the change in average commute time of 'drivers' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0103",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Free screening days.\nAfter the change, the reported metric positive test fraction among tested improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Free screening days",
    "variables": {
      "X": {
        "name": "Intervention (Free screening days)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (positive test fraction among tested)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Free screening days) change who is included in the denominator before Reported metric (positive test fraction among tested) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (positive test fraction among tested) after changing Intervention (Free screening days) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Free screening days) changes who is counted: The aggregate Reported metric (positive test fraction among tested) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (positive test fraction among tested) may be moving because the denominator/population changed after Intervention (Free screening days). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-103",
    "_original_title": "Clinic Screening Campaign",
    "_questions": "Does the change in positive test fraction among tested prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0104",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Attendance rewards.\nAfter the change, the reported metric average absence rate among 'enrolled students' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Attendance rewards",
    "variables": {
      "X": {
        "name": "Intervention (Attendance rewards)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average absence rate among 'enrolled students')",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Attendance rewards) change who is included in the denominator before Reported metric (average absence rate among 'enrolled students') was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average absence rate among 'enrolled students') after changing Intervention (Attendance rewards) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Attendance rewards) changes who is counted: The aggregate Reported metric (average absence rate among 'enrolled students') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average absence rate among 'enrolled students') may be moving because the denominator/population changed after Intervention (Attendance rewards). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-104",
    "_original_title": "School Attendance Incentives",
    "_questions": "Does the change in average absence rate among 'enrolled students' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0105",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: New work visa program.\nAfter the change, the reported metric city unemployment rate improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: New work visa program",
    "variables": {
      "X": {
        "name": "Intervention (New work visa program)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (city unemployment rate)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (New work visa program) change who is included in the denominator before Reported metric (city unemployment rate) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (city unemployment rate) after changing Intervention (New work visa program) can reflect a real outcome shift.",
      "B": "Answer if Intervention (New work visa program) changes who is counted: The aggregate Reported metric (city unemployment rate) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (city unemployment rate) may be moving because the denominator/population changed after Intervention (New work visa program). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-105",
    "_original_title": "Immigration Policy and Unemployment",
    "_questions": "Does the change in city unemployment rate prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0106",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Launching a reporting app.\nAfter the change, the reported metric reported crime incidents improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Launching a reporting app",
    "variables": {
      "X": {
        "name": "Intervention (Launching a reporting app)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (reported crime incidents)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Launching a reporting app) change who is included in the denominator before Reported metric (reported crime incidents) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (reported crime incidents) after changing Intervention (Launching a reporting app) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Launching a reporting app) changes who is counted: The aggregate Reported metric (reported crime incidents) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (reported crime incidents) may be moving because the denominator/population changed after Intervention (Launching a reporting app). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-106",
    "_original_title": "Crime Reporting App",
    "_questions": "Does the change in reported crime incidents prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0107",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Stricter moderation.\nAfter the change, the reported metric average toxicity among remaining posts improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Stricter moderation",
    "variables": {
      "X": {
        "name": "Intervention (Stricter moderation)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average toxicity among remaining posts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Stricter moderation) change who is included in the denominator before Reported metric (average toxicity among remaining posts) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average toxicity among remaining posts) after changing Intervention (Stricter moderation) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Stricter moderation) changes who is counted: The aggregate Reported metric (average toxicity among remaining posts) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average toxicity among remaining posts) may be moving because the denominator/population changed after Intervention (Stricter moderation). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-107",
    "_original_title": "Social Platform Safety Push",
    "_questions": "Does the change in average toxicity among remaining posts prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0108",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Outsourcing hazardous tasks.\nAfter the change, the reported metric injury rate among remaining employees improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Outsourcing hazardous tasks",
    "variables": {
      "X": {
        "name": "Intervention (Outsourcing hazardous tasks)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (injury rate among remaining employees)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Outsourcing hazardous tasks) change who is included in the denominator before Reported metric (injury rate among remaining employees) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (injury rate among remaining employees) after changing Intervention (Outsourcing hazardous tasks) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Outsourcing hazardous tasks) changes who is counted: The aggregate Reported metric (injury rate among remaining employees) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (injury rate among remaining employees) may be moving because the denominator/population changed after Intervention (Outsourcing hazardous tasks). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-108",
    "_original_title": "Workplace Safety Outsourcing",
    "_questions": "Does the change in injury rate among remaining employees prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0109",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Lowering eligibility threshold.\nAfter the change, the reported metric average GPA of scholarship recipients improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Lowering eligibility threshold",
    "variables": {
      "X": {
        "name": "Intervention (Lowering eligibility threshold)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average GPA of scholarship recipients)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Lowering eligibility threshold) change who is included in the denominator before Reported metric (average GPA of scholarship recipients) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average GPA of scholarship recipients) after changing Intervention (Lowering eligibility threshold) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Lowering eligibility threshold) changes who is counted: The aggregate Reported metric (average GPA of scholarship recipients) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average GPA of scholarship recipients) may be moving because the denominator/population changed after Intervention (Lowering eligibility threshold). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-109",
    "_original_title": "Scholarship Program Expansion",
    "_questions": "Does the change in average GPA of scholarship recipients prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0110",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Expanding eligibility.\nAfter the change, the reported metric hospitalizations among vaccinated people improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Expanding eligibility",
    "variables": {
      "X": {
        "name": "Intervention (Expanding eligibility)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (hospitalizations among vaccinated people)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Expanding eligibility) change who is included in the denominator before Reported metric (hospitalizations among vaccinated people) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (hospitalizations among vaccinated people) after changing Intervention (Expanding eligibility) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Expanding eligibility) changes who is counted: The aggregate Reported metric (hospitalizations among vaccinated people) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (hospitalizations among vaccinated people) may be moving because the denominator/population changed after Intervention (Expanding eligibility). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-110",
    "_original_title": "Vaccination Priority Change",
    "_questions": "Does the change in hospitalizations among vaccinated people prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0111",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Raising credit score requirement.\nAfter the change, the reported metric default rate among approved borrowers improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Raising credit score requirement",
    "variables": {
      "X": {
        "name": "Intervention (Raising credit score requirement)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (default rate among approved borrowers)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Raising credit score requirement) change who is included in the denominator before Reported metric (default rate among approved borrowers) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (default rate among approved borrowers) after changing Intervention (Raising credit score requirement) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Raising credit score requirement) changes who is counted: The aggregate Reported metric (default rate among approved borrowers) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (default rate among approved borrowers) may be moving because the denominator/population changed after Intervention (Raising credit score requirement). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-111",
    "_original_title": "Loan Program Tightening",
    "_questions": "Does the change in default rate among approved borrowers prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0112",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Adding a new campus.\nAfter the change, the reported metric average SAT score of admitted students improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Adding a new campus",
    "variables": {
      "X": {
        "name": "Intervention (Adding a new campus)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average SAT score of admitted students)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Adding a new campus) change who is included in the denominator before Reported metric (average SAT score of admitted students) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average SAT score of admitted students) after changing Intervention (Adding a new campus) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Adding a new campus) changes who is counted: The aggregate Reported metric (average SAT score of admitted students) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average SAT score of admitted students) may be moving because the denominator/population changed after Intervention (Adding a new campus). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-112",
    "_original_title": "University Admissions Expansion",
    "_questions": "Does the change in average SAT score of admitted students prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0113",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Changing voucher eligibility.\nAfter the change, the reported metric average rent paid by voucher holders improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Changing voucher eligibility",
    "variables": {
      "X": {
        "name": "Intervention (Changing voucher eligibility)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average rent paid by voucher holders)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Changing voucher eligibility) change who is included in the denominator before Reported metric (average rent paid by voucher holders) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average rent paid by voucher holders) after changing Intervention (Changing voucher eligibility) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Changing voucher eligibility) changes who is counted: The aggregate Reported metric (average rent paid by voucher holders) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average rent paid by voucher holders) may be moving because the denominator/population changed after Intervention (Changing voucher eligibility). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-113",
    "_original_title": "Housing Voucher Reform",
    "_questions": "Does the change in average rent paid by voucher holders prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0114",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Changing sign-up incentives.\nAfter the change, the reported metric average weight loss among participants improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Changing sign-up incentives",
    "variables": {
      "X": {
        "name": "Intervention (Changing sign-up incentives)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average weight loss among participants)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Changing sign-up incentives) change who is included in the denominator before Reported metric (average weight loss among participants) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average weight loss among participants) after changing Intervention (Changing sign-up incentives) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Changing sign-up incentives) changes who is counted: The aggregate Reported metric (average weight loss among participants) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average weight loss among participants) may be moving because the denominator/population changed after Intervention (Changing sign-up incentives). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-114",
    "_original_title": "Wellness Program Redesign",
    "_questions": "Does the change in average weight loss among participants prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0115",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Moving enforcement to highways.\nAfter the change, the reported metric average speed on arterial roads improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Moving enforcement to highways",
    "variables": {
      "X": {
        "name": "Intervention (Moving enforcement to highways)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average speed on arterial roads)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Moving enforcement to highways) change who is included in the denominator before Reported metric (average speed on arterial roads) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average speed on arterial roads) after changing Intervention (Moving enforcement to highways) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Moving enforcement to highways) changes who is counted: The aggregate Reported metric (average speed on arterial roads) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average speed on arterial roads) may be moving because the denominator/population changed after Intervention (Moving enforcement to highways). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-115",
    "_original_title": "Traffic Enforcement Shift",
    "_questions": "Does the change in average speed on arterial roads prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0116",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A policymaker implements an intervention: Switching evaluation to test-score growth.\nAfter the change, the reported metric average teacher rating improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "claim": "A policymaker implements an intervention: Switching evaluation to test-score growth",
    "variables": {
      "X": {
        "name": "Intervention (Switching evaluation to test-score growth)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported metric (average teacher rating)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did Intervention (Switching evaluation to test-score growth) change who is included in the denominator before Reported metric (average teacher rating) was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Reported metric (average teacher rating) after changing Intervention (Switching evaluation to test-score growth) can reflect a real outcome shift.",
      "B": "Answer if Intervention (Switching evaluation to test-score growth) changes who is counted: The aggregate Reported metric (average teacher rating) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average teacher rating) may be moving because the denominator/population changed after Intervention (Switching evaluation to test-score growth). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
    "key_insight": "Averages can change because the set of people counted changes, even if nobody improves.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-116",
    "_original_title": "Teacher Evaluation Metric Change",
    "_questions": "Does the change in average teacher rating prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "_expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time)."
  },
  {
    "case_id": "T3-J1-L2-0117",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: Attending the workshop.\nIn observational data, people who receive the intervention have better outcomes on subsequent performance rating than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: Attending the workshop",
    "variables": {
      "X": {
        "name": "Intervention uptake (Attending the workshop)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (subsequent performance rating)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (subsequent performance rating) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (Attending the workshop) is randomly assigned: A difference in Outcome (subsequent performance rating) across Intervention uptake (Attending the workshop) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (Attending the workshop) vs not-Intervention uptake (Attending the workshop) difference in Outcome (subsequent performance rating) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Attending the workshop)–Outcome (subsequent performance rating) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-117",
    "_original_title": "Voluntary Leadership Workshop",
    "_questions": "Can we conclude that offering Attending the workshop to everyone will improve subsequent performance rating? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0118",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: Taking the prep course.\nIn observational data, people who receive the intervention have better outcomes on SAT score improvement than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: Taking the prep course",
    "variables": {
      "X": {
        "name": "Intervention uptake (Taking the prep course)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (SAT score improvement)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (SAT score improvement) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (Taking the prep course) is randomly assigned: A difference in Outcome (SAT score improvement) across Intervention uptake (Taking the prep course) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (Taking the prep course) vs not-Intervention uptake (Taking the prep course) difference in Outcome (SAT score improvement) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Taking the prep course)–Outcome (SAT score improvement) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-118",
    "_original_title": "Optional SAT Prep Course",
    "_questions": "Can we conclude that offering Taking the prep course to everyone will improve SAT score improvement? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0119",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: Upgrading to premium.\nIn observational data, people who receive the intervention have better outcomes on weekly workouts than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: Upgrading to premium",
    "variables": {
      "X": {
        "name": "Intervention uptake (Upgrading to premium)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (weekly workouts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (weekly workouts) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (Upgrading to premium) is randomly assigned: A difference in Outcome (weekly workouts) across Intervention uptake (Upgrading to premium) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (Upgrading to premium) vs not-Intervention uptake (Upgrading to premium) difference in Outcome (weekly workouts) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Upgrading to premium)–Outcome (weekly workouts) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-119",
    "_original_title": "Fitness App Premium Subscription",
    "_questions": "Can we conclude that offering Upgrading to premium to everyone will improve weekly workouts? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0120",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Psychology",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: Starting therapy.\nIn observational data, people who receive the intervention have better outcomes on reported stress score than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: Starting therapy",
    "variables": {
      "X": {
        "name": "Intervention uptake (Starting therapy)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (reported stress score)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (reported stress score) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (Starting therapy) is randomly assigned: A difference in Outcome (reported stress score) across Intervention uptake (Starting therapy) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (Starting therapy) vs not-Intervention uptake (Starting therapy) difference in Outcome (reported stress score) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Starting therapy)–Outcome (reported stress score) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-120",
    "_original_title": "Mental Health Counseling",
    "_questions": "Can we conclude that offering Starting therapy to everyone will improve reported stress score? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0121",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: Enrolling in charter school.\nIn observational data, people who receive the intervention have better outcomes on graduation likelihood than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: Enrolling in charter school",
    "variables": {
      "X": {
        "name": "Intervention uptake (Enrolling in charter school)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (graduation likelihood)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (graduation likelihood) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (Enrolling in charter school) is randomly assigned: A difference in Outcome (graduation likelihood) across Intervention uptake (Enrolling in charter school) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (Enrolling in charter school) vs not-Intervention uptake (Enrolling in charter school) difference in Outcome (graduation likelihood) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Enrolling in charter school)–Outcome (graduation likelihood) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-121",
    "_original_title": "Charter School Enrollment",
    "_questions": "Can we conclude that offering Enrolling in charter school to everyone will improve graduation likelihood? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0122",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: Using the voucher.\nIn observational data, people who receive the intervention have better outcomes on employment after 6 months than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: Using the voucher",
    "variables": {
      "X": {
        "name": "Intervention uptake (Using the voucher)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (employment after 6 months)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (employment after 6 months) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (Using the voucher) is randomly assigned: A difference in Outcome (employment after 6 months) across Intervention uptake (Using the voucher) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (Using the voucher) vs not-Intervention uptake (Using the voucher) difference in Outcome (employment after 6 months) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Using the voucher)–Outcome (employment after 6 months) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-122",
    "_original_title": "Job Training Voucher",
    "_questions": "Can we conclude that offering Using the voucher to everyone will improve employment after 6 months? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0123",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: getting screened.\nIn observational data, people who receive the intervention have better outcomes on later hospitalization than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: getting screened",
    "variables": {
      "X": {
        "name": "Intervention uptake (getting screened)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (later hospitalization)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (later hospitalization) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (getting screened) is randomly assigned: A difference in Outcome (later hospitalization) across Intervention uptake (getting screened) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (getting screened) vs not-Intervention uptake (getting screened) difference in Outcome (later hospitalization) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (getting screened)–Outcome (later hospitalization) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-123",
    "_original_title": "Preventive Health Screening",
    "_questions": "Can we conclude that offering getting screened to everyone will improve later hospitalization? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0124",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: opting into remote work.\nIn observational data, people who receive the intervention have better outcomes on output per hour than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: opting into remote work",
    "variables": {
      "X": {
        "name": "Intervention uptake (opting into remote work)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (output per hour)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (output per hour) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (opting into remote work) is randomly assigned: A difference in Outcome (output per hour) across Intervention uptake (opting into remote work) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (opting into remote work) vs not-Intervention uptake (opting into remote work) difference in Outcome (output per hour) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (opting into remote work)–Outcome (output per hour) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-124",
    "_original_title": "Remote Work Opt-In",
    "_questions": "Can we conclude that offering opting into remote work to everyone will improve output per hour? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0125",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A decision-maker considers scaling an intervention: joining accelerator.\nIn observational data, people who receive the intervention have better outcomes on funding raised than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "claim": "A decision-maker considers scaling an intervention: joining accelerator",
    "variables": {
      "X": {
        "name": "Intervention uptake (joining accelerator)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Outcome (funding raised)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Outcome (funding raised) occurred—and could selection depend on the outcome?",
    "conditional_answers": {
      "A": "Answer if Intervention uptake (joining accelerator) is randomly assigned: A difference in Outcome (funding raised) across Intervention uptake (joining accelerator) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected: The Intervention uptake (joining accelerator) vs not-Intervention uptake (joining accelerator) difference in Outcome (funding raised) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (joining accelerator)–Outcome (funding raised) differences may reflect selection rather than effect.",
    "causal_structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
    "key_insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-125",
    "_original_title": "Startup Accelerator Admission",
    "_questions": "Can we conclude that offering joining accelerator to everyone will improve funding raised? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "_expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection."
  },
  {
    "case_id": "T3-J1-L2-0126",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between test score and essay quality, but only within a selected group defined by admission decision.\nInside that selected group, test score and essay quality appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (admission decision) is influenced by both test score and essay quality, making it a collider.",
    "claim": "An analyst studies the relationship between test score and essay quality, but only within a selected group defined by admission decision",
    "variables": {
      "Y": {
        "name": "Outcome (first-year GPA)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (first-year GPA)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (first-year GPA) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-126",
    "_original_title": "Among Admitted Applicants",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0127",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between hours worked and manager liking, but only within a selected group defined by promotion.\nInside that selected group, hours worked and manager liking appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (promotion) is influenced by both hours worked and manager liking, making it a collider.",
    "claim": "An analyst studies the relationship between hours worked and manager liking, but only within a selected group defined by promotion",
    "variables": {
      "Y": {
        "name": "Outcome (later performance)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (later performance)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (later performance) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-127",
    "_original_title": "Among Promoted Employees",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0128",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between age and comorbidity index, but only within a selected group defined by hospitalization.\nInside that selected group, age and comorbidity index appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (hospitalization) is influenced by both age and comorbidity index, making it a collider.",
    "claim": "An analyst studies the relationship between age and comorbidity index, but only within a selected group defined by hospitalization",
    "variables": {
      "Y": {
        "name": "Outcome (mortality)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (mortality)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (mortality) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-128",
    "_original_title": "Among Hospitalized Patients",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0129",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between topic controversy and creator follower count, but only within a selected group defined by going viral.\nInside that selected group, topic controversy and creator follower count appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (going viral) is influenced by both topic controversy and creator follower count, making it a collider.",
    "claim": "An analyst studies the relationship between topic controversy and creator follower count, but only within a selected group defined by going viral",
    "variables": {
      "Y": {
        "name": "Outcome (misinformation rate)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (misinformation rate)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (misinformation rate) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-129",
    "_original_title": "Among Viral Posts",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0130",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between family income and grades, but only within a selected group defined by winning scholarship.\nInside that selected group, family income and grades appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (winning scholarship) is influenced by both family income and grades, making it a collider.",
    "claim": "An analyst studies the relationship between family income and grades, but only within a selected group defined by winning scholarship",
    "variables": {
      "Y": {
        "name": "Outcome (college persistence)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (college persistence)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (college persistence) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-130",
    "_original_title": "Among Scholarship Winners",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0131",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between income and credit history, but only within a selected group defined by loan approval.\nInside that selected group, income and credit history appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (loan approval) is influenced by both income and credit history, making it a collider.",
    "claim": "An analyst studies the relationship between income and credit history, but only within a selected group defined by loan approval",
    "variables": {
      "Y": {
        "name": "Outcome (default)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (default)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (default) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-131",
    "_original_title": "Among Loan Approvals",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0132",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between crime severity and police presence, but only within a selected group defined by being arrested.\nInside that selected group, crime severity and police presence appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (being arrested) is influenced by both crime severity and police presence, making it a collider.",
    "claim": "An analyst studies the relationship between crime severity and police presence, but only within a selected group defined by being arrested",
    "variables": {
      "Y": {
        "name": "Outcome (conviction)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (conviction)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (conviction) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-132",
    "_original_title": "Among Arrested Individuals",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0133",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between territory quality and call volume, but only within a selected group defined by being top-10% in sales.\nInside that selected group, territory quality and call volume appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (being top-10% in sales) is influenced by both territory quality and call volume, making it a collider.",
    "claim": "An analyst studies the relationship between territory quality and call volume, but only within a selected group defined by being top-10% in sales",
    "variables": {
      "Y": {
        "name": "Outcome (customer churn)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (customer churn)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (customer churn) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-133",
    "_original_title": "Among Top-Sales Reps",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0134",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Psychology",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "An analyst studies the relationship between study novelty and p-value, but only within a selected group defined by publication.\nInside that selected group, study novelty and p-value appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (publication) is influenced by both study novelty and p-value, making it a collider.",
    "claim": "An analyst studies the relationship between study novelty and p-value, but only within a selected group defined by publication",
    "variables": {
      "Y": {
        "name": "Outcome (replication success)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (replication success)?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable: Associations between X and Outcome (replication success) can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
    "key_insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-134",
    "_original_title": "Among Published Papers",
    "_questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "_expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID."
  },
  {
    "case_id": "T3-J1-L2-0135",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of number of break-in reports.\nThey point to a larger number of events in one group or after installing cameras.\nBut the groups have very different base sizes or exposure levels (neighborhood size differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of number of break-in reports",
    "variables": {
      "X": {
        "name": "Group/intervention status (installing cameras)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (number of break-in reports)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (number of break-in reports) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (number of break-in reports): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (number of break-in reports) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (number of break-in reports) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-135",
    "_original_title": "More Break-ins After Installing Cameras?",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0136",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of side-effect reports.\nThey point to a larger number of events in one group or after new medication rollout.\nBut the groups have very different base sizes or exposure levels (exposure population differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of side-effect reports",
    "variables": {
      "X": {
        "name": "Group/intervention status (new medication rollout)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (side-effect reports)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (side-effect reports) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (side-effect reports): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (side-effect reports) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (side-effect reports) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-136",
    "_original_title": "Side Effects After New Medication Rollout",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0137",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of discipline referrals.\nThey point to a larger number of events in one group or after new conduct rule.\nBut the groups have very different base sizes or exposure levels (group enrollment differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of discipline referrals",
    "variables": {
      "X": {
        "name": "Group/intervention status (new conduct rule)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (discipline referrals)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (discipline referrals) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (discipline referrals): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (discipline referrals) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (discipline referrals) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-137",
    "_original_title": "Discipline Disparities After Rule Change",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0138",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of fraud alerts.\nThey point to a larger number of events in one group or after fraud filter in Store A vs B.\nBut the groups have very different base sizes or exposure levels (transaction volume differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of fraud alerts",
    "variables": {
      "X": {
        "name": "Group/intervention status (fraud filter in Store A vs B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (fraud alerts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (fraud alerts) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (fraud alerts): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (fraud alerts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (fraud alerts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-138",
    "_original_title": "Fraud Alerts in Two Stores",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0139",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of flagged posts.\nThey point to a larger number of events in one group or after topic category.\nBut the groups have very different base sizes or exposure levels (posting volume differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of flagged posts",
    "variables": {
      "X": {
        "name": "Group/intervention status (topic category)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (flagged posts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (flagged posts) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (flagged posts): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (flagged posts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (flagged posts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-139",
    "_original_title": "Misinformation Flags Across Topics",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0140",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of injury counts.\nThey point to a larger number of events in one group or after safety training.\nBut the groups have very different base sizes or exposure levels (work-hours differ), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of injury counts",
    "variables": {
      "X": {
        "name": "Group/intervention status (safety training)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (injury counts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (injury counts) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (injury counts): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (injury counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (injury counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-140",
    "_original_title": "Accidents in Two Factories",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0141",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of default counts.\nThey point to a larger number of events in one group or after loan product type.\nBut the groups have very different base sizes or exposure levels (number of loans differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of default counts",
    "variables": {
      "X": {
        "name": "Group/intervention status (loan product type)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (default counts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (default counts) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (default counts): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (default counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (default counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-141",
    "_original_title": "Defaults in Two Loan Products",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0142",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of hospitalized counts.\nThey point to a larger number of events in one group or after vaccination status.\nBut the groups have very different base sizes or exposure levels (base rate of vaccination differs), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of hospitalized counts",
    "variables": {
      "X": {
        "name": "Group/intervention status (vaccination status)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (hospitalized counts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (hospitalized counts) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (hospitalized counts): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (hospitalized counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (hospitalized counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-142",
    "_original_title": "Hospitalizations Among Vaccinated vs Unvaccinated",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L2-0143",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Easy",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of complaint counts.\nThey point to a larger number of events in one group or after service plan launch.\nBut the groups have very different base sizes or exposure levels (customer base changed), so comparing counts without denominators is misleading.",
    "claim": "A stakeholder argues that an intervention or group is riskier based on raw counts of complaint counts",
    "variables": {
      "X": {
        "name": "Group/intervention status (service plan launch)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Event count (complaint counts)",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "What is the relevant denominator at the time Event count (complaint counts) was counted, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Event count (complaint counts): You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Event count (complaint counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (complaint counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything.",
    "causal_structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
    "key_insight": "A larger count can correspond to a lower rate when the base population is larger.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-143",
    "_original_title": "Complaints After Service Plan Launch",
    "_questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "_expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders."
  },
  {
    "case_id": "T3-J1-L3-0144",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "Applicant retakes standardized test after a rejection.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "Applicant retakes standardized test after a rejection",
    "variables": {
      "X": {
        "name": "Retake decision",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Admission outcome",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Retake decision was (or could have been) chosen, what information was available, and when was Admission outcome realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Retake decision is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-144",
    "_original_title": "Would I Have Been Admitted If I Retook the Test?",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0145",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T12",
    "trap_family": "",
    "trap_subtype": "Late Preemption",
    "scenario": "A candidate declines a job offer and later experiences a downturn in their current job.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A candidate declines a job offer and later experiences a downturn in their current job",
    "variables": {
      "X": {
        "name": "Accept offer",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Income stability after 1 year",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Accept offer was (or could have been) chosen, what information was available, and when was Income stability after 1 year realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Accept offer is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-145",
    "_original_title": "Turning Down a Job Offer and Later Regret",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0146",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Medium",
    "trap_type": "T12",
    "trap_family": "",
    "trap_subtype": "Late Preemption",
    "scenario": "A parole board denies release; later debates whether release would have increased reoffending.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A parole board denies release; later debates whether release would have increased reoffending",
    "variables": {
      "X": {
        "name": "Release on parole",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reoffense within 2 years",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Release on parole was (or could have been) chosen, what information was available, and when was Reoffense within 2 years realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Release on parole is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-146",
    "_original_title": "Parole Decision and Future Crime",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0147",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A patient refuses a recommended procedure and later recovers slowly.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A patient refuses a recommended procedure and later recovers slowly",
    "variables": {
      "X": {
        "name": "Receive procedure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Recovery time",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Receive procedure was (or could have been) chosen, what information was available, and when was Recovery time realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Receive procedure is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-147",
    "_original_title": "Medical Treatment Refusal",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0148",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Medium",
    "trap_type": "T11",
    "trap_family": "",
    "trap_subtype": "Dynamic World Divergence",
    "scenario": "A student stays in a difficult major and wonders if switching would have improved GPA.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A student stays in a difficult major and wonders if switching would have improved GPA",
    "variables": {
      "X": {
        "name": "Switch major",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "GPA after two semesters",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Switch major was (or could have been) chosen, what information was available, and when was GPA after two semesters realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Switch major is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-148",
    "_original_title": "Choosing a Major Switch",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0149",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A student is just below a scholarship cutoff; committee debates if scholarship would change graduation probability.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A student is just below a scholarship cutoff; committee debates if scholarship would change graduation probability",
    "variables": {
      "X": {
        "name": "Receive scholarship",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Graduation",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Receive scholarship was (or could have been) chosen, what information was available, and when was Graduation realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Receive scholarship is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-149",
    "_original_title": "Scholarship Cutoff Appeal",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0150",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Medium",
    "trap_type": "T9",
    "trap_family": "",
    "trap_subtype": "Mediator Fixing Error",
    "scenario": "A city raises minimum wage; policymakers ask what employment would have been without the change.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A city raises minimum wage; policymakers ask what employment would have been without the change",
    "variables": {
      "X": {
        "name": "Minimum wage increase",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Employment rate",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Minimum wage increase was (or could have been) chosen, what information was available, and when was Employment rate realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Minimum wage increase is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-150",
    "_original_title": "Minimum Wage Increase and Employment",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0151",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T11",
    "trap_family": "",
    "trap_subtype": "Dynamic World Divergence",
    "scenario": "Congestion pricing is introduced; the city asks what traffic would have been otherwise.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "Congestion pricing is introduced; the city asks what traffic would have been otherwise",
    "variables": {
      "X": {
        "name": "Introduce congestion pricing",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Average travel time",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Introduce congestion pricing was (or could have been) chosen, what information was available, and when was Average travel time realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Introduce congestion pricing is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-151",
    "_original_title": "Congestion Pricing Rollout",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0152",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T12",
    "trap_family": "",
    "trap_subtype": "Early Preemption",
    "scenario": "A district increases funding to low-income schools; asks what scores would have been absent reform.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A district increases funding to low-income schools; asks what scores would have been absent reform",
    "variables": {
      "X": {
        "name": "Funding reform",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Standardized test scores",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Funding reform was (or could have been) chosen, what information was available, and when was Standardized test scores realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Funding reform is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-152",
    "_original_title": "School Funding Reform",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0153",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A construction ban is enacted; residents debate what rents would have been without the ban.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A construction ban is enacted; residents debate what rents would have been without the ban",
    "variables": {
      "X": {
        "name": "Construction moratorium",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Median rent",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Construction moratorium was (or could have been) chosen, what information was available, and when was Median rent realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Construction moratorium is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-153",
    "_original_title": "Housing Construction Moratorium",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0154",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A new emissions rule is enacted; regulators ask what pollution levels would have been without it.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A new emissions rule is enacted; regulators ask what pollution levels would have been without it",
    "variables": {
      "X": {
        "name": "Emissions regulation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Air pollutant concentration",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Emissions regulation was (or could have been) chosen, what information was available, and when was Air pollutant concentration realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Emissions regulation is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-154",
    "_original_title": "Environmental Regulation Change",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0155",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T9",
    "trap_family": "",
    "trap_subtype": "Mediator Fixing Error",
    "scenario": "New bus routes are added; question is what car usage would have been otherwise.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "New bus routes are added; question is what car usage would have been otherwise",
    "variables": {
      "X": {
        "name": "Transit expansion",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Car trips per capita",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Transit expansion was (or could have been) chosen, what information was available, and when was Car trips per capita realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Transit expansion is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-155",
    "_original_title": "Public Transit Expansion",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0156",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Medium",
    "trap_type": "T9",
    "trap_family": "",
    "trap_subtype": "Mediator Fixing Error",
    "scenario": "A recommender model is updated; auditors ask how outcomes would differ for users under the old model.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A recommender model is updated; auditors ask how outcomes would differ for users under the old model",
    "variables": {
      "X": {
        "name": "Model version",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Exposure to harmful content",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Model version was (or could have been) chosen, what information was available, and when was Exposure to harmful content realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Model version is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-156",
    "_original_title": "Algorithmic Fairness Audit: Who Was Harmed?",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0157",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T12",
    "trap_family": "",
    "trap_subtype": "Early Preemption",
    "scenario": "A candidate is rejected; dispute whether rejection was due to bias or fit.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A candidate is rejected; dispute whether rejection was due to bias or fit",
    "variables": {
      "X": {
        "name": "Candidate protected attribute",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Hiring decision",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Candidate protected attribute was (or could have been) chosen, what information was available, and when was Hiring decision realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Candidate protected attribute is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-157",
    "_original_title": "Discrimination Attribution in Hiring",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0158",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T11",
    "trap_family": "",
    "trap_subtype": "Dynamic World Divergence",
    "scenario": "A university adds a quota; asks how composition and outcomes would look without it.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A university adds a quota; asks how composition and outcomes would look without it",
    "variables": {
      "X": {
        "name": "Quota policy",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Graduation outcomes across groups",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Quota policy was (or could have been) chosen, what information was available, and when was Graduation outcomes across groups realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Quota policy is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-158",
    "_original_title": "Quota Policy Evaluation",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0159",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "Stimulus checks are issued; question is what spending would have been absent stimulus.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "Stimulus checks are issued; question is what spending would have been absent stimulus",
    "variables": {
      "X": {
        "name": "Stimulus payment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Household spending",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Stimulus payment was (or could have been) chosen, what information was available, and when was Household spending realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Stimulus payment is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-159",
    "_original_title": "Economic Stimulus Effectiveness",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L3-0160",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Hard",
    "trap_type": "T11",
    "trap_family": "",
    "trap_subtype": "Dynamic World Divergence",
    "scenario": "A close election; analysts ask what would have happened if turnout rules differed.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "claim": "A close election; analysts ask what would have happened if turnout rules differed",
    "variables": {
      "X": {
        "name": "Turnout rule change",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Election winner",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "At the decision time when Turnout rule change was (or could have been) chosen, what information was available, and when was Election winner realized relative to that choice?",
    "conditional_answers": {
      "A": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.",
      "B": "Answer if the choice of Turnout rule change is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.",
      "C": "Answer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds).",
    "causal_structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
    "key_insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-160",
    "_original_title": "Election Outcome Counterfactual",
    "_questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "_expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U."
  },
  {
    "case_id": "T3-J1-L1-0161",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "A nonprofit reports that applicants from Prep Program A are accepted into selective colleges at a higher overall rate than applicants from Prep Program B. Donors conclude Program A is more effective and recommend expanding it.\n\nA counselor points out that when acceptance rates are broken down by family income bracket (low-income vs higher-income), Program B has a higher acceptance rate in both brackets. Program A has more higher-income participants overall, while Program B serves mostly low-income participants.",
    "claim": "applicants from Prep Program A are accepted into selective colleges at a higher overall rate than applicants from Prep Program B",
    "variables": {
      "X": {
        "name": "Prep program (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "College acceptance rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Family income bracket (low vs. higher)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was family income bracket known before students enrolled in Prep Program A vs. B, making it a true pre-enrollment stratifier?",
    "conditional_answers": {
      "A": "Answer if you only use overall acceptance rates: You may wrongly prefer Program A because it serves more higher-income students.",
      "B": "Answer if you stratify or standardize by income bracket: Compare within strata (or compute standardized rates) to evaluate program performance.",
      "C": "Answer if assignment to programs can be randomized within income strata: Then within-stratum differences can support a causal claim about program effectiveness."
    },
    "wise_refusal": "I can’t determine which program is causally better without baseline comparability (prior grades, support) and a credible assignment mechanism.",
    "causal_structure": "Z affects Y and differs in distribution across X; aggregating across Z reverses the subgroup acceptance-rate comparison.",
    "key_insight": "Aggregate success rates can reverse within-group trends when group composition differs.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-161",
    "_original_title": "Scholarship Acceptance Rates by Prep Program",
    "_questions": "Does the higher overall acceptance rate for Program A imply it is more effective than Program B?\nWhat happens to the comparison if you condition on family income bracket?",
    "_expected_analysis": "Identify Simpson’s Paradox: the aggregate difference is driven by different income composition across programs.\nIncome bracket (Z) predicts acceptance (Y) and differs across programs (X), so the overall rate is not a fair comparison.\nWithin each income bracket, Program B performs better, suggesting Program A’s advantage is compositional.\nConclusion: The claim “Program A is more effective” is INVALID based on the aggregate statistic alone."
  },
  {
    "case_id": "T3-J1-L1-0162",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Finance",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Imbalanced Group Composition",
    "scenario": "A city report states that Bank X has a lower overall loan default rate than Bank Y, and officials argue Bank X has better underwriting practices.\n\nWhen analysts stratify borrowers by credit-risk tier (prime vs subprime), Bank Y has a lower default rate in both tiers. The reversal occurs because Bank X approves a larger share of prime borrowers, while Bank Y serves more subprime borrowers overall.",
    "claim": "A city report states that Bank X has a lower overall loan default rate than Bank Y, and officials argue Bank X has better underwriting practices",
    "variables": {
      "X": {
        "name": "Bank (X vs. Y)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Loan default rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Credit-risk tier (prime vs. subprime)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is credit-risk tier assessed before borrowers choose a bank, making Z pre-treatment with respect to X?",
    "conditional_answers": {
      "A": "Answer if you compare only overall defaults: You may incorrectly attribute Bank X’s lower defaults to better underwriting.",
      "B": "Answer if you compare within risk tiers or compute standardized defaults: Prefer the bank with lower within-tier defaults (and report a standardized overall rate).",
      "C": "Answer if approval decisions themselves change the portfolio mix: Separate “mix effects” from “within-tier performance.”"
    },
    "wise_refusal": "I can’t judge underwriting quality without consistent risk-tier measurement and a standardized comparison across the same risk distribution.",
    "causal_structure": "Z strongly affects Y and the distribution of Z differs by X; aggregate defaults reverse the within-tier comparison.",
    "key_insight": "Comparing institutions using aggregate outcomes is misleading when they serve different mixes of clients.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-162",
    "_original_title": "Two Banks and the Default Rate",
    "_questions": "Does Bank X’s lower overall default rate prove it underwrites better than Bank Y?\nWhy can the conclusion reverse after stratifying by credit-risk tier?",
    "_expected_analysis": "This is Simpson’s Paradox caused by imbalanced risk composition.\nRisk tier (Z) drives default (Y). Banks differ in borrower mix, so overall default rates conflate underwriting with portfolio composition.\nWithin both tiers, Bank Y performs better.\nConclusion: The claim “Bank X has better underwriting” is INVALID from the aggregate comparison alone."
  },
  {
    "case_id": "T3-J1-L1-0163",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Healthcare Administration",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "A hospital network claims Clinic A provides better follow-up care because its overall 30-day readmission rate is lower than Clinic B’s.\n\nWhen readmissions are broken down by patient age group (under 65 vs 65+), Clinic B has lower readmission rates in both groups. Clinic A treats a much larger share of younger patients, while Clinic B treats more older patients overall.",
    "claim": "A hospital network claims Clinic A provides better follow-up care because its overall 30-day readmission rate is lower than Clinic B’s",
    "variables": {
      "X": {
        "name": "Clinic (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "30-day readmission rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Age group (under 65 vs. 65+)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was age determined before clinic selection (a true pre-treatment stratifier), or does clinic choice affect which ages are seen?",
    "conditional_answers": {
      "A": "Answer if you only look at overall readmissions: You might reward Clinic A incorrectly.",
      "B": "Answer if you age-standardize or compare within age strata: The within-stratum comparison is more informative for care quality.",
      "C": "Answer if clinics differ in referral patterns: Adjust for case mix and referral selection before drawing conclusions."
    },
    "wise_refusal": "I can’t attribute readmission differences to care quality without case-mix adjustment and information on referral/triage processes.",
    "causal_structure": "Z affects Y and differs by clinic; aggregating across Z reverses the subgroup comparison.",
    "key_insight": "Overall metrics can hide within-group differences driven by demographic mix.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-163",
    "_original_title": "Clinic Readmissions by Age Mix",
    "_questions": "Does Clinic A’s lower overall readmission rate imply better follow-up care?\nWhat does the comparison show within each age group?",
    "_expected_analysis": "Simpson’s Paradox: age group (Z) strongly affects readmission (Y) and is uneven across clinics (X).\nClinic A’s overall advantage can be explained by treating more younger patients.\nWithin both age groups, Clinic B performs better.\nConclusion: The claim “Clinic A has better follow-up care” is INVALID from the overall rate alone."
  },
  {
    "case_id": "T3-J1-L1-0164",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "A company emails an anonymous satisfaction survey and reports that “85% of employees are satisfied.” Leadership concludes morale is high.\n\nOnly 30% of employees responded. Dissatisfied employees may be less likely to respond because they think nothing will change or fear being identified.",
    "claim": "“85% of employees are satisfied",
    "variables": {
      "X": {
        "name": "Survey response (respond vs. not)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported satisfaction",
        "role": "Outcome"
      },
      "Z": {
        "name": "Dissatisfaction / fear of retaliation",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Did dissatisfaction exist before employees chose whether to respond, or did the survey context change willingness to respond?",
    "conditional_answers": {
      "A": "Answer if response is random: Then the respondent rate estimates overall morale.",
      "B": "Answer if dissatisfied employees avoid responding: Then results are biased upward; improve sampling/response incentives.",
      "C": "Answer if response differs by team or manager: Use stratified follow-ups to assess representation."
    },
    "wise_refusal": "I can’t infer company-wide morale without response-rate patterns and information about non-respondents.",
    "causal_structure": "Z affects response and satisfaction; conditioning on responders yields biased satisfaction estimate.",
    "key_insight": "Survey results can be unrepresentative when response is selective.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-164",
    "_original_title": "The Employee Satisfaction Survey",
    "_questions": "Does 85% satisfaction among respondents represent satisfaction among all employees?\nWhat selection mechanism could bias the observed satisfaction rate?",
    "_expected_analysis": "Selection bias via sampling-on-the-outcome: only responders are observed.\nIf dissatisfied employees respond less, the observed satisfaction rate overestimates true satisfaction.\nConclusion: The claim “morale is high company-wide” is INVALID from respondent-only data."
  },
  {
    "case_id": "T3-J1-L1-0165",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Media Economics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "A blog analyzes “what makes a film profitable” by looking only at movies that won major awards. The author finds that award-winning films have high budgets and concludes that increasing budgets causes higher profits.\n\nThe dataset excludes many high-budget films that were not nominated or that failed commercially. Award status is a filter for inclusion.",
    "claim": "increasing budgets causes higher profits",
    "variables": {
      "X": {
        "name": "Production budget",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Profit",
        "role": "Outcome"
      },
      "Z": {
        "name": "Award nomination/win status",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is award status determined before profits are realized, or is the dataset effectively filtered after outcomes are known?",
    "conditional_answers": {
      "A": "Answer if awards are unrelated to profit (unlikely): Restriction would be less problematic.",
      "B": "Answer if awards filter on quality/visibility correlated with profit: Include all films to avoid selection bias.",
      "C": "Answer if you can model nomination probability: Any correction remains assumption-dependent."
    },
    "wise_refusal": "I can’t draw a causal conclusion without data on non-award films and a clear sampling frame that includes both successes and failures.",
    "causal_structure": "Conditioning on Z (award inclusion) selects a non-representative subset; X–Y relationship differs from full population.",
    "key_insight": "Analyzing only visible successes distorts relationships between inputs and outcomes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-165",
    "_original_title": "Studying Only Award-Winning Films",
    "_questions": "Does the budget–profit pattern among award winners imply bigger budgets cause profits?\nWhy does restricting to award winners bias the analysis?",
    "_expected_analysis": "Selection bias: conditioning on award status creates a selected sample.\nAwards correlate with many factors and exclude failures, so the remaining sample is not representative.\nConclusion: The claim “bigger budgets cause higher profits” is INVALID from award-winner-only data."
  },
  {
    "case_id": "T3-J1-L1-0166",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Labor Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Attrition Bias",
    "scenario": "A city runs a six-month workforce training program and reports that participants’ average wages increased by 20%. Officials conclude the program boosted earnings.\n\nHowever, 40% of participants dropped out and are missing from the final wage measurement. The report analyzes only participants who completed the program.",
    "claim": "participants’ average wages increased by 20%",
    "variables": {
      "X": {
        "name": "Training program enrollment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Post-program wage",
        "role": "Outcome"
      },
      "Z": {
        "name": "Program completion (complete vs. dropout)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Did dropout occur after participants could have benefited (or been harmed) by the program, making completion a post-treatment selection?",
    "conditional_answers": {
      "A": "Answer if dropout is random: Completer outcomes may approximate all enrollees.",
      "B": "Answer if dropout is related to constraints or low baseline readiness: Use intent-to-treat or obtain administrative wage data for dropouts.",
      "C": "Answer if dropouts can be followed up: Measuring outcomes for all reduces attrition bias."
    },
    "wise_refusal": "I can’t estimate overall impact without outcomes (or credible imputations) for dropouts and evidence about why participants left.",
    "causal_structure": "Completion is selected post-enrollment; completers differ from dropouts in factors affecting Y.",
    "key_insight": "Attrition can make observed improvements unrepresentative of the full enrolled group.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-166",
    "_original_title": "Dropout Bias in a Workforce Program",
    "_questions": "Does the 20% wage increase among completers apply to all enrollees?\nHow can attrition bias the reported outcome?",
    "_expected_analysis": "Attrition bias: outcomes are observed only for completers.\nIf dropouts would have had lower gains, analyzing only completers overstates average improvement.\nConclusion: The claim “participants gained 20%” is INVALID without accounting for missing outcomes."
  },
  {
    "case_id": "T3-J1-L1-0167",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "A school district observes that students who attend private tutoring have higher math scores than students who do not. Administrators conclude tutoring causes higher scores.\n\nStudents who get tutoring are more likely to come from higher-income families with more educational resources at home.",
    "claim": "A school district observes that students who attend private tutoring have higher math scores than students who do not",
    "variables": {
      "X": {
        "name": "Tutoring attendance",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Math test score",
        "role": "Outcome"
      },
      "Z": {
        "name": "Socioeconomic resources",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Were family resources present before tutoring began, making them a baseline confounder?",
    "conditional_answers": {
      "A": "Answer if tutoring were randomly assigned: Score differences could be attributed to tutoring.",
      "B": "Answer if tutoring is purchased mostly by higher-resource families: Adjust for baseline resources and prior scores; residual confounding may remain.",
      "C": "Answer if comparing within the same school and income bracket: Confounding is reduced but not eliminated."
    },
    "wise_refusal": "I can’t make a causal claim without baseline scores, resource measures, and a credible strategy to compare similar students.",
    "causal_structure": "Z influences both X and Y, creating a spurious X–Y association.",
    "key_insight": "A common cause can make correlation look like causation.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-167",
    "_original_title": "Tutoring and Test Scores in Two Neighborhoods",
    "_questions": "Does the association between tutoring and scores prove tutoring causes gains?\nWhat socioeconomic confounders could explain both tutoring and higher scores?",
    "_expected_analysis": "Socioeconomic confounding: resources (Z) affect tutoring access (X) and test performance (Y).\nWithout controlling for Z or using a causal design, the tutoring–score gap is not a causal effect.\nConclusion: The claim “tutoring causes higher scores” is INVALID from the observational association alone."
  },
  {
    "case_id": "T3-J1-L1-0168",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Omitted Variable",
    "scenario": "A city analyst finds that days with higher ice cream sales in parks also have more reported minor injuries. They argue ice cream vendors create unsafe conditions.\n\nA parks manager suggests that hot weather increases both park attendance (creating more injury opportunities) and ice cream sales.",
    "claim": "days with higher ice cream sales in parks also have more reported minor injuries",
    "variables": {
      "X": {
        "name": "Ice cream sales",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Park injury reports",
        "role": "Outcome"
      },
      "Z": {
        "name": "Temperature / attendance",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Did temperature rise before both ice cream sales and injuries, suggesting a shared upstream cause?",
    "conditional_answers": {
      "A": "Answer if you control for temperature and attendance: The sales–injury correlation may disappear.",
      "B": "Answer if injuries rise on high-attendance days regardless of vendors: Vendor restriction is not justified by this correlation.",
      "C": "Answer if vendor placement is randomized across similar days: Comparisons become more informative."
    },
    "wise_refusal": "I can’t conclude causality without controlling for weather/attendance and knowing how vendor permits are assigned.",
    "causal_structure": "Z increases both X and Y; omitting Z yields misleading correlation.",
    "key_insight": "An omitted common cause can create a misleading association.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-168",
    "_original_title": "Ice Cream Sales and Park Injuries",
    "_questions": "Does the correlation between ice cream sales and injuries imply vendors cause injuries?\nWhat omitted variable could drive both ice cream sales and injuries?",
    "_expected_analysis": "Omitted-variable confounding: temperature/attendance (Z) increases both sales (X) and injuries (Y).\nCorrelation can appear even if vendors do not affect injuries.\nConclusion: The claim “ice cream sales cause injuries” is INVALID from the raw correlation."
  },
  {
    "case_id": "T3-J1-L1-0169",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Healthcare Administration",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Confounding by Indication",
    "scenario": "A hospital audit reports that patients prescribed a stronger painkiller have higher 30-day mortality than patients prescribed a milder painkiller. A supervisor argues the stronger drug is dangerous.\n\nClinicians respond that the stronger painkiller is typically prescribed to patients with more severe conditions, who are already at higher risk of death.",
    "claim": "patients prescribed a stronger painkiller have higher 30-day mortality than patients prescribed a milder painkiller",
    "variables": {
      "X": {
        "name": "Painkiller prescribed (strong vs. mild)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "30-day mortality",
        "role": "Outcome"
      },
      "Z": {
        "name": "Underlying illness severity",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was severity assessed before the prescription decision, and is it measured well enough to adjust for it?",
    "conditional_answers": {
      "A": "Answer if patients of similar severity are compared: The estimate is less confounded.",
      "B": "Answer if severity is poorly measured: Residual confounding remains even after adjustment.",
      "C": "Answer if prescribing follows a protocol unrelated to individual severity: Comparisons are closer to causal."
    },
    "wise_refusal": "I can’t infer a causal drug effect without detailed severity/comorbidity measures and timing of prescription relative to deterioration.",
    "causal_structure": "Severity (Z) influences both treatment choice (X) and mortality (Y).",
    "key_insight": "Sicker patients are more likely to receive stronger treatments, confounding naive comparisons.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-169",
    "_original_title": "Stronger Painkillers and Mortality",
    "_questions": "Does higher mortality among patients receiving the stronger painkiller imply the drug causes deaths?\nHow does confounding by indication distort the comparison?",
    "_expected_analysis": "Confounding by indication: severity (Z) drives prescribing (X) and mortality risk (Y).\nThe observed association may reflect baseline severity rather than a harmful drug effect.\nConclusion: The claim “the stronger drug increases mortality” is INVALID from this observational comparison alone."
  },
  {
    "case_id": "T3-J1-L2-0170",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A health system introduced telemedicine visits in some clinics while others stayed in-person only. A summary report claims telemedicine reduced follow-up adherence because telemedicine clinics show a lower overall rate of patients completing a recommended follow-up within 30 days.\n\nThe evaluation compares 30-day follow-up completion rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline chronic-disease burden of the clinic’s patient panel (high vs. low), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A health system introduced telemedicine visits in some clinics while others stayed in-person only",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "30-day follow-up completion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline chronic-disease burden of the clinic’s patient panel (high vs. low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline chronic-disease burden of the clinic’s patient panel (high vs. low) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each baseline chronic-disease burden of the clinic’s patient panel (high vs. low) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline chronic-disease burden of the clinic’s patient panel (high vs. low) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-170",
    "_original_title": "Telemedicine and Missed Follow-ups",
    "_questions": "If we expand the intervention, will 30-day follow-up completion rate improve?\nWhy does conditioning on baseline chronic-disease burden of the clinic’s patient panel (high vs. low) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0171",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A district piloted an after-school tutoring app in some middle schools. The district claims the app lowered math performance because app schools have lower average end-of-year math scores than non-app schools.\n\nThe evaluation compares average end-of-year math score in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline student achievement level (higher vs. lower prior-year scores), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A district piloted an after-school tutoring app in some middle schools",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "average end-of-year math score",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline student achievement level (higher vs. lower prior-year scores)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline student achievement level (higher vs. lower prior-year scores) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each baseline student achievement level (higher vs. lower prior-year scores) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline student achievement level (higher vs. lower prior-year scores) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-171",
    "_original_title": "After-School Tutoring App Results",
    "_questions": "If we expand the intervention, will average end-of-year math score improve?\nWhy does conditioning on baseline student achievement level (higher vs. lower prior-year scores) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0172",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A police department deployed body cameras in some precincts first. City leaders claim body cameras increased misconduct because camera precincts show higher overall citizen complaint rates than non-camera precincts during the evaluation period.\n\nThe evaluation compares citizen complaint rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline complaint environment of the precinct (historically high vs. low complaint rate), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A police department deployed body cameras in some precincts first",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "citizen complaint rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline complaint environment of the precinct (historically high vs. low complaint rate)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline complaint environment of the precinct (historically high vs. low complaint rate) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each baseline complaint environment of the precinct (historically high vs. low complaint rate) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline complaint environment of the precinct (historically high vs. low complaint rate) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-172",
    "_original_title": "Body Cameras and Citizen Complaints",
    "_questions": "If we expand the intervention, will citizen complaint rate improve?\nWhy does conditioning on baseline complaint environment of the precinct (historically high vs. low complaint rate) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0173",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Planning",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A city offered subsidized monthly transit passes at certain large worksites. A memo claims the subsidy increased commute times because subsidized sites show longer average commutes than non-subsidized sites.\n\nThe evaluation compares average door-to-door commute time in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by home-to-work distance category (short vs. long), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A city offered subsidized monthly transit passes at certain large worksites",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "average door-to-door commute time",
        "role": "Outcome"
      },
      "Z": {
        "name": "home-to-work distance category (short vs. long)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was home-to-work distance category (short vs. long) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each home-to-work distance category (short vs. long) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in home-to-work distance category (short vs. long) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-173",
    "_original_title": "Transit Pass Subsidy and Commute Times",
    "_questions": "If we expand the intervention, will average door-to-door commute time improve?\nWhy does conditioning on home-to-work distance category (short vs. long) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0174",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Business Operations",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A company added an automated chatbot to handle customer support for some product lines. Executives claim the chatbot reduced satisfaction because chatbot product lines have lower overall satisfaction scores than product lines without the chatbot.\n\nThe evaluation compares customer satisfaction score in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by issue complexity level (simple vs. complex tickets), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A company added an automated chatbot to handle customer support for some product lines",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "customer satisfaction score",
        "role": "Outcome"
      },
      "Z": {
        "name": "issue complexity level (simple vs. complex tickets)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was issue complexity level (simple vs. complex tickets) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each issue complexity level (simple vs. complex tickets) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in issue complexity level (simple vs. complex tickets) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-174",
    "_original_title": "Chatbot Support and Customer Satisfaction",
    "_questions": "If we expand the intervention, will customer satisfaction score improve?\nWhy does conditioning on issue complexity level (simple vs. complex tickets) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0175",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A firm allowed remote work in certain teams first. Leadership claims remote work reduced productivity because remote-eligible teams show lower overall weekly output than teams that remained on-site.\n\nThe evaluation compares weekly output per employee in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by role type (individual contributor vs. people manager), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A firm allowed remote work in certain teams first",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "weekly output per employee",
        "role": "Outcome"
      },
      "Z": {
        "name": "role type (individual contributor vs. people manager)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was role type (individual contributor vs. people manager) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each role type (individual contributor vs. people manager) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in role type (individual contributor vs. people manager) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-175",
    "_original_title": "Remote Work Policy and Output",
    "_questions": "If we expand the intervention, will weekly output per employee improve?\nWhy does conditioning on role type (individual contributor vs. people manager) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0176",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Environmental Policy",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A region piloted a carbon fee in some municipalities. A headline article claims the carbon fee increased emissions because fee municipalities show higher overall per-capita emissions than non-fee municipalities after the pilot begins.\n\nThe evaluation compares per-capita CO₂ emissions in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by industrial intensity of the municipality (high vs. low share of heavy industry), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A region piloted a carbon fee in some municipalities",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "per-capita CO₂ emissions",
        "role": "Outcome"
      },
      "Z": {
        "name": "industrial intensity of the municipality (high vs. low share of heavy industry)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was industrial intensity of the municipality (high vs. low share of heavy industry) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each industrial intensity of the municipality (high vs. low share of heavy industry) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in industrial intensity of the municipality (high vs. low share of heavy industry) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-176",
    "_original_title": "Carbon Fee Pilot and Emissions",
    "_questions": "If we expand the intervention, will per-capita CO₂ emissions improve?\nWhy does conditioning on industrial intensity of the municipality (high vs. low share of heavy industry) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0177",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A university introduced an optional pass/fail grading policy in some gateway courses. An internal report claims pass/fail reduced completion because pass/fail courses have a lower overall completion rate than comparable graded courses.\n\nThe evaluation compares course completion rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by course difficulty tier (hard vs. moderate), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A university introduced an optional pass/fail grading policy in some gateway courses",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "course completion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "course difficulty tier (hard vs. moderate)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was course difficulty tier (hard vs. moderate) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each course difficulty tier (hard vs. moderate) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in course difficulty tier (hard vs. moderate) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-177",
    "_original_title": "Pass/Fail Option and Course Completion",
    "_questions": "If we expand the intervention, will course completion rate improve?\nWhy does conditioning on course difficulty tier (hard vs. moderate) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0178",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Local Economic Development",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A city offered emergency microgrants to small businesses in selected corridors. A press release claims the grants did not help because grant-recipient corridors show lower overall one-year business survival than corridors without grants.\n\nThe evaluation compares one-year business survival rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline business fragility (low vs. high pre-grant revenue volatility), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "claim": "A city offered emergency microgrants to small businesses in selected corridors",
    "variables": {
      "X": {
        "name": "Intervention adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "one-year business survival rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline business fragility (low vs. high pre-grant revenue volatility)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline business fragility (low vs. high pre-grant revenue volatility) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "conditional_answers": {
      "A": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.",
      "B": "Answer if you compare within each baseline business fragility (low vs. high pre-grant revenue volatility) stratum or standardize: This addresses the imbalance and yields a more credible estimate.",
      "C": "Answer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly."
    },
    "wise_refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline business fragility (low vs. high pre-grant revenue volatility) between treated and untreated units.",
    "causal_structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
    "key_insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-178",
    "_original_title": "Microgrants and Small Business Survival",
    "_questions": "If we expand the intervention, will one-year business survival rate improve?\nWhy does conditioning on baseline business fragility (low vs. high pre-grant revenue volatility) reverse the aggregate conclusion?",
    "_expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z."
  },
  {
    "case_id": "T3-J1-L2-0179",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A city raises the minimum wage and later observes a higher rate of restaurant closures than in neighboring cities that did not raise wages. Commentators claim the wage increase caused closures.\n\nThe city that raised wages was already experiencing rapidly rising commercial rents and a decline in foot traffic due to major construction, both of which affect closure risk and also influenced the political push for wage reform.",
    "claim": "A city raises the minimum wage and later observes a higher rate of restaurant closures than in neighboring cities that did not raise wages",
    "variables": {
      "X": {
        "name": "minimum wage increase (raised vs. not)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "restaurant closure rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "commercial rent pressure and foot traffic trends",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was commercial rent pressure and foot traffic trends measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of minimum wage increase (raised vs. not) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if minimum wage increase (raised vs. not) is targeted to units with different baseline commercial rent pressure and foot traffic trends: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for minimum wage increase (raised vs. not) and credible measurement of commercial rent pressure and foot traffic trends (and other confounders).",
    "causal_structure": "Z → X and Z → Y; without blocking Z, X–Y comparison is confounded.",
    "key_insight": "Policy adoption can be correlated with underlying economic pressures that also affect outcomes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-179",
    "_original_title": "Minimum Wage Policy and Restaurant Closures",
    "_questions": "If we implement minimum wage increase (raised vs. not), will restaurant closure rate change?\nWhy might units receiving minimum wage increase (raised vs. not) differ from units not receiving it due to commercial rent pressure and foot traffic trends?",
    "_expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0180",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A district adopts a new reading curriculum in schools flagged as “at risk.” After one year, adopting schools have lower reading scores than non-adopting schools, and critics claim the curriculum harmed learning.\n\nAdoption was prioritized for schools with declining prior scores and higher poverty rates—factors that also predict future scores regardless of curriculum.",
    "claim": "A district adopts a new reading curriculum in schools flagged as “at risk",
    "variables": {
      "X": {
        "name": "new reading curriculum adoption (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "reading test scores",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline performance trend and student poverty rate",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline performance trend and student poverty rate measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of new reading curriculum adoption (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if new reading curriculum adoption (yes vs. no) is targeted to units with different baseline baseline performance trend and student poverty rate: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for new reading curriculum adoption (yes vs. no) and credible measurement of baseline performance trend and student poverty rate (and other confounders).",
    "causal_structure": "Z influences both adoption X and outcomes Y; treated schools start on different trajectories.",
    "key_insight": "Targeted interventions create treated groups that differ systematically from controls.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-180",
    "_original_title": "New Reading Curriculum and Test Scores",
    "_questions": "If we implement new reading curriculum adoption (yes vs. no), will reading test scores change?\nWhy might units receiving new reading curriculum adoption (yes vs. no) differ from units not receiving it due to baseline performance trend and student poverty rate?",
    "_expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0181",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A federal grant funds community policing in selected neighborhoods. A year later, funded neighborhoods show higher reported crime than unfunded neighborhoods, leading to claims that the grants increased crime.\n\nGrant selection prioritized neighborhoods with historically high crime and recent upward trends, which also predict future crime.",
    "claim": "the grants increased crime",
    "variables": {
      "X": {
        "name": "community policing grant funding (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "reported crime rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline crime level and pre-grant trend",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline crime level and pre-grant trend measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of community policing grant funding (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if community policing grant funding (yes vs. no) is targeted to units with different baseline baseline crime level and pre-grant trend: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for community policing grant funding (yes vs. no) and credible measurement of baseline crime level and pre-grant trend (and other confounders).",
    "causal_structure": "Z → X and Z → Y; selecting on need confounds causal interpretation.",
    "key_insight": "Comparing funded vs. unfunded sites confounds intervention with baseline risk.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-181",
    "_original_title": "Community Policing Grants and Crime",
    "_questions": "If we implement community policing grant funding (yes vs. no), will reported crime rate change?\nWhy might units receiving community policing grant funding (yes vs. no) differ from units not receiving it due to baseline crime level and pre-grant trend?",
    "_expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0182",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Behavioral Economics",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A grocery chain introduces front-of-package nutrition labels in some stores. Purchases of sugary snacks are higher in labeled stores, and an executive claims labels backfired.\n\nThe chain piloted labels first in dense urban stores with distinct customer baskets and higher baseline snack purchases.",
    "claim": "A grocery chain introduces front-of-package nutrition labels in some stores",
    "variables": {
      "X": {
        "name": "nutrition label rollout (store labeled vs. not)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "purchases of sugary snacks",
        "role": "Outcome"
      },
      "Z": {
        "name": "store neighborhood type and baseline basket patterns",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was store neighborhood type and baseline basket patterns measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of nutrition label rollout (store labeled vs. not) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if nutrition label rollout (store labeled vs. not) is targeted to units with different baseline store neighborhood type and baseline basket patterns: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for nutrition label rollout (store labeled vs. not) and credible measurement of store neighborhood type and baseline basket patterns (and other confounders).",
    "causal_structure": "Z affects store selection and Y; naive comparison attributes Z effects to labeling.",
    "key_insight": "Rollout choices can confound estimated behavioral impacts.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-182",
    "_original_title": "Nutrition Labels and Snack Purchases",
    "_questions": "If we implement nutrition label rollout (store labeled vs. not), will purchases of sugary snacks change?\nWhy might units receiving nutrition label rollout (store labeled vs. not) differ from units not receiving it due to store neighborhood type and baseline basket patterns?",
    "_expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0183",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A company offers a hybrid-work stipend to certain teams and later observes higher turnover in stipend teams. Management claims hybrid work drives attrition.\n\nThe stipend was offered first to teams undergoing reorganization and leadership turnover, which also increases attrition risk.",
    "claim": "A company offers a hybrid-work stipend to certain teams and later observes higher turnover in stipend teams",
    "variables": {
      "X": {
        "name": "hybrid-work stipend offered (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "employee turnover rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "team reorganization/leadership instability",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was team reorganization/leadership instability measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of hybrid-work stipend offered (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if hybrid-work stipend offered (yes vs. no) is targeted to units with different baseline team reorganization/leadership instability: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for hybrid-work stipend offered (yes vs. no) and credible measurement of team reorganization/leadership instability (and other confounders).",
    "causal_structure": "Z → X and Z → Y; treated teams face other shocks that raise turnover.",
    "key_insight": "Interventions often coincide with organizational changes that also affect outcomes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-183",
    "_original_title": "Hybrid Work Stipend and Turnover",
    "_questions": "If we implement hybrid-work stipend offered (yes vs. no), will employee turnover rate change?\nWhy might units receiving hybrid-work stipend offered (yes vs. no) differ from units not receiving it due to team reorganization/leadership instability?",
    "_expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0184",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A city expands protected bike lanes on selected commercial corridors and later sees lower retail sales on those corridors than on others. Critics claim bike lanes hurt businesses.\n\nBike lanes were prioritized for corridors already facing construction disruption and declining sales trends, which also predict future sales.",
    "claim": "A city expands protected bike lanes on selected commercial corridors and later sees lower retail sales on those corridors than on others",
    "variables": {
      "X": {
        "name": "protected bike lane expansion (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "retail sales",
        "role": "Outcome"
      },
      "Z": {
        "name": "baseline construction disruption and pre-policy sales trend",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was baseline construction disruption and pre-policy sales trend measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of protected bike lane expansion (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if protected bike lane expansion (yes vs. no) is targeted to units with different baseline baseline construction disruption and pre-policy sales trend: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for protected bike lane expansion (yes vs. no) and credible measurement of baseline construction disruption and pre-policy sales trend (and other confounders).",
    "causal_structure": "Z influences both which corridors get X and future Y.",
    "key_insight": "Targeting infrastructure projects to struggling corridors confounds causal claims about sales impacts.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-184",
    "_original_title": "Bike Lane Expansion and Retail Sales",
    "_questions": "If we implement protected bike lane expansion (yes vs. no), will retail sales change?\nWhy might units receiving protected bike lane expansion (yes vs. no) differ from units not receiving it due to baseline construction disruption and pre-policy sales trend?",
    "_expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0185",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Time-varying Confounding",
    "scenario": "A district uses an adaptive policy: students who score poorly on weekly quizzes are assigned additional tutoring hours the following week. Analysts find that students with more tutoring hours have smaller end-of-term learning gains and argue tutoring is ineffective.\n\nWeekly quiz scores (past outcomes) influence future tutoring assignment and also predict end-of-term gains.",
    "claim": "students with more tutoring hours have smaller end-of-term learning gains and argue tutoring is ineffective",
    "variables": {
      "X": {
        "name": "assigned tutoring hours (time-varying)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "end-of-term learning gains",
        "role": "Outcome"
      },
      "Z": {
        "name": "weekly quiz score history (past performance)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was weekly quiz score history (past performance) measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of assigned tutoring hours (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if assigned tutoring hours (time-varying) is targeted to units with different baseline weekly quiz score history (past performance): The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for assigned tutoring hours (time-varying) and credible measurement of weekly quiz score history (past performance) (and other confounders).",
    "causal_structure": "Past Y → future X and past Y → future Y; time-varying assignment creates confounding.",
    "key_insight": "When treatment responds to outcomes, naive correlations can reverse the true effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-185",
    "_original_title": "Adaptive Tutoring Hours and Learning Gains",
    "_questions": "If we implement assigned tutoring hours (time-varying), will end-of-term learning gains change?\nWhy might units receiving assigned tutoring hours (time-varying) differ from units not receiving it due to weekly quiz score history (past performance)?",
    "_expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0186",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Time-varying Confounding",
    "scenario": "A clinic increases outreach call frequency for patients who have not scheduled vaccinations. Later, patients who received more calls have lower vaccination rates, and a manager claims calls deter people.\n\nCall frequency rises when patients remain unvaccinated over time (past outcome), and those patients are harder to reach and less likely to vaccinate.",
    "claim": "A clinic increases outreach call frequency for patients who have not scheduled vaccinations",
    "variables": {
      "X": {
        "name": "outreach call intensity (time-varying)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "vaccination uptake",
        "role": "Outcome"
      },
      "Z": {
        "name": "prior vaccination status / past non-response",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was prior vaccination status / past non-response measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of outreach call intensity (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if outreach call intensity (time-varying) is targeted to units with different baseline prior vaccination status / past non-response: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for outreach call intensity (time-varying) and credible measurement of prior vaccination status / past non-response (and other confounders).",
    "causal_structure": "Past Y drives future X; past Y also predicts future Y.",
    "key_insight": "Reactive intensification of an intervention creates time-varying confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-186",
    "_original_title": "Outreach Calls and Vaccination Uptake",
    "_questions": "If we implement outreach call intensity (time-varying), will vaccination uptake change?\nWhy might units receiving outreach call intensity (time-varying) differ from units not receiving it due to prior vaccination status / past non-response?",
    "_expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0187",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Platform Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Time-varying Confounding",
    "scenario": "A ride-share platform displays “high-demand pricing” warnings more often on routes where cancellations are rising. Analysts observe that rides with warnings have higher cancellation rates and claim warnings cause cancellations.\n\nWarnings are triggered in response to congestion and earlier cancellation surges, which also predict future cancellations.",
    "claim": "A ride-share platform displays “high-demand pricing” warnings more often on routes where cancellations are rising",
    "variables": {
      "X": {
        "name": "displaying a surge-pricing warning (time-varying)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "ride cancellation rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "recent congestion and cancellation history",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was recent congestion and cancellation history measured before the intervention decision, and did it influence both adoption and outcomes?",
    "conditional_answers": {
      "A": "Answer if assignment of displaying a surge-pricing warning (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.",
      "B": "Answer if displaying a surge-pricing warning (time-varying) is targeted to units with different baseline recent congestion and cancellation history: The naive comparison is confounded; adjust or use quasi-experimental methods.",
      "C": "Answer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression."
    },
    "wise_refusal": "I can’t infer the causal effect without a clear assignment rule for displaying a surge-pricing warning (time-varying) and credible measurement of recent congestion and cancellation history (and other confounders).",
    "causal_structure": "Past demand conditions influence X and Y; time-varying confounding biases naive estimates.",
    "key_insight": "When interventions respond to worsening conditions, effects can be misattributed.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-187",
    "_original_title": "Surge Warnings and Ride Cancellations",
    "_questions": "If we implement displaying a surge-pricing warning (time-varying), will ride cancellation rate change?\nWhy might units receiving displaying a surge-pricing warning (time-varying) differ from units not receiving it due to recent congestion and cancellation history?",
    "_expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed."
  },
  {
    "case_id": "T3-J1-L2-0188",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Workplace Health",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A company offers a voluntary wellness program with weekly coaching. The HR report compares blood pressure changes only among employees who attended at least 8 of 10 sessions and concludes the program substantially lowers blood pressure.\n\nEmployees who miss sessions are excluded from the analysis, and attendance is affected by workload, baseline health, and motivation.",
    "claim": "A company offers a voluntary wellness program with weekly coaching",
    "variables": {
      "X": {
        "name": "wellness program enrollment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "blood pressure change",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into high attendance (8+ sessions) occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into high attendance (8+ sessions) is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by high attendance (8+ sessions) and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-188",
    "_original_title": "Wellness Program and Blood Pressure",
    "_questions": "Does wellness program enrollment cause a change in blood pressure change?\nHow can restricting the analysis to high attendance (8+ sessions) bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0189",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A university pairs scholarship recipients with mentors. The evaluation reports that mentored students graduate at higher rates, but it includes only students who met with their mentor at least once per month.\n\nStudents who miss meetings—often due to jobs or family obligations—are excluded from the graduation analysis.",
    "claim": "mentored students graduate at higher rates, but it includes only students who met with their mentor at least once per month",
    "variables": {
      "X": {
        "name": "mentorship program participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "graduation rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into regular mentor-meeting compliance occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into regular mentor-meeting compliance is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by regular mentor-meeting compliance and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-189",
    "_original_title": "Scholarship Mentoring and Graduation",
    "_questions": "Does mentorship program participation cause a change in graduation rate?\nHow can restricting the analysis to regular mentor-meeting compliance bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0190",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A workforce agency offers job placement workshops. The agency reports strong employment gains by comparing employment rates only among participants who completed the full workshop series.\n\nParticipants who drop out early are excluded, even though their employment outcomes may differ.",
    "claim": "A workforce agency offers job placement workshops",
    "variables": {
      "X": {
        "name": "workshop enrollment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "employment status after 3 months",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into workshop completion occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into workshop completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by workshop completion and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-190",
    "_original_title": "Job Placement Workshops and Employment",
    "_questions": "Does workshop enrollment cause a change in employment status after 3 months?\nHow can restricting the analysis to workshop completion bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0191",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city issues nutrition benefit cards. The city reports improved food security by surveying only households that used the card at least once per week.\n\nHouseholds that rarely used the card (due to access barriers or stigma) are excluded from the reported outcomes.",
    "claim": "A city issues nutrition benefit cards",
    "variables": {
      "X": {
        "name": "receiving a nutrition benefit card",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "food security score",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into frequent card usage occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into frequent card usage is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by frequent card usage and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-191",
    "_original_title": "Nutrition Benefit Cards and Food Security",
    "_questions": "Does receiving a nutrition benefit card cause a change in food security score?\nHow can restricting the analysis to frequent card usage bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0192",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A company introduces new onboarding modules. Managers claim the new onboarding reduces early attrition because employees who completed all modules had high 90-day retention.\n\nEmployees who did not complete modules are excluded from the retention calculation.",
    "claim": "A company introduces new onboarding modules",
    "variables": {
      "X": {
        "name": "new onboarding process",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "90-day retention",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into completion of onboarding modules occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into completion of onboarding modules is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by completion of onboarding modules and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-192",
    "_original_title": "New Onboarding Modules and Retention",
    "_questions": "Does new onboarding process cause a change in 90-day retention?\nHow can restricting the analysis to completion of onboarding modules bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0193",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city funds community gardens. A report claims gardens increase neighborhood cohesion because survey results are positive among residents who attended at least one garden event.\n\nResidents who never attended events are excluded from the survey analysis.",
    "claim": "A city funds community gardens",
    "variables": {
      "X": {
        "name": "community garden program",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "neighborhood cohesion index",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into event attendance occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into event attendance is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by event attendance and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-193",
    "_original_title": "Community Garden Events and Cohesion",
    "_questions": "Does community garden program cause a change in neighborhood cohesion index?\nHow can restricting the analysis to event attendance bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0194",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Behavioral Economics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A bank rolls out a financial literacy app. The bank claims the app increases savings because users who completed all lessons increased their savings balances.\n\nThe analysis excludes users who installed the app but did not finish lessons.",
    "claim": "A bank rolls out a financial literacy app",
    "variables": {
      "X": {
        "name": "financial literacy app rollout",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "savings balance change",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into lesson completion occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into lesson completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by lesson completion and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-194",
    "_original_title": "Financial Literacy App and Savings",
    "_questions": "Does financial literacy app rollout cause a change in savings balance change?\nHow can restricting the analysis to lesson completion bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0195",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Transportation Safety",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city gives away free bike helmets at transit hubs. The city reports lower cyclist injuries among those who registered their helmet pickup online, concluding the giveaway reduces injuries.\n\nCyclists who took helmets but did not register are excluded from injury tracking.",
    "claim": "A city gives away free bike helmets at transit hubs",
    "variables": {
      "X": {
        "name": "helmet giveaway program",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "cyclist injury rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into online registration of helmet pickup occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into online registration of helmet pickup is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by online registration of helmet pickup and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-195",
    "_original_title": "Bike Helmet Giveaway and Injuries",
    "_questions": "Does helmet giveaway program cause a change in cyclist injury rate?\nHow can restricting the analysis to online registration of helmet pickup bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0196",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A district offers optional teacher training. The district reports improved classroom observation scores among teachers who completed the training and submitted all follow-up reflections.\n\nTeachers who attended but did not submit reflections are excluded from the reported outcomes.",
    "claim": "A district offers optional teacher training",
    "variables": {
      "X": {
        "name": "teacher training program",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "classroom observation score",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into submission of required follow-up reflections occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into submission of required follow-up reflections is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by submission of required follow-up reflections and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-196",
    "_original_title": "Teacher Training and Observation Scores",
    "_questions": "Does teacher training program cause a change in classroom observation score?\nHow can restricting the analysis to submission of required follow-up reflections bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0197",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Housing Policy",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A county provides rent assistance. The county reports the program prevents evictions by analyzing only households that submitted all required documentation by the deadline.\n\nHouseholds missing paperwork are excluded from eviction outcome statistics.",
    "claim": "A county provides rent assistance",
    "variables": {
      "X": {
        "name": "rent assistance program",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "eviction rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into documentation completion occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into documentation completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by documentation completion and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-197",
    "_original_title": "Rent Assistance Documentation and Evictions",
    "_questions": "Does rent assistance program cause a change in eviction rate?\nHow can restricting the analysis to documentation completion bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0198",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Compliance",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A firm launches an internal fraud-reporting hotline. Leadership claims the hotline speeds up resolutions because hotline cases closed quickly.\n\nThe report includes only cases resolved within the quarter; unresolved cases are excluded.",
    "claim": "A firm launches an internal fraud-reporting hotline",
    "variables": {
      "X": {
        "name": "fraud-reporting hotline introduction",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "case resolution time",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into cases resolved within the quarter occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into cases resolved within the quarter is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by cases resolved within the quarter and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-198",
    "_original_title": "Fraud Hotline and Case Duration",
    "_questions": "Does fraud-reporting hotline introduction cause a change in case resolution time?\nHow can restricting the analysis to cases resolved within the quarter bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0199",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A school offers an after-school sports program to improve attendance. The school reports improved attendance among students who participated in at least 75% of practices.\n\nStudents who enrolled but rarely attended practices are excluded from the attendance comparison.",
    "claim": "A school offers an after-school sports program to improve attendance",
    "variables": {
      "X": {
        "name": "sports program enrollment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "school attendance rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into high practice participation occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into high practice participation is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by high practice participation and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-199",
    "_original_title": "After-School Sports and Attendance",
    "_questions": "Does sports program enrollment cause a change in school attendance rate?\nHow can restricting the analysis to high practice participation bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0200",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A clinic sends medication reminder texts. The clinic reports higher refill rates among patients who clicked the confirmation link in the texts.\n\nPatients who received texts but never clicked are excluded from the refill calculation.",
    "claim": "A clinic sends medication reminder texts",
    "variables": {
      "X": {
        "name": "text reminder program",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "medication refill rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into clicking the confirmation link occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into clicking the confirmation link is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by clicking the confirmation link and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-200",
    "_original_title": "Medication Reminder Texts and Refills",
    "_questions": "Does text reminder program cause a change in medication refill rate?\nHow can restricting the analysis to clicking the confirmation link bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0201",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A nonprofit funds coding bootcamp scholarships. The nonprofit reports large salary gains using only scholarship recipients who completed the bootcamp and self-reported job outcomes.\n\nRecipients who did not report outcomes are excluded from salary statistics.",
    "claim": "A nonprofit funds coding bootcamp scholarships",
    "variables": {
      "X": {
        "name": "bootcamp scholarship funding",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "post-bootcamp salary",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into completion plus outcome reporting occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into completion plus outcome reporting is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by completion plus outcome reporting and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-201",
    "_original_title": "Bootcamp Scholarships and Salary Outcomes",
    "_questions": "Does bootcamp scholarship funding cause a change in post-bootcamp salary?\nHow can restricting the analysis to completion plus outcome reporting bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0202",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city launches a neighborhood watch app. The city claims the app reduces crime because high-adoption neighborhoods show fewer incidents.\n\nThe evaluation excludes neighborhoods where adoption was low and relies heavily on app-based reporting.",
    "claim": "A city launches a neighborhood watch app",
    "variables": {
      "X": {
        "name": "neighborhood watch app rollout",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "reported crime incidents",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into active app usage / high adoption occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into active app usage / high adoption is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by active app usage / high adoption and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-202",
    "_original_title": "Neighborhood Watch App and Reported Crime",
    "_questions": "Does neighborhood watch app rollout cause a change in reported crime incidents?\nHow can restricting the analysis to active app usage / high adoption bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0203",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Transportation Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A transit agency launches a reliability initiative. The agency reports higher satisfaction by surveying riders who signed up for service-alert notifications.\n\nRiders who did not sign up are excluded from the satisfaction survey sample.",
    "claim": "A transit agency launches a reliability initiative",
    "variables": {
      "X": {
        "name": "reliability initiative",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "rider satisfaction",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into subscription to service alerts occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into subscription to service alerts is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by subscription to service alerts and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-203",
    "_original_title": "Transit Reliability Alerts and Satisfaction",
    "_questions": "Does reliability initiative cause a change in rider satisfaction?\nHow can restricting the analysis to subscription to service alerts bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0204",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Development Economics",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A microfinance organization offers microloans with optional coaching. The organization claims coaching improves repayment because borrowers who attended at least three sessions repaid at higher rates.\n\nBorrowers offered coaching but attending fewer sessions are excluded from the coached-group analysis.",
    "claim": "A microfinance organization offers microloans with optional coaching",
    "variables": {
      "X": {
        "name": "coaching add-on to microloans",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "loan repayment rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into attending 3+ coaching sessions occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into attending 3+ coaching sessions is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by attending 3+ coaching sessions and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-204",
    "_original_title": "Microloan Coaching and Repayment",
    "_questions": "Does coaching add-on to microloans cause a change in loan repayment rate?\nHow can restricting the analysis to attending 3+ coaching sessions bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0205",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A college redesigns orientation into a multi-day program. Administrators claim the new orientation improves academic performance because students who attended all days had higher first-year GPAs.\n\nStudents who missed days due to work or travel constraints are excluded from the “full-attendance” group.",
    "claim": "A college redesigns orientation into a multi-day program",
    "variables": {
      "X": {
        "name": "new multi-day orientation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "first-year GPA",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Does selection into full orientation attendance occur after the intervention could influence it, and is selection related to outcomes?",
    "conditional_answers": {
      "A": "Answer if selection into full orientation attendance is unrelated to outcomes (rare): Conditioning may not bias the estimate.",
      "B": "Answer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.",
      "C": "Answer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions."
    },
    "wise_refusal": "I can’t estimate the intervention effect without outcomes for those excluded by full orientation attendance and a clear picture of why selection differs between treated and untreated units.",
    "causal_structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
    "key_insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-205",
    "_original_title": "Multi-day Orientation and First-Year GPA",
    "_questions": "Does new multi-day orientation cause a change in first-year GPA?\nHow can restricting the analysis to full orientation attendance bias the estimated effect?",
    "_expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes."
  },
  {
    "case_id": "T3-J1-L2-0206",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A clinic waives medication co-pays for a subset of patients. Analysts compare blood-pressure outcomes only among patients who took at least 90% of doses (adherent patients) and find that those with the co-pay waiver have worse blood pressure control. They conclude the waiver harms outcomes.\n\nAdherence is influenced by both the waiver (making adherence easier) and patients’ underlying health-management capacity and stress, which also affect blood pressure.",
    "claim": "those with the co-pay waiver have worse blood pressure control",
    "variables": {
      "X": {
        "name": "co-pay waiver (yes vs. no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "blood pressure control",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (adherent (90%+ doses)) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become adherent (90%+ doses) and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-206",
    "_original_title": "Medication Co-pay Waiver Among Adherent Patients",
    "_questions": "Among those who are adherent (90%+ doses), does X appear to change Y?\nWhy can conditioning on adherent (90%+ doses) create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0207",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A county offers a parole support program. A report compares recidivism only among parolees who attended all required meetings and finds higher recidivism in the program group, concluding the program is ineffective.\n\nMeeting attendance depends on program assignment (some meetings are mandatory under the program) and on unobserved stability factors (transportation, housing), which also affect recidivism.",
    "claim": "A county offers a parole support program",
    "variables": {
      "X": {
        "name": "parole support program assignment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "recidivism",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (fully compliant with meetings) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become fully compliant with meetings and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-207",
    "_original_title": "Parole Support Program Among Those Who Attend Meetings",
    "_questions": "Among those who are fully compliant with meetings, does X appear to change Y?\nWhy can conditioning on fully compliant with meetings create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0208",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A university offers academic counseling to scholarship students. Evaluators compare GPA only among students who attended at least five counseling sessions and find that counseled students have lower GPAs, concluding counseling hurts performance.\n\nSession attendance is affected by counseling availability and by unobserved academic difficulty and motivation, which also influence GPA.",
    "claim": "counseled students have lower GPAs, concluding counseling hurts performance",
    "variables": {
      "X": {
        "name": "academic counseling offer",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "semester GPA",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (attended ≥5 sessions) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become attended ≥5 sessions and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-208",
    "_original_title": "Scholarship Counseling Among Students Who Show Up",
    "_questions": "Among those who are attended ≥5 sessions, does X appear to change Y?\nWhy can conditioning on attended ≥5 sessions create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0209",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A job-search platform introduces a new “smart recommendations” feature. Analysts compare job-offer rates only among users who were active weekly and find that users with the feature have lower offer rates, concluding the feature is harmful.\n\nWeekly activity is affected by feature exposure and by unobserved job-seeker urgency and constraints, which also affect job offers.",
    "claim": "users with the feature have lower offer rates, concluding the feature is harmful",
    "variables": {
      "X": {
        "name": "smart recommendations feature exposure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "job-offer rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (weekly active user) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become weekly active user and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-209",
    "_original_title": "Job Search Platform and Active Users",
    "_questions": "Among those who are weekly active user, does X appear to change Y?\nWhy can conditioning on weekly active user create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0210",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Environmental Policy",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A utility sends home energy reports to some households. The evaluation compares electricity use only among households that opened the emailed report and finds that treated households used more energy, suggesting reports backfire.\n\nEmail opening is influenced by being sent the report and by unobserved engagement levels and household routines that also affect electricity use.",
    "claim": "treated households used more energy, suggesting reports backfire",
    "variables": {
      "X": {
        "name": "receiving home energy report emails",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "electricity consumption",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (opened/read the report) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become opened/read the report and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-210",
    "_original_title": "Home Energy Reports Among Readers",
    "_questions": "Among those who are opened/read the report, does X appear to change Y?\nWhy can conditioning on opened/read the report create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0211",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A district offers teacher coaching. Analysts compare classroom observation scores only among teachers who completed all post-coaching surveys and find coaching teachers score worse, concluding coaching reduces performance.\n\nSurvey completion is influenced by coaching participation and by unobserved conscientiousness and workload, which also influence observation outcomes.",
    "claim": "A district offers teacher coaching",
    "variables": {
      "X": {
        "name": "teacher coaching participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "classroom observation score",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (completed all follow-up surveys) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become completed all follow-up surveys and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-211",
    "_original_title": "Teacher Coaching Among Participants Who Complete Surveys",
    "_questions": "Among those who are completed all follow-up surveys, does X appear to change Y?\nWhy can conditioning on completed all follow-up surveys create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0212",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Psychology",
    "difficulty": "Medium",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A campus provides a mental health app to students. A study compares stress scores only among students who used the app daily and finds higher stress among app users, concluding the app increases stress.\n\nDaily use is influenced by app access and by unobserved baseline stress and help-seeking behavior, which also predict later stress scores.",
    "claim": "A campus provides a mental health app to students",
    "variables": {
      "X": {
        "name": "app access/encouragement",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "reported stress score",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (daily app user) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become daily app user and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-212",
    "_original_title": "Mental Health App Among Daily Users",
    "_questions": "Among those who are daily app user, does X appear to change Y?\nWhy can conditioning on daily app user create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0213",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Transportation Policy",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A transit agency implements fare capping. Analysts compare satisfaction only among riders who took at least 20 trips per month and find lower satisfaction among capped-fare riders, concluding fare capping reduces satisfaction.\n\nHigh trip frequency is influenced by fare capping (making frequent riding cheaper) and by unobserved commuter dependence and route constraints, which also affect satisfaction.",
    "claim": "A transit agency implements fare capping",
    "variables": {
      "X": {
        "name": "fare-capping policy exposure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "rider satisfaction",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (frequent rider (≥20 trips/month)) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become frequent rider (≥20 trips/month) and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-213",
    "_original_title": "Transit Fare Capping Among Frequent Riders",
    "_questions": "Among those who are frequent rider (≥20 trips/month), does X appear to change Y?\nWhy can conditioning on frequent rider (≥20 trips/month) create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0214",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Policy",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A city offers community mediation for neighbor disputes. The evaluation compares conflict recurrence only among disputes that completed mediation sessions and finds higher recurrence when mediation was offered, concluding mediation worsens conflicts.\n\nCompletion depends on mediation offer (providing a path to completion) and on unobserved conflict intensity and willingness to compromise, which also affects recurrence.",
    "claim": "A city offers community mediation for neighbor disputes",
    "variables": {
      "X": {
        "name": "mediation offer",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "conflict recurrence rate",
        "role": "Outcome"
      }
    },
    "label": "NO",
    "hidden_question": "Did compliance/participation (completed mediation) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "conditional_answers": {
      "A": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.",
      "B": "Answer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.",
      "C": "Answer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy."
    },
    "wise_refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become completed mediation and whether those determinants also affect Y.",
    "causal_structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
    "key_insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-214",
    "_original_title": "Community Mediation Program Among Participants Who Complete Mediation",
    "_questions": "Among those who are completed mediation, does X appear to change Y?\nWhy can conditioning on completed mediation create a spurious association between X and Y?",
    "_expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L2-0215",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A professor uses an automated plagiarism detector that flags 2% of submissions. The professor says, “If the system flags you, you basically plagiarized,” and proposes an automatic penalty policy.\n\nIn reality, confirmed plagiarism is rare in this class, and the detector can produce false positives, especially on common template phrases.",
    "claim": "A professor uses an automated plagiarism detector that flags 2% of submissions",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (true plagiarism prevalence in the class) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-215",
    "_original_title": "Plagiarism Detector Flags in a Large Class",
    "_questions": "Given the signal (detector flag (positive)), what is the probability the underlying event is actually true?\nHow does the base rate (true plagiarism prevalence in the class) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates."
  },
  {
    "case_id": "T3-J1-L2-0216",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Safety",
    "difficulty": "Hard",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "An airport runs passengers through a watchlist system and gets a small number of alerts. A security officer claims, “An alert means the passenger is almost certainly dangerous.”\n\nThe true prevalence of dangerous individuals among passengers is extremely low, and the system can generate false positives due to name similarity.",
    "claim": "An airport runs passengers through a watchlist system and gets a small number of alerts",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (prevalence of dangerous individuals among passengers) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-216",
    "_original_title": "Terror Watchlist Alerts at an Airport",
    "_questions": "Given the signal (watchlist alert (positive)), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of dangerous individuals among passengers) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates."
  },
  {
    "case_id": "T3-J1-L2-0217",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Finance",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A payment platform’s fraud model flags transactions as “high risk.” A manager claims, “High-risk flagged transactions are almost always fraud,” and wants to auto-decline all flagged purchases.\n\nFraud is uncommon relative to total transactions, and the model can misclassify unusual but legitimate purchases.",
    "claim": "A payment platform’s fraud model flags transactions as “high risk",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (prevalence of fraud among all transactions) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-217",
    "_original_title": "Fraud Alerts in Online Payments",
    "_questions": "Given the signal (fraud-model high-risk flag), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of fraud among all transactions) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates."
  },
  {
    "case_id": "T3-J1-L2-0218",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A company offers screening for a rare disease. An employee tests positive and the HR office says, “A positive test means you probably have the disease,” and recommends immediate treatment.\n\nThe disease is very rare in the workforce, and the test has non-zero false positives.",
    "claim": "A company offers screening for a rare disease",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (prevalence of the disease in the screened population) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-218",
    "_original_title": "Rare Disease Screening in a Workplace",
    "_questions": "Given the signal (positive screening test), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of the disease in the screened population) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates."
  },
  {
    "case_id": "T3-J1-L2-0219",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Operations",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A factory installs a camera system that flags items as defective. A supervisor states, “If the camera flags an item, it’s defective,” and increases scrap rates.\n\nTrue defects are uncommon on this stabilized line, and the camera sometimes flags harmless cosmetic variations.",
    "claim": "A factory installs a camera system that flags items as defective",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (baseline defect prevalence on the line) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-219",
    "_original_title": "Defect Detection on a Production Line",
    "_questions": "Given the signal (camera defect flag), what is the probability the underlying event is actually true?\nHow does the base rate (baseline defect prevalence on the line) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates."
  },
  {
    "case_id": "T3-J1-L2-0220",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Workplace Policy",
    "difficulty": "Hard",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Conditional Fallacy",
    "scenario": "A workplace drug test is said to be “99% accurate.” Management argues that if a test is positive, the employee almost certainly used drugs.\n\nThis reasoning uses the test’s accuracy as if it directly gave P(Drug use | Positive), without considering the base rate of drug use in the tested workforce.",
    "claim": "A workplace drug test is said to be “99% accurate",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (drug-use prevalence in the workforce) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-220",
    "_original_title": "Interpreting a Positive Drug Test",
    "_questions": "Given the signal (positive drug test result), what is the probability the underlying event is actually true?\nHow does the base rate (drug-use prevalence in the workforce) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule."
  },
  {
    "case_id": "T3-J1-L2-0221",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Hard",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Conditional Fallacy",
    "scenario": "A company receives an anonymous harassment report. A manager says, “Most real harassers get reported, so if someone is reported they are likely guilty.”\n\nThis confuses the likelihood of a report given guilt with the probability of guilt given a report, and ignores how common false or ambiguous reports are relative to true cases.",
    "claim": "A company receives an anonymous harassment report",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (prevalence of actual harassment among employees) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-221",
    "_original_title": "Harassment Report and Guilt",
    "_questions": "Given the signal (being reported), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of actual harassment among employees) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule."
  },
  {
    "case_id": "T3-J1-L2-0222",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Conditional Fallacy",
    "scenario": "A predictive policing tool labels a neighborhood as “high risk.” An official argues, “High-risk labels are accurate because most high-crime neighborhoods get labeled high risk.”\n\nThis mixes up P(labeled | high crime) with P(high crime | labeled) and ignores how many neighborhoods receive labels relative to the true high-crime base rate.",
    "claim": "A predictive policing tool labels a neighborhood as “high risk",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (base rate of truly high-crime neighborhoods) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-222",
    "_original_title": "Predictive Policing “High-Risk” Label",
    "_questions": "Given the signal (high-risk label), what is the probability the underlying event is actually true?\nHow does the base rate (base rate of truly high-crime neighborhoods) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule."
  },
  {
    "case_id": "T3-J1-L2-0223",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Information Systems",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Conditional Fallacy",
    "scenario": "An email filter catches “95% of spam.” A user claims that any email sent to spam is almost certainly spam and deletes the folder regularly.\n\nThis uses P(sent to spam | spam) as if it were P(spam | sent to spam) and ignores the fraction of all emails that are spam and the false positive rate.",
    "claim": "any email sent to spam is almost certainly spam and deletes the folder regularly",
    "variables": {},
    "label": "NO",
    "hidden_question": "Were the base rate (base rate of spam among all incoming emails) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "conditional_answers": {
      "A": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.",
      "B": "Answer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.",
      "C": "Answer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition."
    },
    "wise_refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation.",
    "causal_structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
    "key_insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-223",
    "_original_title": "Spam Filter and Important Emails",
    "_questions": "Given the signal (email sent to spam folder), what is the probability the underlying event is actually true?\nHow does the base rate (base rate of spam among all incoming emails) affect interpretation of the signal?",
    "_expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule."
  },
  {
    "case_id": "T3-J1-L3-0224",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A university asks: “Would participants have graduated if they had not joined an intensive college-success program?” An analyst proposes comparing graduates among participants and non-participants only within the subset of students who report high academic confidence at the end of the first year.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic end-of-year academic confidence.",
    "claim": "A university asks: “Would participants have graduated if they had not joined an intensive college-success program?” An analyst proposes comparing graduates among participants and non-participants only...",
    "variables": {
      "X": {
        "name": "program participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "graduation",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does end-of-year academic confidence occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if end-of-year academic confidence is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if end-of-year academic confidence is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether end-of-year academic confidence is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-224",
    "_original_title": "College Program Counterfactual Using Post-Program Confidence",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on end-of-year academic confidence?\nWhy is end-of-year academic confidence a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0225",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A department asks: “Would officer complaints have fallen if we had not implemented a de-escalation training?” A report proposes comparing trained vs. untrained officers only among those who later report high commitment to de-escalation principles.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-training de-escalation commitment.",
    "claim": "A department asks: “Would officer complaints have fallen if we had not implemented a de-escalation training?” A report proposes comparing trained vs",
    "variables": {
      "X": {
        "name": "de-escalation training",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "citizen complaints",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does post-training de-escalation commitment occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if post-training de-escalation commitment is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if post-training de-escalation commitment is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether post-training de-escalation commitment is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-225",
    "_original_title": "Police Training Counterfactual Using Post-Training Attitudes",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-training de-escalation commitment?\nWhy is post-training de-escalation commitment a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0226",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Housing Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A city asks: “Would households have avoided eviction if we had not provided a one-time cash transfer?” An analyst suggests comparing treated and untreated households only among those who later have a high savings balance.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later savings balance.",
    "claim": "A city asks: “Would households have avoided eviction if we had not provided a one-time cash transfer?” An analyst suggests comparing treated and untreated households only among those who later have a ...",
    "variables": {
      "X": {
        "name": "cash transfer receipt",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "eviction occurrence",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does later savings balance occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if later savings balance is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if later savings balance is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether later savings balance is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-226",
    "_original_title": "Cash Transfer Counterfactual Using Later Savings",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on later savings balance?\nWhy is later savings balance a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0227",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A firm asks: “Would productivity have been higher if we had not moved to remote work?” A manager proposes comparing remote vs. on-site teams only among teams that later report high cohesion.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-policy team cohesion score.",
    "claim": "A firm asks: “Would productivity have been higher if we had not moved to remote work?” A manager proposes comparing remote vs",
    "variables": {
      "X": {
        "name": "remote-work policy exposure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "team productivity",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does post-policy team cohesion score occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if post-policy team cohesion score is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if post-policy team cohesion score is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether post-policy team cohesion score is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-227",
    "_original_title": "Remote Work Counterfactual Using Post-Policy Team Cohesion",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-policy team cohesion score?\nWhy is post-policy team cohesion score a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0228",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A nonprofit asks: “Would mentees have found jobs as quickly if they had not received mentoring?” The evaluation compares mentored vs. non-mentored applicants only among those who later report having a large professional network.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later professional network size.",
    "claim": "A nonprofit asks: “Would mentees have found jobs as quickly if they had not received mentoring?” The evaluation compares mentored vs",
    "variables": {
      "X": {
        "name": "mentoring participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "time to job offer",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does later professional network size occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if later professional network size is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if later professional network size is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether later professional network size is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-228",
    "_original_title": "Mentoring Counterfactual Using Later Network Size",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on later professional network size?\nWhy is later professional network size a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0229",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Transportation Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A city asks: “Would congestion have been worse if we had not subsidized transit?” An analyst compares subsidized vs. non-subsidized commuters only among those who became frequent transit riders afterward.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-subsidy ridership frequency.",
    "claim": "A city asks: “Would congestion have been worse if we had not subsidized transit?” An analyst compares subsidized vs",
    "variables": {
      "X": {
        "name": "transit subsidy exposure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "traffic congestion contribution",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does post-subsidy ridership frequency occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if post-subsidy ridership frequency is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if post-subsidy ridership frequency is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether post-subsidy ridership frequency is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-229",
    "_original_title": "Public Transit Subsidy Counterfactual Using Post-Subsidy Ridership",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-subsidy ridership frequency?\nWhy is post-subsidy ridership frequency a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0230",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Platform Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A platform asks: “Would toxicity have been higher if we had not changed the moderation policy?” A team proposes comparing communities with and without the policy only among communities that later exhibit high engagement.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later engagement level.",
    "claim": "A platform asks: “Would toxicity have been higher if we had not changed the moderation policy?” A team proposes comparing communities with and without the policy only among communities that later exhi...",
    "variables": {
      "X": {
        "name": "moderation policy change",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "toxicity rate",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does later engagement level occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if later engagement level is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if later engagement level is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether later engagement level is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-230",
    "_original_title": "Moderation Policy Counterfactual Using Later User Engagement",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on later engagement level?\nWhy is later engagement level a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0231",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A public health agency asks: “Would smoking rates have fallen without the campaign?” A report compares exposed vs. unexposed groups only among individuals who later report high motivation to quit.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-campaign motivation to quit.",
    "claim": "A public health agency asks: “Would smoking rates have fallen without the campaign?” A report compares exposed vs",
    "variables": {
      "X": {
        "name": "campaign exposure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "smoking cessation",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does post-campaign motivation to quit occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if post-campaign motivation to quit is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if post-campaign motivation to quit is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether post-campaign motivation to quit is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-231",
    "_original_title": "Smoking Cessation Campaign Counterfactual Using Post-Campaign Motivation",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-campaign motivation to quit?\nWhy is post-campaign motivation to quit a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0232",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A college asks: “Would scholarship recipients have maintained the same GPA without the scholarship?” The analysis compares scholarship and non-scholarship students only among those who later took a heavy course load.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-receipt course load intensity.",
    "claim": "A college asks: “Would scholarship recipients have maintained the same GPA without the scholarship?” The analysis compares scholarship and non-scholarship students only among those who later took a he...",
    "variables": {
      "X": {
        "name": "scholarship receipt",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "semester GPA",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does post-receipt course load intensity occur after X is assigned, meaning X can change M before Y is realized?",
    "conditional_answers": {
      "A": "Answer if post-receipt course load intensity is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).",
      "B": "Answer if post-receipt course load intensity is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.",
      "C": "Answer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M."
    },
    "wise_refusal": "I can’t answer the counterfactual without clarifying whether post-receipt course load intensity is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects).",
    "causal_structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
    "key_insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-232",
    "_original_title": "Scholarship Counterfactual Using Post-Scholarship Course Load",
    "_questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-receipt course load intensity?\nWhy is post-receipt course load intensity a cross-world confounder in this counterfactual comparison?",
    "_expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions)."
  },
  {
    "case_id": "T3-J1-L3-0233",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Housing Policy",
    "difficulty": "Hard",
    "trap_type": "T9",
    "trap_family": "",
    "trap_subtype": "Mediator Fixing Error",
    "scenario": "A county asks: “Would employment rise if we offered housing vouchers, but kept families’ neighborhood quality fixed at what it is now?” They argue this isolates the “voucher effect” that is not due to moving.\n\nIn practice, vouchers often enable moves to different neighborhoods. Neighborhood quality affects access to jobs, networks, transportation, and childcare, which all influence employment outcomes.",
    "claim": "A county asks: “Would employment rise if we offered housing vouchers, but kept families’ neighborhood quality fixed at what it is now?” They argue this isolates the “voucher effect” that is not due to...",
    "variables": {
      "X": {
        "name": "housing voucher receipt",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "employment status",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does M occur after X and before Y, making it a mediator rather than a baseline confounder?",
    "conditional_answers": {
      "A": "Answer if you want the total effect of X on Y: Do not fix M; compare Y under different X values.",
      "B": "Answer if you want a direct effect not through M: Use formal mediation analysis with explicit assumptions, or design an intervention that manipulates M.",
      "C": "Answer if M is actually pre-treatment: Then fixing M may be reasonable, but verify timing carefully."
    },
    "wise_refusal": "I can’t answer the “hold M fixed” counterfactual without clarifying whether M is post-treatment and specifying the mediation estimand and assumptions.",
    "causal_structure": "X → M → Y; attempting to fix M while changing X creates an ill-defined counterfactual unless mediation assumptions are made explicit.",
    "key_insight": "Holding a mediator fixed while changing the treatment can create an incoherent counterfactual world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-233",
    "_original_title": "Housing Vouchers and Employment: Fixing Neighborhood Quality",
    "_questions": "Is it valid to answer “What would Y be if we change X but keep M fixed?” when M is affected by X?\nWhat goes wrong when we try to fix the mediator to its observed value?",
    "_expected_analysis": "This is Confounder–Mediator Error (Mediator Fixing Error).\nM lies on the causal pathway from X to Y. The proposed counterfactual “change X while holding M fixed” is generally not identifiable from standard observational data and can be logically inconsistent with how M would respond to X.\nFixing M can create a counterfactual world that is incompatible with the causal system, leading to biased or undefined effects.\nConclusion: The proposed counterfactual is INVALID unless you define a well-posed mediation estimand and justify strong assumptions (or perform a suitable intervention on M)."
  },
  {
    "case_id": "T3-J1-L3-0234",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Hard",
    "trap_type": "T10",
    "trap_family": "",
    "trap_subtype": "Outcome-dependent Worlds",
    "scenario": "A manager asks: “Would Alex have been promoted if they hadn’t worked so much overtime?” The manager treats overtime as the cause and promotion as the effect.\n\nBut in this organization, employees are assigned overtime when leadership already expects they are promotion candidates and wants to test them under pressure. Overtime increases after early promotion signals appear.",
    "claim": "A manager asks: “Would Alex have been promoted if they hadn’t worked so much overtime?” The manager treats overtime as the cause and promotion as the effect",
    "variables": {
      "X": {
        "name": "working overtime",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "promotion decision",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Did promotion decision (or early indicators of it) occur before changes in working overtime, potentially causing X rather than being caused by it?",
    "conditional_answers": {
      "A": "Answer if X clearly occurs before Y and can be intervened on: Then the counterfactual is meaningful and can be analyzed with a causal model.",
      "B": "Answer if Y (or early Y signals) drives X: Then interpret the relationship as reverse causation; redesign the study to capture pre-outcome X.",
      "C": "Answer if you can instrument X with an exogenous shock: Then you may identify a causal effect with strong assumptions."
    },
    "wise_refusal": "I can’t assess the counterfactual without a clear timeline showing whether working overtime precedes promotion decision and whether X can be manipulated independently of Y.",
    "causal_structure": "Y (or early Y) → X; treating X as a cause leads to outcome-dependent counterfactuals.",
    "key_insight": "Counterfactuals are not valid when the “cause” is actually reacting to the outcome.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-234",
    "_original_title": "Overtime and Promotion: A Counterfactual Misorder (2)",
    "_questions": "Is the counterfactual “What would Y have been if X were different?” well-defined here?\nHow could the outcome (or anticipation of it) influence X, making the counterfactual world outcome-dependent?",
    "_expected_analysis": "This is L3 Reverse Causation (Outcome-dependent Worlds).\nThe setup suggests X is chosen or changes in response to the outcome (or strong early signals of the outcome), so imagining “set X differently” may implicitly change the underlying situation that produced Y.\nIf X is downstream of Y (or of early manifestations of Y), naive counterfactual comparisons can be ill-posed or misleading.\nConclusion: The proposed counterfactual claim is INVALID unless timing is clarified and a causal ordering that makes X antecedent to Y is justified."
  },
  {
    "case_id": "T3-J1-L3-0235",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T10",
    "trap_family": "",
    "trap_subtype": "Outcome-dependent Worlds",
    "scenario": "A city council asks: “Would crime have been lower if we had not increased patrols in Neighborhood Q?” A memo treats patrol increases as the cause of crime levels.\n\nPatrol hours are increased in response to early spikes in crime reports and calls for service, so rising crime drives deployment changes rather than the other way around.",
    "claim": "A city council asks: “Would crime have been lower if we had not increased patrols in Neighborhood Q?” A memo treats patrol increases as the cause of crime levels",
    "variables": {
      "X": {
        "name": "increased patrol deployment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "crime rate",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Did crime rate (or early indicators of it) occur before changes in increased patrol deployment, potentially causing X rather than being caused by it?",
    "conditional_answers": {
      "A": "Answer if X clearly occurs before Y and can be intervened on: Then the counterfactual is meaningful and can be analyzed with a causal model.",
      "B": "Answer if Y (or early Y signals) drives X: Then interpret the relationship as reverse causation; redesign the study to capture pre-outcome X.",
      "C": "Answer if you can instrument X with an exogenous shock: Then you may identify a causal effect with strong assumptions."
    },
    "wise_refusal": "I can’t assess the counterfactual without a clear timeline showing whether increased patrol deployment precedes crime rate and whether X can be manipulated independently of Y.",
    "causal_structure": "Y (or early Y) → X; treating X as a cause leads to outcome-dependent counterfactuals.",
    "key_insight": "Counterfactuals are not valid when the “cause” is actually reacting to the outcome.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-235",
    "_original_title": "Extra Policing and Crime: Reactive Deployment (2)",
    "_questions": "Is the counterfactual “What would Y have been if X were different?” well-defined here?\nHow could the outcome (or anticipation of it) influence X, making the counterfactual world outcome-dependent?",
    "_expected_analysis": "This is L3 Reverse Causation (Outcome-dependent Worlds).\nThe setup suggests X is chosen or changes in response to the outcome (or strong early signals of the outcome), so imagining “set X differently” may implicitly change the underlying situation that produced Y.\nIf X is downstream of Y (or of early manifestations of Y), naive counterfactual comparisons can be ill-posed or misleading.\nConclusion: The proposed counterfactual claim is INVALID unless timing is clarified and a causal ordering that makes X antecedent to Y is justified."
  },
  {
    "case_id": "T3-J1-L3-0236",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Platform Policy",
    "difficulty": "Hard",
    "trap_type": "T11",
    "trap_family": "",
    "trap_subtype": "Dynamic World Divergence",
    "scenario": "A platform asks: “If we had not changed the recommendation algorithm last year, would political polarization on the site be lower today?” The team wants to extrapolate from a short A/B test run for two weeks.\n\nHowever, the algorithm shapes what content users see, which changes who stays on the platform, how users post, and what content is produced. Those shifts then change future recommendation data and future exposure patterns.",
    "claim": "A platform asks: “If we had not changed the recommendation algorithm last year, would political polarization on the site be lower today?” The team wants to extrapolate from a short A/B test run for tw...",
    "variables": {
      "X": {
        "name": "recommendation algorithm change",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "polarization on the platform",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "Does user-content ecosystem and retention dynamics evolve after X changes and then influence future Y, creating path dependence?",
    "conditional_answers": {
      "A": "Answer if you only care about immediate effects before F adapts: A short-run causal estimate may be informative.",
      "B": "Answer if you care about long-run outcomes: Model the feedback dynamics explicitly; the counterfactual path under alternative X can diverge.",
      "C": "Answer if policy changes F in a way that changes who is exposed later: Then simple extrapolation from short-run data is unreliable."
    },
    "wise_refusal": "I can’t answer the long-run counterfactual without assumptions (or data) about how user-content ecosystem and retention dynamics evolves and how behavior adapts under different X values.",
    "causal_structure": "X changes system state F; F influences future exposure and outcomes; trajectories diverge across counterfactual worlds.",
    "key_insight": "With feedback, the alternative world is a different evolving system, not a one-shot swap of X.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-236",
    "_original_title": "Recommendation Algorithm and Polarization Over Time",
    "_questions": "Why can short-run estimates of changing X fail to predict long-run counterfactual Y under a different policy?\nHow does user-content ecosystem and retention dynamics create dynamic world divergence across counterfactual worlds?",
    "_expected_analysis": "This is L3 Feedback Loops (Dynamic World Divergence).\nChanging X alters the system state F over time, and F then changes future behaviors and outcomes, creating different trajectories under different counterfactual policies.\nA static comparison that ignores how the environment adapts can misstate “what would have happened” under an alternative X.\nConclusion: The counterfactual claim is CONDITIONAL: you need a dynamic causal model (or simulation/longitudinal design) that accounts for feedback."
  },
  {
    "case_id": "T3-J1-L3-0237",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T11",
    "trap_family": "",
    "trap_subtype": "Dynamic World Divergence",
    "scenario": "A city asks: “If we had used a less aggressive policing strategy, would community trust be higher after three years?” An analyst tries to answer using a one-time comparison of neighborhoods with different patrol styles.\n\nPolicing style changes residents’ willingness to report incidents and cooperate, which changes observed crime statistics and subsequent allocation decisions. The policy and the environment co-evolve over time.",
    "claim": "A city asks: “If we had used a less aggressive policing strategy, would community trust be higher after three years?” An analyst tries to answer using a one-time comparison of neighborhoods with diffe...",
    "variables": {
      "X": {
        "name": "policing strategy aggressiveness",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "community trust after three years",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "Does reporting/cooperation dynamics affecting future allocation evolve after X changes and then influence future Y, creating path dependence?",
    "conditional_answers": {
      "A": "Answer if you only care about immediate effects before F adapts: A short-run causal estimate may be informative.",
      "B": "Answer if you care about long-run outcomes: Model the feedback dynamics explicitly; the counterfactual path under alternative X can diverge.",
      "C": "Answer if policy changes F in a way that changes who is exposed later: Then simple extrapolation from short-run data is unreliable."
    },
    "wise_refusal": "I can’t answer the long-run counterfactual without assumptions (or data) about how reporting/cooperation dynamics affecting future allocation evolves and how behavior adapts under different X values.",
    "causal_structure": "X changes system state F; F influences future exposure and outcomes; trajectories diverge across counterfactual worlds.",
    "key_insight": "With feedback, the alternative world is a different evolving system, not a one-shot swap of X.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-237",
    "_original_title": "Policing Strategy and Community Trust Trajectories",
    "_questions": "Why can short-run estimates of changing X fail to predict long-run counterfactual Y under a different policy?\nHow does reporting/cooperation dynamics affecting future allocation create dynamic world divergence across counterfactual worlds?",
    "_expected_analysis": "This is L3 Feedback Loops (Dynamic World Divergence).\nChanging X alters the system state F over time, and F then changes future behaviors and outcomes, creating different trajectories under different counterfactual policies.\nA static comparison that ignores how the environment adapts can misstate “what would have happened” under an alternative X.\nConclusion: The counterfactual claim is CONDITIONAL: you need a dynamic causal model (or simulation/longitudinal design) that accounts for feedback."
  },
  {
    "case_id": "T3-J1-L3-0238",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Political Science",
    "difficulty": "Hard",
    "trap_type": "T12",
    "trap_family": "",
    "trap_subtype": "Early Preemption",
    "scenario": "A city asks: “If the primary protest organizer had been arrested the night before, would the protest have happened?” In the observed world, the organizer led the march and the protest occurred.\n\nMultiple groups were prepared to take over leadership. If the arrest had happened, a backup organizer who was already mobilizing might have led an alternative march that started earlier and still resulted in a protest.",
    "claim": "A city asks: “If the primary protest organizer had been arrested the night before, would the protest have happened?” In the observed world, the organizer led the march and the protest occurred",
    "variables": {
      "X": {
        "name": "arresting the primary organizer",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "a large protest occurs",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "If arresting the primary organizer were prevented, what alternative cause would have been most likely to produce Y, and would it occur earlier or later?",
    "conditional_answers": {
      "A": "Answer if no other sufficient cause exists: Then preventing X would prevent Y, so X is a but-for cause.",
      "B": "Answer if alternative sufficient causes exist: Y may still occur without X; you need a structural model of competing causes and timing.",
      "C": "Answer if timing determines which cause preempts the other: Identify whether X was early/late relative to the alternative cause to assess counterfactual dependence."
    },
    "wise_refusal": "I can’t decide whether X is a but-for cause without specifying competing causes and their timing; otherwise the counterfactual is underdetermined.",
    "causal_structure": "Two sufficient causes for Y; removing one may not change Y because the other would occur.",
    "key_insight": "Observed causation does not imply counterfactual dependence when multiple sufficient causes can produce the outcome.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-238",
    "_original_title": "Early Preemption in a Protest Outcome",
    "_questions": "If we prevent arresting the primary organizer, does Y necessarily change?\nHow does early preemption complicate attributing Y to X in the counterfactual world?",
    "_expected_analysis": "This is L3 Preemption (Early Preemption).\nThere are multiple sufficient causal paths to Y. Even if X occurs in the observed world, removing X in the counterfactual world may not change Y because an alternative cause would produce Y instead.\nAttribution requires modeling which cause would have fired in the absence of X (counterfactual dependence), not just observing that X happened.\nConclusion: The naive statement “X caused Y” is CONDITIONAL; it depends on whether Y counterfactually depends on X given competing causes."
  },
  {
    "case_id": "T3-J1-L3-0239",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Infrastructure Policy",
    "difficulty": "Hard",
    "trap_type": "T12",
    "trap_family": "",
    "trap_subtype": "Late Preemption",
    "scenario": "A utility asks: “If the first transformer had not failed, would the neighborhood still have lost power?” In the observed world, transformer A failed and an outage occurred.\n\nBut transformer B was already overheating due to the same heatwave. Even if transformer A had not failed, transformer B likely would have failed later that day, still producing an outage.",
    "claim": "A utility asks: “If the first transformer had not failed, would the neighborhood still have lost power?” In the observed world, transformer A failed and an outage occurred",
    "variables": {
      "X": {
        "name": "failure of transformer A",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "power outage occurs",
        "role": "Outcome"
      }
    },
    "label": "CONDITIONAL",
    "hidden_question": "If failure of transformer A were prevented, what alternative cause would have been most likely to produce Y, and would it occur earlier or later?",
    "conditional_answers": {
      "A": "Answer if no other sufficient cause exists: Then preventing X would prevent Y, so X is a but-for cause.",
      "B": "Answer if alternative sufficient causes exist: Y may still occur without X; you need a structural model of competing causes and timing.",
      "C": "Answer if timing determines which cause preempts the other: Identify whether X was early/late relative to the alternative cause to assess counterfactual dependence."
    },
    "wise_refusal": "I can’t decide whether X is a but-for cause without specifying competing causes and their timing; otherwise the counterfactual is underdetermined.",
    "causal_structure": "Two sufficient causes for Y; removing one may not change Y because the other would occur.",
    "key_insight": "Observed causation does not imply counterfactual dependence when multiple sufficient causes can produce the outcome.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-239",
    "_original_title": "Late Preemption in a Power Outage",
    "_questions": "If we prevent failure of transformer A, does Y necessarily change?\nHow does late preemption complicate attributing Y to X in the counterfactual world?",
    "_expected_analysis": "This is L3 Preemption (Late Preemption).\nThere are multiple sufficient causal paths to Y. Even if X occurs in the observed world, removing X in the counterfactual world may not change Y because an alternative cause would produce Y instead.\nAttribution requires modeling which cause would have fired in the absence of X (counterfactual dependence), not just observing that X happened.\nConclusion: The naive statement “X caused Y” is CONDITIONAL; it depends on whether Y counterfactually depends on X given competing causes."
  },
  {
    "case_id": "T3-J1-L3-0240",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T9",
    "trap_family": "",
    "trap_subtype": "Mediator Fixing Error",
    "scenario": "A policymaker asks: “Would employment increase if we offered job training, but kept participants’ confidence fixed at its current level?” They argue this isolates the ‘non-confidence’ part of training.\n\nTraining often changes participants’ confidence, and confidence itself affects job search intensity and employer interactions.",
    "claim": "A policymaker asks: “Would employment increase if we offered job training, but kept participants’ confidence fixed at its current level?” They argue this isolates the ‘non-confidence’ part of training...",
    "variables": {
      "X": {
        "name": "job training offer",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "employment probability",
        "role": "Outcome"
      }
    },
    "label": "INVALID",
    "hidden_question": "Does M occur after X and before Y, making it a mediator rather than a baseline confounder?",
    "conditional_answers": {
      "A": "Answer if you want the total effect of X on Y: Do not fix M; compare Y under different X values.",
      "B": "Answer if you want a direct effect not through M: Use formal mediation analysis with explicit assumptions, or design an intervention that manipulates M.",
      "C": "Answer if M is actually pre-treatment: Then fixing M may be reasonable, but verify timing carefully."
    },
    "wise_refusal": "I can’t answer the “hold M fixed” counterfactual without clarifying whether M is post-treatment and specifying the mediation estimand and assumptions.",
    "causal_structure": "X → M → Y; attempting to fix M while changing X creates an ill-defined counterfactual unless mediation assumptions are made explicit.",
    "key_insight": "Holding a mediator fixed while changing the treatment can create an incoherent counterfactual world.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-240",
    "_original_title": "Job Training, Confidence, and Employment: Fixing the Mediator",
    "_questions": "Is it valid to answer “What would Y be if we change X but keep M fixed?” when M is affected by X?\nWhat goes wrong when we try to fix the mediator to its observed value?",
    "_expected_analysis": "This is Confounder–Mediator Error (Mediator Fixing Error).\nM lies on the causal pathway from X to Y. The proposed counterfactual “change X while holding M fixed” is generally not identifiable from standard observational data and can be logically inconsistent with how M would respond to X.\nFixing M can create a counterfactual world that is incompatible with the causal system, leading to biased or undefined effects.\nConclusion: The proposed counterfactual is INVALID unless you define a well-posed mediation estimand and justify strong assumptions (or perform a suitable intervention on M)."
  },
  {
    "case_id": "T3-J1-L1-0001",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "A university releases a report stating that Department A has a higher overall graduation rate than Department B. Administrators conclude that Department A’s curriculum is more effective and consider expanding it.\nHowever, a faculty member notes that when graduation rates are broken down by student preparedness level (high vs. low incoming GPA), Department B has higher graduation rates in both groups.\nThe discrepancy arises because Department A enrolls a much larger proportion of high-preparedness students, while Department B enrolls more low-preparedness students overall.",
    "claim": "Department A’s curriculum is more effective and consider expanding it",
    "variables": {
      "X": {
        "name": "Department (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Graduation rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Student preparedness level (high / low incoming GPA)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was Student preparedness level (high / low incoming GPA) determined before Department (A vs. B) was chosen, and could Student preparedness level (high / low incoming GPA) have influenced the choice of Department (A vs. B) before Graduation rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Department (A vs. B) on Graduation rate may be reversed because the mix of subgroups differs between Department (A vs. B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Student preparedness level (high / low incoming GPA): Use the within-stratum differences (or a standardized effect). If Department (A vs. B) improves Graduation rate in each stratum, prefer Department (A vs. B) even if the aggregate looks worse.",
      "C": "Answer if Department (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Graduation rate by the key strata (e.g., Student preparedness level (high / low incoming GPA) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Department (A vs. B) is unevenly applied across strata.",
    "causal_structure": "Student preparedness (Z) affects graduation (Y) and differs in distribution across departments (X). Aggregating across Z reverses subgroup-level trends.",
    "key_insight": "Aggregate performance metrics can contradict subgroup-level performance due to population composition.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-01",
    "_original_title": "The Department Graduation Rates",
    "_questions": "Does the higher overall graduation rate of Department A imply that its curriculum is more effective than Department B’s?\nHow does conditioning on student preparedness level change the interpretation of the graduation data?",
    "_expected_analysis": "This case requires associational reasoning and identification of Simpson’s Paradox.\nKey reasoning step: Recognize that student preparedness (Z) is a confounding variable that strongly influences graduation outcomes and is unevenly distributed across departments.\nHidden temporal structure: Student preparedness is determined prior to department enrollment, so it cannot be caused by the department.\nSubgroup analysis: When conditioning on Z, Department B outperforms Department A for both preparedness levels.\nFailure mode: Inferring causal superiority of a curriculum from aggregate outcomes without stratification.\nCorrect conclusion:\nThe claim that Department A’s curriculum is superior is INVALID. The aggregate association reflects differences in student composition rather than instructional quality.\nWise refusal:\nA valid assessment of curricular effectiveness would require controlling for preparedness or using a causal design (e.g., randomized assignment or matched cohorts)."
  },
  {
    "case_id": "T3-J1-L1-0002",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Imbalanced Group Composition",
    "scenario": "A city publishes annual crime statistics showing that District East has a higher overall crime clearance rate than District West. City officials argue that policing strategies used in District East are more effective and consider expanding them citywide.\nHowever, analysts examining the data by crime severity (violent vs. non-violent offenses) find that District West has higher clearance rates for both categories.\nThe apparent contradiction arises because District East handles a much larger share of non-violent crimes, which are generally easier to solve, while District West deals disproportionately with violent crimes.",
    "claim": "District West has higher clearance rates for both categories",
    "variables": {
      "X": {
        "name": "Police district (East vs. West)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Crime clearance rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Crime severity (violent / non-violent)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was Crime severity (violent / non-violent) determined before Police district (East vs. West) was chosen, and could Crime severity (violent / non-violent) have influenced the choice of Police district (East vs. West) before Crime clearance rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Police district (East vs. West) on Crime clearance rate may be reversed because the mix of subgroups differs between Police district (East vs. West) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Crime severity (violent / non-violent): Use the within-stratum differences (or a standardized effect). If Police district (East vs. West) improves Crime clearance rate in each stratum, prefer Police district (East vs. West) even if the aggregate looks worse.",
      "C": "Answer if Police district (East vs. West) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Crime clearance rate by the key strata (e.g., Crime severity (violent / non-violent) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Police district (East vs. West) is unevenly applied across strata.",
    "causal_structure": "Crime severity (Z) affects clearance probability (Y) and differs in prevalence across districts (X). Aggregation across Z reverses subgroup-level performance.",
    "key_insight": "Aggregate performance metrics can mask inferior performance within every relevant subgroup.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-02",
    "_original_title": "The Crime Clearance Rates",
    "_questions": "Does the higher overall clearance rate in District East imply more effective policing than in District West?\nWhy does stratifying by crime severity reverse the apparent ranking between the districts?",
    "_expected_analysis": "This case requires associational reasoning and recognition of Simpson’s Paradox.\nKey reasoning step: Crime severity (Z) strongly influences clearance rates and is unevenly distributed across districts.\nHidden temporal structure: Crime severity is determined before police investigation begins and is not caused by district-level policing.\nSubgroup analysis: When violent and non-violent crimes are analyzed separately, District West outperforms District East in both categories.\nFailure mode: Interpreting aggregate clearance rates as evidence of policing effectiveness without accounting for case mix.\nCorrect conclusion:\nThe claim that District East’s policing strategy is superior is INVALID.\nWise refusal:\nValid comparisons require risk-adjusted or severity-adjusted clearance metrics rather than raw aggregates."
  },
  {
    "case_id": "T3-J1-L1-0003",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Technology",
    "difficulty": "Easy",
    "trap_type": "T8",
    "trap_family": "",
    "trap_subtype": "Aggregation Bias",
    "scenario": "An online education platform reports that Course X has a higher overall completion rate than Course Y. The platform promotes Course X as better designed and more engaging.\nHowever, when completion rates are examined by learner experience level (beginner vs. advanced), Course Y shows higher completion rates in both groups.\nThis occurs because Course X attracts a much larger proportion of advanced learners, who are more likely to complete any course regardless of design quality.",
    "claim": "Course X has a higher overall completion rate than Course Y",
    "variables": {
      "X": {
        "name": "Course (X vs. Y)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Course completion rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Learner experience level (beginner / advanced)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Was Learner experience level (beginner / advanced) determined before Course (X vs. Y) was chosen, and could Learner experience level (beginner / advanced) have influenced the choice of Course (X vs. Y) before Course completion rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Course (X vs. Y) on Course completion rate may be reversed because the mix of subgroups differs between Course (X vs. Y) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Learner experience level (beginner / advanced): Use the within-stratum differences (or a standardized effect). If Course (X vs. Y) improves Course completion rate in each stratum, prefer Course (X vs. Y) even if the aggregate looks worse.",
      "C": "Answer if Course (X vs. Y) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Course completion rate by the key strata (e.g., Learner experience level (beginner / advanced) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Course (X vs. Y) is unevenly applied across strata.",
    "causal_structure": "Learner experience (Z) affects completion (Y) and differs across courses (X), producing a reversal when aggregated.",
    "key_insight": "Differences in participant composition can dominate aggregate outcome measures.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-03",
    "_original_title": "The Online Course Completion Rates",
    "_questions": "Does Course X’s higher overall completion rate demonstrate superior instructional design?\nHow does learner experience level alter the interpretation of completion rates?",
    "_expected_analysis": "This case requires associational reasoning and identification of Simpson’s Paradox.\nKey reasoning step: Learner experience (Z) is a confounder that influences completion rates and is unevenly distributed across courses.\nHidden temporal structure: Learner experience is determined before course enrollment.\nSubgroup analysis: Course Y outperforms Course X among both beginners and advanced learners.\nFailure mode: Treating aggregate engagement metrics as causal indicators of quality.\nCorrect conclusion:\nThe claim that Course X is better designed is INVALID.\nWise refusal:\nCourse effectiveness should be evaluated within comparable learner groups or via randomized exposure."
  },
  {
    "case_id": "T3-J1-L1-0004",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Consumer Behavior",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "A company reports that 92% of customers are satisfied with its new subscription service, based on responses to a voluntary online satisfaction survey. Executives conclude that the service rollout has been a major success.\nHowever, internal data shows that only 15% of customers responded to the survey. Customer support logs indicate that dissatisfied users are more likely to cancel their subscriptions and disengage from company communications altogether.",
    "claim": "the service rollout has been a major success",
    "variables": {
      "X": {
        "name": "Survey response participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Reported customer satisfaction",
        "role": "Outcome"
      },
      "Z": {
        "name": "Customer satisfaction status (satisfied / dissatisfied)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "At what point were units selected into the observed sample—before or after Reported customer satisfaction occurred—and is selection related to Customer satisfaction status (satisfied / dissatisfied) or Reported customer satisfaction?",
    "conditional_answers": {
      "A": "Answer if Survey response participation is randomly assigned: A difference in Reported customer satisfaction across Survey response participation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Customer satisfaction status (satisfied / dissatisfied)): The Survey response participation vs not-Survey response participation difference in Reported customer satisfaction is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Customer satisfaction status (satisfied / dissatisfied)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Customer satisfaction status (satisfied / dissatisfied)); otherwise Survey response participation–Reported customer satisfaction differences may reflect selection rather than effect.",
    "causal_structure": "Customer satisfaction (Z) affects likelihood of survey participation (X), and the outcome (Y) is measured only among respondents.",
    "key_insight": "Observed satisfaction reflects who responds, not the true customer population.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-04",
    "_original_title": "The Customer Satisfaction Survey",
    "_questions": "Does the 92% satisfaction rate accurately represent overall customer sentiment?\nHow does voluntary participation affect the interpretation of survey results?",
    "_expected_analysis": "This case requires associational reasoning and identification of selection bias.\nKey reasoning step: Survey participation (X) is not random; it depends on satisfaction status (Z).\nHidden temporal structure: Satisfaction exists before the decision to respond to the survey.\nFailure mode: Treating respondent statistics as representative of the full population.\nCorrect conclusion:\nThe claim that most customers are satisfied is INVALID.\nWise refusal:\nValid inference would require correcting for non-response or using randomized sampling methods."
  },
  {
    "case_id": "T3-J1-L1-0005",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Entrepreneurship",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "A startup accelerator advertises that 70% of companies in its program succeed, defining success as raising a Series A funding round within two years. Aspiring founders interpret this statistic as evidence that participation dramatically increases startup success.\nHowever, the accelerator accepts only a small fraction of applicants and explicitly selects founders with strong prior experience, existing traction, and elite educational backgrounds.",
    "claim": "A startup accelerator advertises that 70% of companies in its program succeed, defining success as raising a Series A funding round within two years",
    "variables": {
      "X": {
        "name": "Accelerator participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Startup success (Series A funding)",
        "role": "Outcome"
      },
      "Z": {
        "name": "Founder quality / prior advantages",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "At what point were units selected into the observed sample—before or after Startup success (Series A funding) occurred—and is selection related to Founder quality / prior advantages or Startup success (Series A funding)?",
    "conditional_answers": {
      "A": "Answer if Accelerator participation is randomly assigned: A difference in Startup success (Series A funding) across Accelerator participation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Founder quality / prior advantages): The Accelerator participation vs not-Accelerator participation difference in Startup success (Series A funding) is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Founder quality / prior advantages) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Founder quality / prior advantages); otherwise Accelerator participation–Startup success (Series A funding) differences may reflect selection rather than effect.",
    "causal_structure": "Founder quality (Z) affects both selection into the accelerator (X) and startup success (Y).",
    "key_insight": "High success rates may reflect who is admitted rather than program effectiveness.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-05",
    "_original_title": "The Startup Accelerator Success Rate",
    "_questions": "Does the reported success rate imply that the accelerator causes startups to succeed?\nWhat role does the selection process play in shaping observed outcomes?",
    "_expected_analysis": "This case requires associational reasoning and identification of selection bias.\nKey reasoning step: Accelerator participation is correlated with success because both are driven by founder quality (Z).\nHidden temporal structure: Founder characteristics exist before accelerator admission.\nFailure mode: Inferring causal impact from outcomes among a selectively chosen group.\nCorrect conclusion:\nThe causal claim about accelerator effectiveness is INVALID.\nWise refusal:\nEstimating causal impact would require comparing accepted startups to comparable rejected applicants."
  },
  {
    "case_id": "T3-J1-L1-0006",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Workplace Health",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "",
    "trap_subtype": "Sampling-on-the-Outcome",
    "scenario": "A company reports that employees enrolled in its voluntary wellness program take fewer sick days than employees who do not enroll. Management concludes that the wellness program improves employee health and considers expanding it.\nFurther examination reveals that employees who opt into the program tend to be healthier, more health-conscious, and more engaged with company initiatives even before the program begins.",
    "claim": "the wellness program improves employee health and considers expanding it",
    "variables": {
      "X": {
        "name": "Wellness program participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Number of sick days taken",
        "role": "Outcome"
      },
      "Z": {
        "name": "Baseline employee health / engagement",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "At what point were units selected into the observed sample—before or after Number of sick days taken occurred—and is selection related to Baseline employee health / engagement or Number of sick days taken?",
    "conditional_answers": {
      "A": "Answer if Wellness program participation is randomly assigned: A difference in Number of sick days taken across Wellness program participation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Baseline employee health / engagement): The Wellness program participation vs not-Wellness program participation difference in Number of sick days taken is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Baseline employee health / engagement) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Baseline employee health / engagement); otherwise Wellness program participation–Number of sick days taken differences may reflect selection rather than effect.",
    "causal_structure": "Baseline health (Z) influences both program participation (X) and health outcomes (Y).",
    "key_insight": "Voluntary programs often attract participants who would perform better regardless.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-06",
    "_original_title": "The Employee Wellness Program",
    "_questions": "Can the difference in sick days be attributed to the wellness program itself?\nWhy does voluntary enrollment complicate causal interpretation?",
    "_expected_analysis": "This case requires associational reasoning and recognition of selection bias.\nKey reasoning step: Healthier employees are more likely to enroll, biasing comparisons.\nHidden temporal structure: Baseline health precedes enrollment.\nFailure mode: Mistaking correlation within a selected group for causal impact.\nCorrect conclusion:\nThe claim that the wellness program reduces sick days is INVALID.\nWise refusal:\nCausal evaluation would require random assignment or strong controls for baseline health."
  },
  {
    "case_id": "T3-J1-L1-0007",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Political Science",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "An analyst observes that states with higher average household income tend to vote for Party A in national elections. Based on this pattern, a commentator concludes that wealthier individuals are more likely to support Party A.\nHowever, individual-level polling data within states shows that higher-income individuals are actually more likely to support Party B, while lower-income individuals are more likely to support Party A.\nThe apparent contradiction arises because wealthier states differ from poorer states in urbanization, education levels, and industry composition.",
    "claim": "wealthier individuals are more likely to support Party A",
    "variables": {
      "X": {
        "name": "State-level average income",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Voting outcome (Party A vs. Party B)",
        "role": "Outcome"
      },
      "Z": {
        "name": "Individual income level",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is State-level average income measured at an aggregate level while Voting outcome (Party A vs. Party B) is an individual claim, and when/where does aggregation into Individual income level happen relative to measuring Voting outcome (Party A vs. Party B)?",
    "conditional_answers": {
      "A": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.",
      "B": "Answer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.",
      "C": "Answer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how State-level average income relates to Voting outcome (Party A vs. Party B) for a person.",
    "causal_structure": "State-level income aggregates over heterogeneous individuals; group-level correlations do not reflect individual-level behavior.",
    "key_insight": "Correlations observed at the group level cannot be assumed to hold at the individual level.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-07",
    "_original_title": "The Wealthy State, Poor Voter",
    "_questions": "Does the voting pattern of wealthy states imply that wealthy individuals support Party A?\nWhy can state-level correlations mislead conclusions about individual behavior?",
    "_expected_analysis": "This case requires associational reasoning and identification of the ecological fallacy.\nKey reasoning step: State-level income is an aggregate statistic that obscures within-state heterogeneity.\nHidden temporal structure: Individual income precedes voting decisions; aggregation occurs afterward.\nFailure mode: Inferring individual preferences from group-level data.\nCorrect conclusion:\nThe claim that wealthy individuals support Party A is INVALID.\nWise refusal:\nIndividual-level data is required to infer individual voting behavior; state averages are insufficient."
  },
  {
    "case_id": "T3-J1-L1-0008",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Education Sociology",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "A city education report shows that neighborhoods with higher average adult education levels have higher student test scores. Policymakers infer that students living in highly educated neighborhoods perform better academically.\nHowever, individual-level data reveals that students from less-educated households can perform as well as their peers when controlling for school quality and family support, regardless of neighborhood averages.\nThe neighborhood-level correlation reflects broader structural differences rather than individual household effects.",
    "claim": "A city education report shows that neighborhoods with higher average adult education levels have higher student test scores",
    "variables": {
      "X": {
        "name": "Neighborhood average education level",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Student test scores",
        "role": "Outcome"
      },
      "Z": {
        "name": "Household-level educational support",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is Neighborhood average education level measured at an aggregate level while Student test scores is an individual claim, and when/where does aggregation into Household-level educational support happen relative to measuring Student test scores?",
    "conditional_answers": {
      "A": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.",
      "B": "Answer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.",
      "C": "Answer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Neighborhood average education level relates to Student test scores for a person.",
    "causal_structure": "Neighborhood averages mask variation across households; individual outcomes depend on household-level factors.",
    "key_insight": "Group characteristics do not deterministically apply to individuals within the group.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-08",
    "_original_title": "Neighborhood Education and Student Achievement",
    "_questions": "Can neighborhood education averages be used to predict individual student performance?\nWhy does this inference risk ecological fallacy?",
    "_expected_analysis": "This case requires associational reasoning and recognition of the ecological fallacy.\nKey reasoning step: Neighborhood-level statistics aggregate diverse households.\nHidden temporal structure: Household characteristics precede both neighborhood averages and student outcomes.\nFailure mode: Assuming individuals inherit group-level attributes.\nCorrect conclusion:\nThe inference about individual performance is INVALID.\nWise refusal:\nIndividual-level causal claims require individual-level data."
  },
  {
    "case_id": "T3-J1-L1-0009",
    "pearl_level": "L1",
    "domain": "D10",
    "subdomain": "Sociology",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Socioeconomic",
    "scenario": "International surveys show that Country A ranks higher than Country B on average happiness scores. Commentators argue that citizens of Country A are happier in their daily lives.\nHowever, within both countries, individual happiness varies widely, and individual-level analysis shows that factors such as income security, health, and social relationships are stronger predictors of personal well-being than nationality.\nThe national ranking reflects aggregated survey responses rather than uniform individual experiences.",
    "claim": "International surveys show that Country A ranks higher than Country B on average happiness scores",
    "variables": {
      "X": {
        "name": "Country of residence",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Average happiness score",
        "role": "Outcome"
      },
      "Z": {
        "name": "Individual-level well-being determinants",
        "role": "Confounder/Mediator"
      }
    },
    "label": "S",
    "hidden_question": "Is Country of residence measured at an aggregate level while Average happiness score is an individual claim, and when/where does aggregation into Individual-level well-being determinants happen relative to measuring Average happiness score?",
    "conditional_answers": {
      "A": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.",
      "B": "Answer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.",
      "C": "Answer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Country of residence relates to Average happiness score for a person.",
    "causal_structure": "National averages collapse heterogeneous individual experiences into a single metric.",
    "key_insight": "Country-level statistics cannot substitute for individual-level causal explanations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-09",
    "_original_title": "National Happiness and Personal Well-Being",
    "_questions": "Does a higher national happiness score imply that every individual is happier?\nWhy is this reasoning flawed?",
    "_expected_analysis": "This case requires associational reasoning and identification of the ecological fallacy.\nKey reasoning step: National happiness scores are averages, not individual guarantees.\nHidden temporal structure: Individual experiences determine survey responses; aggregation follows.\nFailure mode: Treating aggregate indicators as individual truths.\nCorrect conclusion:\nThe claim that individuals in Country A are happier is INVALID.\nWise refusal:\nIndividual well-being must be assessed at the individual level, not inferred from national averages."
  },
  {
    "case_id": "T3-J1-L2-0010",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A hospital system compares two treatments for a chronic condition: Treatment A (new) and Treatment B (standard). Hospital leadership reports that Treatment A has a higher overall recovery rate and decides to adopt it system-wide.\nHowever, when outcomes are analyzed separately for mild cases and severe cases, Treatment B has a higher recovery rate in both severity groups.\nThe apparent superiority of Treatment A arises because it is used far more often for mild cases, while Treatment B is disproportionately used for severe cases.",
    "claim": "Treatment A has a higher overall recovery rate and decides to adopt it system-wide",
    "variables": {
      "X": {
        "name": "Treatment type (A vs. B)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Recovery outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Disease severity (mild / severe)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Disease severity (mild / severe) determined before Treatment type (A vs. B) was chosen, and could Disease severity (mild / severe) have influenced the choice of Treatment type (A vs. B) before Recovery outcome was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Treatment type (A vs. B) on Recovery outcome may be reversed because the mix of subgroups differs between Treatment type (A vs. B) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Disease severity (mild / severe): Use the within-stratum differences (or a standardized effect). If Treatment type (A vs. B) improves Recovery outcome in each stratum, prefer Treatment type (A vs. B) even if the aggregate looks worse.",
      "C": "Answer if Treatment type (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Recovery outcome by the key strata (e.g., Disease severity (mild / severe) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Treatment type (A vs. B) is unevenly applied across strata.",
    "causal_structure": "Disease severity (Z) influences both treatment assignment (X) and recovery (Y). Aggregating outcomes across severity levels reverses the treatment comparison.",
    "key_insight": "An intervention may appear effective overall while being inferior within every relevant subgroup.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-10",
    "_original_title": "The New Medical Treatment Rollout",
    "_questions": "If the hospital switches all patients to Treatment A, should it expect higher recovery rates?\nWhy does stratifying by disease severity reverse the apparent treatment effect?",
    "_expected_analysis": "This case requires interventional reasoning and identification of Simpson’s Paradox under intervention.\nKey reasoning step: Disease severity (Z) is a pre-treatment variable that affects both treatment choice and outcomes.\nIntervention framing: The question concerns the effect of doing X = Treatment A for all patients.\nSubgroup analysis: Within both mild and severe cases, Treatment B yields better recovery outcomes.\nFailure mode: Inferring that an intervention is beneficial based on aggregate observational success rates.\nCorrect conclusion:\nAdopting Treatment A system-wide is INVALID based on the given evidence.\nWise refusal:\nA valid policy decision would require randomized assignment or severity-adjusted comparisons before intervention."
  },
  {
    "case_id": "T3-J1-L2-0011",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A school district pilots a class size reduction program in several schools and reports that schools with smaller classes have higher average test scores. Based on these results, district officials propose expanding the program to all schools.\nHowever, when test scores are analyzed separately for high-performing schools and low-performing schools, schools without class size reductions outperform those with reductions in both categories.\nThis discrepancy arises because class size reductions were primarily implemented in already high-performing schools, while struggling schools retained larger classes.",
    "claim": "schools with smaller classes have higher average test scores",
    "variables": {
      "X": {
        "name": "Class size intervention (reduced vs. standard)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Student test scores",
        "role": "Outcome"
      },
      "Z": {
        "name": "Baseline school performance (high / low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Baseline school performance (high / low) determined before Class size intervention (reduced vs. standard) was chosen, and could Baseline school performance (high / low) have influenced the choice of Class size intervention (reduced vs. standard) before Student test scores was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Class size intervention (reduced vs. standard) on Student test scores may be reversed because the mix of subgroups differs between Class size intervention (reduced vs. standard) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Baseline school performance (high / low): Use the within-stratum differences (or a standardized effect). If Class size intervention (reduced vs. standard) improves Student test scores in each stratum, prefer Class size intervention (reduced vs. standard) even if the aggregate looks worse.",
      "C": "Answer if Class size intervention (reduced vs. standard) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Student test scores by the key strata (e.g., Baseline school performance (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Class size intervention (reduced vs. standard) is unevenly applied across strata.",
    "causal_structure": "Baseline school performance (Z) affects both likelihood of receiving the intervention (X) and student outcomes (Y), leading to aggregate reversal.",
    "key_insight": "An intervention’s apparent success may reflect where it was implemented rather than its causal effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-11",
    "_original_title": "The Class Size Reduction Program",
    "_questions": "Would expanding class size reduction to all schools likely improve test scores?\nWhy does conditioning on baseline school performance change the interpretation?",
    "_expected_analysis": "This case requires interventional reasoning and recognition of Simpson’s Paradox.\nKey reasoning step: Baseline performance (Z) is determined before intervention and strongly influences outcomes.\nIntervention framing: The policy question is about the effect of doing X = reducing class sizes everywhere.\nSubgroup analysis: Within both high- and low-performing schools, non-reduced classes perform better.\nFailure mode: Mistaking correlation between intervention presence and outcomes for causal effect.\nCorrect conclusion:\nThe proposal to expand the program is INVALID based on current evidence.\nWise refusal:\nRandomized rollout or matched comparisons are needed to estimate the true effect of class size reduction."
  },
  {
    "case_id": "T3-J1-L2-0012",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A corporation pilots a productivity training program across several departments. Management reports that employees who received the training show higher average productivity scores than those who did not, and proposes mandatory rollout.\nWhen productivity is analyzed separately for junior and senior employees, however, untrained employees outperform trained employees in both groups.\nThe discrepancy arises because the training was offered primarily to senior employees, who are more productive on average regardless of training.",
    "claim": "employees who received the training show higher average productivity scores than those who did not, and proposes mandatory rollout",
    "variables": {
      "X": {
        "name": "Training participation (yes / no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Productivity score",
        "role": "Outcome"
      },
      "Z": {
        "name": "Employee seniority (junior / senior)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Employee seniority (junior / senior) determined before Training participation (yes / no) was chosen, and could Employee seniority (junior / senior) have influenced the choice of Training participation (yes / no) before Productivity score was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Training participation (yes / no) on Productivity score may be reversed because the mix of subgroups differs between Training participation (yes / no) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Employee seniority (junior / senior): Use the within-stratum differences (or a standardized effect). If Training participation (yes / no) improves Productivity score in each stratum, prefer Training participation (yes / no) even if the aggregate looks worse.",
      "C": "Answer if Training participation (yes / no) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Productivity score by the key strata (e.g., Employee seniority (junior / senior) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Training participation (yes / no) is unevenly applied across strata.",
    "causal_structure": "Seniority (Z) affects both training participation (X) and productivity (Y), reversing subgroup trends when aggregated.",
    "key_insight": "Aggregate productivity gains may reflect employee mix rather than training impact.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-12",
    "_original_title": "The Productivity Training Initiative",
    "_questions": "Should the company mandate the training program for all employees?\nWhy does separating employees by seniority reverse the apparent effect?",
    "_expected_analysis": "Seniority precedes training and productivity.\nWithin both junior and senior groups, training is associated with lower productivity.\nAggregate improvement is driven by overrepresentation of senior employees in the trained group.\nConclusion: Mandatory rollout is INVALID based on current evidence.\nWise refusal: A randomized or staggered rollout is required to estimate causal impact."
  },
  {
    "case_id": "T3-J1-L2-0013",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A police department introduces a predictive analytics tool and observes that precincts using the tool report higher crime resolution rates. City leaders plan to deploy the tool across all precincts.\nHowever, when outcomes are examined separately for high-crime and low-crime precincts, precincts without the tool show higher resolution rates in both categories.\nThe tool was initially deployed in low-crime precincts where resolution rates are naturally higher.",
    "claim": "A police department introduces a predictive analytics tool and observes that precincts using the tool report higher crime resolution rates",
    "variables": {
      "X": {
        "name": "Predictive tool deployment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Crime resolution rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Baseline precinct crime rate (high / low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Baseline precinct crime rate (high / low) determined before Predictive tool deployment was chosen, and could Baseline precinct crime rate (high / low) have influenced the choice of Predictive tool deployment before Crime resolution rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Predictive tool deployment on Crime resolution rate may be reversed because the mix of subgroups differs between Predictive tool deployment arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Baseline precinct crime rate (high / low): Use the within-stratum differences (or a standardized effect). If Predictive tool deployment improves Crime resolution rate in each stratum, prefer Predictive tool deployment even if the aggregate looks worse.",
      "C": "Answer if Predictive tool deployment can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Crime resolution rate by the key strata (e.g., Baseline precinct crime rate (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Predictive tool deployment is unevenly applied across strata.",
    "causal_structure": "Baseline crime rate (Z) affects both deployment (X) and outcomes (Y), leading to misleading aggregates.",
    "key_insight": "Apparent intervention success may reflect selective deployment.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-13",
    "_original_title": "The Digital Policing Tool",
    "_questions": "Will deploying the tool citywide improve crime resolution?\nWhy does conditioning on baseline crime rate change the conclusion?",
    "_expected_analysis": "Baseline crime rate is pre-intervention.\nWithin both high- and low-crime precincts, non-tool precincts perform better.\nAggregate benefit reflects deployment pattern, not tool efficacy.\nConclusion: Citywide deployment is INVALID.\nWise refusal: Proper evaluation requires randomized precinct assignment."
  },
  {
    "case_id": "T3-J1-L2-0014",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Marketing Analytics",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A company runs a new marketing campaign and reports that customers exposed to it have a higher overall conversion rate. The campaign is labeled a success and expanded nationally.\nWhen analyzed by customer purchasing power (high vs. low), however, customers not exposed to the campaign convert at higher rates in both segments.\nThe campaign was targeted primarily at high-spending customers, inflating aggregate performance.",
    "claim": "customers exposed to it have a higher overall conversion rate",
    "variables": {
      "X": {
        "name": "Campaign exposure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Purchase conversion",
        "role": "Outcome"
      },
      "Z": {
        "name": "Customer purchasing power (high / low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Customer purchasing power (high / low) determined before Campaign exposure was chosen, and could Customer purchasing power (high / low) have influenced the choice of Campaign exposure before Purchase conversion was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Campaign exposure on Purchase conversion may be reversed because the mix of subgroups differs between Campaign exposure arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Customer purchasing power (high / low): Use the within-stratum differences (or a standardized effect). If Campaign exposure improves Purchase conversion in each stratum, prefer Campaign exposure even if the aggregate looks worse.",
      "C": "Answer if Campaign exposure can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Purchase conversion by the key strata (e.g., Customer purchasing power (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Campaign exposure is unevenly applied across strata.",
    "causal_structure": "Purchasing power (Z) influences both exposure (X) and conversion (Y), reversing subgroup effects.",
    "key_insight": "Targeted interventions can distort aggregate success metrics.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-14",
    "_original_title": "The Marketing Campaign Conversion Rates",
    "_questions": "Does the campaign cause higher conversion rates?\nHow does targeting affect causal interpretation?",
    "_expected_analysis": "Purchasing power precedes exposure and conversion.\nWithin each purchasing group, the campaign underperforms.\nAggregate success is driven by customer mix.\nConclusion: Expansion decision is INVALID.\nWise refusal: A/B testing across comparable customers is required."
  },
  {
    "case_id": "T3-J1-L2-0015",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A government issues technology grants to schools and finds that grant-receiving schools show higher average student performance. Officials propose expanding funding.\nWhen performance is analyzed separately for urban and rural schools, non-recipient schools outperform recipients in both categories.\nThe grants were disproportionately awarded to urban schools, which already have stronger academic performance.",
    "claim": "grant-receiving schools show higher average student performance",
    "variables": {
      "X": {
        "name": "Grant receipt",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Student performance",
        "role": "Outcome"
      },
      "Z": {
        "name": "School location (urban / rural)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was School location (urban / rural) determined before Grant receipt was chosen, and could School location (urban / rural) have influenced the choice of Grant receipt before Student performance was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Grant receipt on Student performance may be reversed because the mix of subgroups differs between Grant receipt arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by School location (urban / rural): Use the within-stratum differences (or a standardized effect). If Grant receipt improves Student performance in each stratum, prefer Grant receipt even if the aggregate looks worse.",
      "C": "Answer if Grant receipt can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Student performance by the key strata (e.g., School location (urban / rural) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Grant receipt is unevenly applied across strata.",
    "causal_structure": "Location (Z) affects both grant allocation (X) and outcomes (Y), producing aggregate reversal.",
    "key_insight": "Geographic imbalance can dominate apparent policy effects.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-15",
    "_original_title": "The School Technology Grant",
    "_questions": "Should the grant program be expanded nationwide?\nWhy does stratifying by school location alter the conclusion?",
    "_expected_analysis": "School location is fixed prior to grant allocation.\nWithin both urban and rural strata, non-recipient schools perform better.\nAggregate effect reflects allocation bias.\nConclusion: Expansion is INVALID.\nWise refusal: Causal evaluation requires randomized or needs-based allocation.\nL2-A (Simpson’s Paradox under intervention)"
  },
  {
    "case_id": "T3-J1-L2-0016",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Health Policy",
    "difficulty": "Hard",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A hospital increases nurse-to-patient ratios in select wards and reports that wards with higher staffing levels have lower overall mortality rates. Administrators propose expanding the staffing reform hospital-wide.\nHowever, when mortality is examined separately for high-risk and low-risk patients, wards without the staffing increase show lower mortality in both groups.\nThe staffing reform was initially implemented in wards that treated a larger proportion of low-risk patients.",
    "claim": "wards with higher staffing levels have lower overall mortality rates",
    "variables": {
      "X": {
        "name": "Staffing reform (higher vs. standard staffing)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Patient mortality",
        "role": "Outcome"
      },
      "Z": {
        "name": "Patient risk level (high / low)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Patient risk level (high / low) determined before Staffing reform (higher vs. standard staffing) was chosen, and could Patient risk level (high / low) have influenced the choice of Staffing reform (higher vs. standard staffing) before Patient mortality was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Staffing reform (higher vs. standard staffing) on Patient mortality may be reversed because the mix of subgroups differs between Staffing reform (higher vs. standard staffing) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Patient risk level (high / low): Use the within-stratum differences (or a standardized effect). If Staffing reform (higher vs. standard staffing) improves Patient mortality in each stratum, prefer Staffing reform (higher vs. standard staffing) even if the aggregate looks worse.",
      "C": "Answer if Staffing reform (higher vs. standard staffing) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Patient mortality by the key strata (e.g., Patient risk level (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Staffing reform (higher vs. standard staffing) is unevenly applied across strata.",
    "causal_structure": "Patient risk (Z) affects both staffing assignment (X) and mortality (Y), reversing subgroup-level effects when aggregated.",
    "key_insight": "Apparent benefits of an intervention may be driven by patient composition rather than causal impact.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-16",
    "_original_title": "The Hospital Staffing Reform",
    "_questions": "Would expanding the staffing reform reduce mortality hospital-wide?\nWhy does stratifying by patient risk reverse the conclusion?",
    "_expected_analysis": "Patient risk is determined prior to staffing decisions.\nWithin both risk strata, standard-staffed wards perform better.\nAggregate benefit reflects selective placement.\nConclusion: Expansion is INVALID.\nWise refusal: Randomized ward assignment or risk-adjusted analysis is required."
  },
  {
    "case_id": "T3-J1-L2-0017",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A university offers an optional tutoring program and finds that participants have higher overall course pass rates. Administrators consider making tutoring mandatory.\nWhen outcomes are analyzed separately for introductory and advanced courses, non-participants outperform participants in both categories.\nTutoring was most commonly used by students enrolled in advanced courses, which already have higher pass rates.",
    "claim": "participants have higher overall course pass rates",
    "variables": {
      "X": {
        "name": "Tutoring participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Course pass rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Course level (introductory / advanced)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Course level (introductory / advanced) determined before Tutoring participation was chosen, and could Course level (introductory / advanced) have influenced the choice of Tutoring participation before Course pass rate was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Tutoring participation on Course pass rate may be reversed because the mix of subgroups differs between Tutoring participation arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Course level (introductory / advanced): Use the within-stratum differences (or a standardized effect). If Tutoring participation improves Course pass rate in each stratum, prefer Tutoring participation even if the aggregate looks worse.",
      "C": "Answer if Tutoring participation can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Course pass rate by the key strata (e.g., Course level (introductory / advanced) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Tutoring participation is unevenly applied across strata.",
    "causal_structure": "Course level (Z) influences both tutoring use (X) and pass rates (Y), producing aggregate reversal.",
    "key_insight": "Participation patterns can dominate aggregate intervention outcomes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-17",
    "_original_title": "The University Tutoring Program",
    "_questions": "Should tutoring be made mandatory for all students?\nHow does course level affect interpretation?",
    "_expected_analysis": "Course level is fixed prior to tutoring.\nWithin both course strata, tutoring underperforms.\nAggregate success reflects enrollment mix.\nConclusion: Mandatory tutoring is INVALID.\nWise refusal: Causal effect requires random assignment within courses."
  },
  {
    "case_id": "T3-J1-L2-0018",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Workplace Policy",
    "difficulty": "Medium",
    "trap_type": "T8",
    "trap_family": "F5",
    "trap_subtype": "Stratified Intervention Reversal",
    "scenario": "A firm allows employees to opt into remote work and reports that remote workers have higher average productivity. Leadership considers mandating remote work for all eligible roles.\nHowever, when productivity is examined separately for technical and non-technical roles, in-office employees outperform remote employees in both categories.\nRemote work was disproportionately adopted by technical staff, who are more productive on average.",
    "claim": "remote workers have higher average productivity",
    "variables": {
      "X": {
        "name": "Work arrangement (remote / in-office)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Productivity",
        "role": "Outcome"
      },
      "Z": {
        "name": "Job role type (technical / non-technical)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Was Job role type (technical / non-technical) determined before Work arrangement (remote / in-office) was chosen, and could Job role type (technical / non-technical) have influenced the choice of Work arrangement (remote / in-office) before Productivity was measured?",
    "conditional_answers": {
      "A": "Answer if you only compare aggregates: The apparent effect of Work arrangement (remote / in-office) on Productivity may be reversed because the mix of subgroups differs between Work arrangement (remote / in-office) arms.",
      "B": "Answer if you compare within strata after stratifying/standardizing by Job role type (technical / non-technical): Use the within-stratum differences (or a standardized effect). If Work arrangement (remote / in-office) improves Productivity in each stratum, prefer Work arrangement (remote / in-office) even if the aggregate looks worse.",
      "C": "Answer if Work arrangement (remote / in-office) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Productivity by the key strata (e.g., Job role type (technical / non-technical) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Work arrangement (remote / in-office) is unevenly applied across strata.",
    "causal_structure": "Job role (Z) affects both remote eligibility (X) and productivity (Y), reversing subgroup effects when aggregated.",
    "key_insight": "Apparent productivity gains may reflect workforce composition.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-18",
    "_original_title": "The Remote Work Policy",
    "_questions": "Would mandating remote work increase productivity?\nWhy does role stratification change the conclusion?",
    "_expected_analysis": "Role type precedes work arrangement.\nWithin both role categories, in-office work performs better.\nAggregate effect is misleading.\nConclusion: Mandate is INVALID.\nWise refusal: Policy evaluation requires role-stratified experimentation.\n✅ L2-A COMPLETE\nL2-A cases completed: 9 / 9\nSimpson’s under intervention fully covered\nL2-B (Composition Effects)\n(Population changes, not treatment effects)"
  },
  {
    "case_id": "T3-J1-L2-0019",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Urban Economics",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city reports that after a neighborhood redevelopment project, average household income in the area increased by 40%. Officials conclude that redevelopment improved residents’ economic well-being.\nFurther analysis shows that many original low-income residents moved out, while higher-income residents moved in.",
    "claim": "redevelopment improved residents’ economic well-being",
    "variables": {
      "X": {
        "name": "Neighborhood redevelopment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Average household income",
        "role": "Outcome"
      },
      "Z": {
        "name": "Resident population composition",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Neighborhood redevelopment alter the composition (Resident population composition) of who is counted before Average household income was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Average household income after changing Neighborhood redevelopment can reflect a real outcome shift.",
      "B": "Answer if Neighborhood redevelopment changes who is counted via Resident population composition: The aggregate Average household income can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Average household income may be moving because the denominator/population changed after Neighborhood redevelopment via composition variable Resident population composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Redevelopment changes who lives in the neighborhood (Z), which alters average income (Y) without improving original residents’ outcomes.",
    "key_insight": "Aggregate improvement can reflect population turnover, not individual benefit.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-19",
    "_original_title": "The Gentrification Income Report",
    "_questions": "Did redevelopment make original residents wealthier?\nHow does population change affect income statistics?",
    "_expected_analysis": "Redevelopment affects population composition.\nAverage income rises despite no improvement for original residents.\nConclusion: Claim of resident enrichment is INVALID.\nWise refusal: Individual-level longitudinal data is required."
  },
  {
    "case_id": "T3-J1-L2-0020",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminology",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city introduces a crime reduction initiative in a high-crime area. One year later, the neighborhood’s crime rate drops significantly. Officials claim the initiative was successful.\nSubsequent analysis reveals that many high-risk residents relocated during the same period due to rising housing costs, while lower-risk residents moved in.",
    "claim": "A city introduces a crime reduction initiative in a high-crime area",
    "variables": {
      "X": {
        "name": "Crime reduction initiative",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Neighborhood crime rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Population turnover",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Crime reduction initiative alter the composition (Population turnover) of who is counted before Neighborhood crime rate was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Neighborhood crime rate after changing Crime reduction initiative can reflect a real outcome shift.",
      "B": "Answer if Crime reduction initiative changes who is counted via Population turnover: The aggregate Neighborhood crime rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Neighborhood crime rate may be moving because the denominator/population changed after Crime reduction initiative via composition variable Population turnover. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Population change (Z) drives crime reduction (Y), independent of the intervention.",
    "key_insight": "Crime rates can fall due to who leaves, not what policies change.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-20",
    "_original_title": "The Crime Reduction Initiative",
    "_questions": "Can the crime reduction be attributed to the initiative?\nWhy does population displacement complicate causal inference?",
    "_expected_analysis": "Initiative coincides with population change.\nReduced crime reflects altered risk pool.\nConclusion: Attribution to the initiative is INVALID.\nWise refusal: Evaluation requires tracking crime risk among original residents.\nL2-B (Composition Effects)"
  },
  {
    "case_id": "T3-J1-L2-0021",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A school district reports that after implementing a new admissions lottery, the average test scores across district schools increased. Officials conclude that the lottery policy improved academic performance.\nFurther analysis shows that the lottery led to a redistribution of students: higher-performing students concentrated in certain schools, while lower-performing students were reassigned elsewhere.",
    "claim": "the lottery policy improved academic performance",
    "variables": {
      "X": {
        "name": "Admissions lottery policy",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Average test scores",
        "role": "Outcome"
      },
      "Z": {
        "name": "Student distribution across schools",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Admissions lottery policy alter the composition (Student distribution across schools) of who is counted before Average test scores was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Average test scores after changing Admissions lottery policy can reflect a real outcome shift.",
      "B": "Answer if Admissions lottery policy changes who is counted via Student distribution across schools: The aggregate Average test scores can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Average test scores may be moving because the denominator/population changed after Admissions lottery policy via composition variable Student distribution across schools. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "The lottery changes student composition (Z), altering school averages (Y) without changing individual achievement.",
    "key_insight": "Improved averages can result from reshuffling students, not learning gains.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-21",
    "_original_title": "The School Test Score Improvement",
    "_questions": "Did the lottery policy improve individual student performance?\nHow does redistribution affect average test scores?",
    "_expected_analysis": "Policy affects student allocation, not instruction.\nAverage scores rise due to compositional changes.\nConclusion: Individual performance improvement claim is INVALID.\nWise refusal: Longitudinal student-level data is required."
  },
  {
    "case_id": "T3-J1-L2-0022",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor & Organizations",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A corporation announces that after a diversity initiative, the percentage of women in leadership roles increased. Leadership attributes this to improved promotion practices.\nFurther inspection reveals that the increase is driven largely by hiring women directly into senior roles, while promotion rates within the firm remain unchanged.",
    "claim": "A corporation announces that after a diversity initiative, the percentage of women in leadership roles increased",
    "variables": {
      "X": {
        "name": "Diversity initiative",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Share of women in leadership",
        "role": "Outcome"
      },
      "Z": {
        "name": "Entry vs. promotion composition",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Diversity initiative alter the composition (Entry vs. promotion composition) of who is counted before Share of women in leadership was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Share of women in leadership after changing Diversity initiative can reflect a real outcome shift.",
      "B": "Answer if Diversity initiative changes who is counted via Entry vs. promotion composition: The aggregate Share of women in leadership can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Share of women in leadership may be moving because the denominator/population changed after Diversity initiative via composition variable Entry vs. promotion composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Leadership composition (Y) changes due to hiring mix (Z), not internal advancement.",
    "key_insight": "Stock metrics can change without flow changes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-22",
    "_original_title": "The Workforce Diversity Metric",
    "_questions": "Did the initiative improve promotion equity?\nWhy does leadership composition change without internal progress?",
    "_expected_analysis": "Initiative changes who enters leadership.\nInternal dynamics remain unchanged.\nConclusion: Promotion improvement claim is INVALID.\nWise refusal: Promotion-rate analysis is required."
  },
  {
    "case_id": "T3-J1-L2-0023",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Healthcare Management",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A hospital reports a decline in 30-day readmission rates after introducing a discharge planning program. Administrators credit the program with improving patient outcomes.\nLater analysis shows that more high-risk patients were transferred to long-term care facilities rather than discharged home during the same period.",
    "claim": "A hospital reports a decline in 30-day readmission rates after introducing a discharge planning program",
    "variables": {
      "X": {
        "name": "Discharge planning program",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Readmission rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Discharged patient risk profile",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Discharge planning program alter the composition (Discharged patient risk profile) of who is counted before Readmission rate was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Readmission rate after changing Discharge planning program can reflect a real outcome shift.",
      "B": "Answer if Discharge planning program changes who is counted via Discharged patient risk profile: The aggregate Readmission rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Readmission rate may be moving because the denominator/population changed after Discharge planning program via composition variable Discharged patient risk profile. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Patient mix (Z) changes the denominator for readmissions (Y).",
    "key_insight": "Outcome metrics can improve by excluding high-risk cases.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-23",
    "_original_title": "The Hospital Readmission Decline",
    "_questions": "Does the program reduce readmissions for comparable patients?\nHow does discharge selection affect reported rates?",
    "_expected_analysis": "High-risk patients are removed from measurement pool.\nReadmission rate declines mechanically.\nConclusion: Program effectiveness claim is INVALID.\nWise refusal: Risk-adjusted readmission analysis is required."
  },
  {
    "case_id": "T3-J1-L2-0024",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city reports that after an influx of immigrants, the unemployment rate declined. Officials claim immigration strengthened the local labor market.\nCloser inspection reveals that immigrants were more likely to be employed upon arrival, while some unemployed residents moved away due to rising rents.",
    "claim": "after an influx of immigrants, the unemployment rate declined",
    "variables": {
      "X": {
        "name": "Immigration influx",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Unemployment rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Labor force composition",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Immigration influx alter the composition (Labor force composition) of who is counted before Unemployment rate was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Unemployment rate after changing Immigration influx can reflect a real outcome shift.",
      "B": "Answer if Immigration influx changes who is counted via Labor force composition: The aggregate Unemployment rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Unemployment rate may be moving because the denominator/population changed after Immigration influx via composition variable Labor force composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Employment statistics change due to who enters and exits the labor force.",
    "key_insight": "Labor metrics are sensitive to population flows.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-24",
    "_original_title": "The Immigration Employment Statistic",
    "_questions": "Did immigration create jobs for existing residents?\nWhy can unemployment fall without job creation?",
    "_expected_analysis": "Employment rate reflects labor pool composition.\nDecline not attributable to job growth.\nConclusion: Job creation claim is INVALID.\nWise refusal: Separate employment effects by resident status."
  },
  {
    "case_id": "T3-J1-L2-0025",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Transportation Policy",
    "difficulty": "Easy",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "After expanding a public transit line, a city reports a 30% increase in ridership and claims the expansion reduced car usage.\nFurther analysis shows that many riders were former bus users whose routes were discontinued, forcing them onto the new line.",
    "claim": "After expanding a public transit line, a city reports a 30% increase in ridership and claims the expansion reduced car usage",
    "variables": {
      "X": {
        "name": "Transit line expansion",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Ridership counts",
        "role": "Outcome"
      },
      "Z": {
        "name": "Mode substitution patterns",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Transit line expansion alter the composition (Mode substitution patterns) of who is counted before Ridership counts was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Ridership counts after changing Transit line expansion can reflect a real outcome shift.",
      "B": "Answer if Transit line expansion changes who is counted via Mode substitution patterns: The aggregate Ridership counts can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Ridership counts may be moving because the denominator/population changed after Transit line expansion via composition variable Mode substitution patterns. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Ridership growth reflects reclassification of existing users.",
    "key_insight": "Usage metrics can rise without behavior change.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-25",
    "_original_title": "The Public Transit Ridership Surge",
    "_questions": "Did the expansion reduce car travel?\nHow does reclassification inflate ridership numbers?",
    "_expected_analysis": "Riders shift between transit categories.\nNo evidence of reduced car usage.\nConclusion: Car reduction claim is INVALID.\nWise refusal: Mode-shift analysis is required."
  },
  {
    "case_id": "T3-J1-L2-0026",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A university rises significantly in national rankings after launching a selective honors program. Administrators claim that the program improved overall academic quality.\nFurther analysis shows that the university admitted a smaller cohort of highly qualified honors students while reducing enrollment elsewhere, without changing instructional practices for existing students.",
    "claim": "the program improved overall academic quality",
    "variables": {
      "X": {
        "name": "Honors program introduction",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "University ranking metrics",
        "role": "Outcome"
      },
      "Z": {
        "name": "Student intake composition",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Honors program introduction alter the composition (Student intake composition) of who is counted before University ranking metrics was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in University ranking metrics after changing Honors program introduction can reflect a real outcome shift.",
      "B": "Answer if Honors program introduction changes who is counted via Student intake composition: The aggregate University ranking metrics can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for University ranking metrics may be moving because the denominator/population changed after Honors program introduction via composition variable Student intake composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Rankings improve because the student body composition (Z) changes, not because educational quality improves.",
    "key_insight": "Institutional metrics can improve through selective enrollment rather than better outcomes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-26",
    "_original_title": "The University Ranking Improvement",
    "_questions": "Did the honors program improve education for existing students?\nWhy can rankings rise without instructional change?",
    "_expected_analysis": "Program changes who is admitted.\nRankings reflect input quality, not value added.\nConclusion: Educational improvement claim is INVALID.\nWise refusal: Value-added measures are required."
  },
  {
    "case_id": "T3-J1-L2-0027",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Environmental Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city reports a decline in average per-capita carbon emissions after adopting a climate action plan. Officials credit the plan with reducing emissions.\nFurther investigation reveals that several energy-intensive factories closed during the same period, relocating to neighboring regions.",
    "claim": "A city reports a decline in average per-capita carbon emissions after adopting a climate action plan",
    "variables": {
      "X": {
        "name": "Climate action plan",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Per-capita carbon emissions",
        "role": "Outcome"
      },
      "Z": {
        "name": "Industrial activity composition",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Climate action plan alter the composition (Industrial activity composition) of who is counted before Per-capita carbon emissions was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Per-capita carbon emissions after changing Climate action plan can reflect a real outcome shift.",
      "B": "Answer if Climate action plan changes who is counted via Industrial activity composition: The aggregate Per-capita carbon emissions can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Per-capita carbon emissions may be moving because the denominator/population changed after Climate action plan via composition variable Industrial activity composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Emissions fall because polluting activity exits the measurement region.",
    "key_insight": "Environmental metrics can improve via relocation, not reduction.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-27",
    "_original_title": "The Environmental Emissions Drop",
    "_questions": "Did the policy reduce emissions behavior?\nWhy does displacement complicate attribution?",
    "_expected_analysis": "Emissions decline reflects industrial exit.\nNo evidence of cleaner production.\nConclusion: Policy effectiveness claim is INVALID.\nWise refusal: Consumption-based emissions accounting is required.\n✅ L2-B COMPLETE\nL2-B cases: 9 / 9\nL2-C (Selection into Treatment)\n(People who receive the intervention differ systematically)"
  },
  {
    "case_id": "T3-J1-L2-0028",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor Policy",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "A government reports that participants in a job training program have higher post-program employment rates than non-participants. Officials conclude the program is effective.\nHowever, enrollment in the program is voluntary, and participants are more motivated and actively job-seeking than non-participants even before enrollment.",
    "claim": "participants in a job training program have higher post-program employment rates than non-participants",
    "variables": {
      "X": {
        "name": "Program participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Employment outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Job-seeking motivation",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Employment outcome occurred—and is selection related to Job-seeking motivation or Employment outcome?",
    "conditional_answers": {
      "A": "Answer if Program participation is randomly assigned: A difference in Employment outcome across Program participation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Job-seeking motivation): The Program participation vs not-Program participation difference in Employment outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Job-seeking motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Job-seeking motivation); otherwise Program participation–Employment outcome differences may reflect selection rather than effect.",
    "causal_structure": "Motivation (Z) affects both participation (X) and employment (Y).",
    "key_insight": "Participants would have better outcomes regardless of treatment.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-28",
    "_original_title": "The Job Training Program",
    "_questions": "Does higher employment among participants imply program effectiveness?\nWhy does voluntary enrollment bias inference?",
    "_expected_analysis": "Motivation precedes treatment.\nEmployment differences reflect selection.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Random assignment is required."
  },
  {
    "case_id": "T3-J1-L2-0029",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "Patients who undergo preventive health screenings have lower mortality rates than those who do not. Health officials promote screenings as life-saving.\nHowever, individuals who choose screenings tend to be healthier, wealthier, and more health-conscious than those who decline.",
    "claim": "Patients who undergo preventive health screenings have lower mortality rates than those who do not",
    "variables": {
      "X": {
        "name": "Screening participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Mortality",
        "role": "Outcome"
      },
      "Z": {
        "name": "Baseline health behavior",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Mortality occurred—and is selection related to Baseline health behavior or Mortality?",
    "conditional_answers": {
      "A": "Answer if Screening participation is randomly assigned: A difference in Mortality across Screening participation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Baseline health behavior): The Screening participation vs not-Screening participation difference in Mortality is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Baseline health behavior) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Baseline health behavior); otherwise Screening participation–Mortality differences may reflect selection rather than effect.",
    "causal_structure": "Health behavior (Z) influences both screening (X) and mortality (Y).",
    "key_insight": "Observed benefit may reflect who chooses screening.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-29",
    "_original_title": "The Preventive Health Screening",
    "_questions": "Does screening causally reduce mortality?\nWhy are screened and unscreened populations incomparable?",
    "_expected_analysis": "Health behavior precedes screening.\nMortality differences are confounded.\nConclusion: Causal claim is INVALID.\nWise refusal: Controlled trials or instrumental variables are needed."
  },
  {
    "case_id": "T3-J1-L2-0030",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "Employees who attend a leadership development program are promoted at higher rates than those who do not. Management credits the program with improving leadership skills.\nHowever, attendance is limited to employees already identified as high-potential by senior managers.",
    "claim": "Employees who attend a leadership development program are promoted at higher rates than those who do not",
    "variables": {
      "X": {
        "name": "Program attendance",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Promotion outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Pre-existing leadership potential",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Promotion outcome occurred—and is selection related to Pre-existing leadership potential or Promotion outcome?",
    "conditional_answers": {
      "A": "Answer if Program attendance is randomly assigned: A difference in Promotion outcome across Program attendance groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Pre-existing leadership potential): The Program attendance vs not-Program attendance difference in Promotion outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Pre-existing leadership potential) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Pre-existing leadership potential); otherwise Program attendance–Promotion outcome differences may reflect selection rather than effect.",
    "causal_structure": "Leadership potential (Z) affects both selection into the program (X) and promotion (Y).",
    "key_insight": "Programs targeting high performers inflate apparent effectiveness.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-30",
    "_original_title": "The Leadership Development Program",
    "_questions": "Does the program cause higher promotion rates?\nHow does pre-selection bias evaluation?",
    "_expected_analysis": "Selection precedes treatment.\nPromotions reflect prior assessments.\nConclusion: Program impact claim is INVALID.\nWise refusal: Compare with matched non-selected employees."
  },
  {
    "case_id": "T3-J1-L2-0031",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Higher Education",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Time-varying Confounding",
    "scenario": "Students who receive a merit-based scholarship graduate at higher rates than those who do not. University administrators conclude that the scholarship improves student success.\nHowever, scholarship recipients are selected based on prior academic achievement, strong recommendations, and demonstrated motivation.",
    "claim": "the scholarship improves student success",
    "variables": {
      "X": {
        "name": "Scholarship receipt",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Graduation outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Prior academic achievement and motivation",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Graduation outcome occurred—and is selection related to Prior academic achievement and motivation or Graduation outcome?",
    "conditional_answers": {
      "A": "Answer if Scholarship receipt is randomly assigned: A difference in Graduation outcome across Scholarship receipt groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Prior academic achievement and motivation): The Scholarship receipt vs not-Scholarship receipt difference in Graduation outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Prior academic achievement and motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Prior academic achievement and motivation); otherwise Scholarship receipt–Graduation outcome differences may reflect selection rather than effect.",
    "causal_structure": "Prior achievement (Z) affects both scholarship receipt (X) and graduation (Y).",
    "key_insight": "Scholarships may select strong students rather than create success.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-31",
    "_original_title": "The College Scholarship Program",
    "_questions": "Does receiving the scholarship cause higher graduation rates?\nWhy does merit-based selection bias the comparison?",
    "_expected_analysis": "Academic strength precedes scholarship.\nGraduation differences reflect selection.\nConclusion: Causal claim is INVALID.\nWise refusal: Randomized or need-based assignment is required."
  },
  {
    "case_id": "T3-J1-L2-0032",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Technology",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "Users who enroll in a voluntary online learning platform show greater skill improvement than non-users. Platform developers claim the platform is effective.\nHowever, enrollment is optional, and users are typically more self-motivated and already interested in skill development.",
    "claim": "Users who enroll in a voluntary online learning platform show greater skill improvement than non-users",
    "variables": {
      "X": {
        "name": "Platform enrollment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Skill improvement",
        "role": "Outcome"
      },
      "Z": {
        "name": "Learner motivation",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Skill improvement occurred—and is selection related to Learner motivation or Skill improvement?",
    "conditional_answers": {
      "A": "Answer if Platform enrollment is randomly assigned: A difference in Skill improvement across Platform enrollment groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Learner motivation): The Platform enrollment vs not-Platform enrollment difference in Skill improvement is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Learner motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Learner motivation); otherwise Platform enrollment–Skill improvement differences may reflect selection rather than effect.",
    "causal_structure": "Motivation (Z) influences both enrollment (X) and learning outcomes (Y).",
    "key_insight": "Voluntary participation inflates perceived effectiveness.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-32",
    "_original_title": "The Voluntary Online Learning Platform",
    "_questions": "Does platform usage cause skill gains?\nWhy are users and non-users incomparable?",
    "_expected_analysis": "Motivation predates platform use.\nGains reflect selection bias.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Randomized access or encouragement design is needed."
  },
  {
    "case_id": "T3-J1-L2-0033",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "Children enrolled in a voluntary early childhood education program perform better academically later in life. Policymakers cite this as evidence of program success.\nHowever, parents who enroll their children tend to be more engaged, have higher educational attainment, and provide more academic support at home.",
    "claim": "Children enrolled in a voluntary early childhood education program perform better academically later in life",
    "variables": {
      "X": {
        "name": "Program enrollment",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Later academic performance",
        "role": "Outcome"
      },
      "Z": {
        "name": "Parental engagement",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Later academic performance occurred—and is selection related to Parental engagement or Later academic performance?",
    "conditional_answers": {
      "A": "Answer if Program enrollment is randomly assigned: A difference in Later academic performance across Program enrollment groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Parental engagement): The Program enrollment vs not-Program enrollment difference in Later academic performance is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Parental engagement) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Parental engagement); otherwise Program enrollment–Later academic performance differences may reflect selection rather than effect.",
    "causal_structure": "Parental engagement (Z) affects both enrollment (X) and child outcomes (Y).",
    "key_insight": "Family background confounds program evaluation.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-33",
    "_original_title": "The Early Childhood Education Program",
    "_questions": "Can better outcomes be attributed to the program itself?\nHow does parental choice bias evaluation?",
    "_expected_analysis": "Engagement precedes enrollment.\nOutcomes reflect family inputs.\nConclusion: Program impact claim is INVALID.\nWise refusal: Randomized access or sibling comparisons are required."
  },
  {
    "case_id": "T3-J1-L2-0034",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Health",
    "difficulty": "Easy",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "Users of a fitness tracking app lose more weight than non-users. The app is marketed as effective for weight loss.\nHowever, app users are typically more health-conscious and already motivated to exercise.",
    "claim": "Users of a fitness tracking app lose more weight than non-users",
    "variables": {
      "X": {
        "name": "App usage",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Weight loss",
        "role": "Outcome"
      },
      "Z": {
        "name": "Health motivation",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Weight loss occurred—and is selection related to Health motivation or Weight loss?",
    "conditional_answers": {
      "A": "Answer if App usage is randomly assigned: A difference in Weight loss across App usage groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Health motivation): The App usage vs not-App usage difference in Weight loss is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Health motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Health motivation); otherwise App usage–Weight loss differences may reflect selection rather than effect.",
    "causal_structure": "Motivation (Z) influences both app adoption (X) and outcomes (Y).",
    "key_insight": "Technology uptake selects motivated users.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-34",
    "_original_title": "The Fitness App Effectiveness Claim",
    "_questions": "Does app usage cause weight loss?\nWhy is comparing users and non-users misleading?",
    "_expected_analysis": "Motivation predates app usage.\nWeight loss reflects user characteristics.\nConclusion: App effectiveness claim is INVALID.\nWise refusal: Randomized trials are needed."
  },
  {
    "case_id": "T3-J1-L2-0035",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Human Resources",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "F4",
    "trap_subtype": "Unblocked Backdoor",
    "scenario": "Employees who participate in a mentorship program receive higher performance ratings than non-participants. Managers argue that mentoring improves performance.\nHowever, mentors are assigned to employees already identified as high performers.",
    "claim": "Employees who participate in a mentorship program receive higher performance ratings than non-participants",
    "variables": {
      "X": {
        "name": "Mentorship participation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Performance rating",
        "role": "Outcome"
      },
      "Z": {
        "name": "Prior performance level",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Performance rating occurred—and is selection related to Prior performance level or Performance rating?",
    "conditional_answers": {
      "A": "Answer if Mentorship participation is randomly assigned: A difference in Performance rating across Mentorship participation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Prior performance level): The Mentorship participation vs not-Mentorship participation difference in Performance rating is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Prior performance level) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Prior performance level); otherwise Mentorship participation–Performance rating differences may reflect selection rather than effect.",
    "causal_structure": "Prior performance (Z) influences mentorship assignment (X) and ratings (Y).",
    "key_insight": "Programs targeting strong performers exaggerate impact.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-35",
    "_original_title": "The Mentorship Program Outcomes",
    "_questions": "Does mentorship cause higher performance?\nHow does selection bias affect evaluation?",
    "_expected_analysis": "Performance precedes mentoring.\nRatings reflect prior ability.\nConclusion: Mentorship impact claim is INVALID.\nWise refusal: Compare with matched non-selected employees."
  },
  {
    "case_id": "T3-J1-L2-0036",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Mental Health",
    "difficulty": "Hard",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "Patients who complete a voluntary therapy program show better mental health outcomes than those who drop out or never enroll. Providers claim the therapy is effective.\nHowever, patients who complete therapy are those who respond early or have fewer barriers to participation.",
    "claim": "Patients who complete a voluntary therapy program show better mental health outcomes than those who drop out or never enroll",
    "variables": {
      "X": {
        "name": "Therapy completion",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Mental health outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Treatment adherence capacity",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "At what point were units selected into the observed sample—before or after Mental health outcome occurred—and is selection related to Treatment adherence capacity or Mental health outcome?",
    "conditional_answers": {
      "A": "Answer if Therapy completion is randomly assigned: A difference in Mental health outcome across Therapy completion groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Treatment adherence capacity): The Therapy completion vs not-Therapy completion difference in Mental health outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Treatment adherence capacity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Treatment adherence capacity); otherwise Therapy completion–Mental health outcome differences may reflect selection rather than effect.",
    "causal_structure": "Adherence capacity (Z) influences both completion (X) and outcomes (Y).",
    "key_insight": "Conditioning on completion induces selection bias.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-36",
    "_original_title": "The Therapy Program Success Rate",
    "_questions": "Does completing therapy cause better outcomes?\nWhy is completion a biased conditioning variable?",
    "_expected_analysis": "Adherence differences precede outcomes.\nCompletion-based comparison is biased.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Intention-to-treat analysis is required.\nL2-D (Collider Bias) — COMPLETE (9 cases)"
  },
  {
    "case_id": "T3-J1-L2-0037",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "A hospital studies patients admitted with a severe illness and finds that smokers have lower mortality rates than non-smokers among admitted patients. Administrators speculate that smoking may be protective.\nHowever, hospital admission occurs only for patients who become seriously ill. Smoking and other health conditions both increase the likelihood of severe illness and admission.",
    "claim": "smokers have lower mortality rates than non-smokers among admitted patients",
    "variables": {
      "X": {
        "name": "Smoking status",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Mortality",
        "role": "Outcome"
      },
      "Z": {
        "name": "Hospital admission (conditioning variable)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Hospital admission (conditioning variable) that is determined after upstream factors affecting both Smoking status and Mortality, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Hospital admission (conditioning variable)): Associations between Smoking status and Mortality can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Hospital admission (conditioning variable)), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "Smoking and mortality both influence hospital admission; conditioning on admission induces spurious correlation.",
    "key_insight": "Conditioning on a collider can reverse associations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-37",
    "_original_title": "The Hospital Survival Paradox",
    "_questions": "Does smoking reduce mortality among hospitalized patients?\nWhy does conditioning on hospital admission distort the association?",
    "_expected_analysis": "Admission is a collider influenced by smoking and health.\nConditioning induces negative correlation.\nConclusion: Protective effect claim is INVALID.\nWise refusal: Analyze population-level data without conditioning on admission."
  },
  {
    "case_id": "T3-J1-L2-0038",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among patients with heart disease, overweight patients appear to have better survival rates than normal-weight patients. Some interpret this as evidence of an “obesity paradox.”\nHeart disease diagnosis depends on both underlying health risks and body weight, creating a selected population.",
    "claim": "Among patients with heart disease, overweight patients appear to have better survival rates than normal-weight patients",
    "variables": {
      "X": {
        "name": "Body weight",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Survival outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Heart disease diagnosis",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Heart disease diagnosis that is determined after upstream factors affecting both Body weight and Survival outcome, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Heart disease diagnosis): Associations between Body weight and Survival outcome can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Heart disease diagnosis), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on disease status biases associations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-38",
    "_original_title": "The Obesity Survival Puzzle",
    "_questions": "",
    "_expected_analysis": "Diagnosis is a collider.\nApparent benefit is spurious.\nConclusion: Causal claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0039",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among students admitted to elite colleges, students from less privileged backgrounds outperform wealthier peers academically. Commentators argue that disadvantage improves performance.\nAdmission depends on both background and academic potential.",
    "claim": "Among students admitted to elite colleges, students from less privileged backgrounds outperform wealthier peers academically",
    "variables": {
      "X": {
        "name": "Socioeconomic background",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Academic performance",
        "role": "Outcome"
      },
      "Z": {
        "name": "Elite college admission",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Elite college admission that is determined after upstream factors affecting both Socioeconomic background and Academic performance, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Elite college admission): Associations between Socioeconomic background and Academic performance can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Elite college admission), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on selective admission distorts comparisons.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-39",
    "_original_title": "Elite College Admissions and Success",
    "_questions": "",
    "_expected_analysis": "Admission is a collider.\nHigh-performing disadvantaged students are overrepresented.\nConclusion: Advantage of disadvantage claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0040",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among employees who are promoted, those with lower job satisfaction before promotion show higher post-promotion performance. Managers infer dissatisfaction drives improvement.\nPromotion depends on both performance and dissatisfaction signals.",
    "claim": "Among employees who are promoted, those with lower job satisfaction before promotion show higher post-promotion performance",
    "variables": {
      "X": {
        "name": "Job satisfaction",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Performance",
        "role": "Outcome"
      },
      "Z": {
        "name": "Promotion status",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Promotion status that is determined after upstream factors affecting both Job satisfaction and Performance, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Promotion status): Associations between Job satisfaction and Performance can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Promotion status), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on selection outcomes creates false correlations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-40",
    "_original_title": "Promotion and Job Satisfaction",
    "_questions": "",
    "_expected_analysis": "Promotion is a collider.\nNegative association is spurious.\nConclusion: Interpretation is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0041",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Entrepreneurship",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among founders of successful startups, those without formal business training appear more innovative. Observers conclude training stifles creativity.\nStartup success depends on both training and innovation.",
    "claim": "Among founders of successful startups, those without formal business training appear more innovative",
    "variables": {
      "X": {
        "name": "Business training",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Innovation",
        "role": "Outcome"
      },
      "Z": {
        "name": "Startup success",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Startup success that is determined after upstream factors affecting both Business training and Innovation, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Startup success): Associations between Business training and Innovation can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Startup success), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on success induces tradeoff illusion.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-41",
    "_original_title": "Startup Founder Traits",
    "_questions": "",
    "_expected_analysis": "Success is a collider.\nApparent inverse relationship is spurious.\nConclusion: Claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0042",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among disaster survivors, individuals with chronic conditions appear more resilient. Analysts infer chronic illness builds resilience.\nSurvival depends on both health status and exposure.",
    "claim": "Among disaster survivors, individuals with chronic conditions appear more resilient",
    "variables": {
      "X": {
        "name": "Chronic illness",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Resilience",
        "role": "Outcome"
      },
      "Z": {
        "name": "Survival",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Survival that is determined after upstream factors affecting both Chronic illness and Resilience, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Survival): Associations between Chronic illness and Resilience can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Survival), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on survival distorts health associations.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-42",
    "_original_title": "Disaster Survivor Health",
    "_questions": "",
    "_expected_analysis": "Survival is a collider.\nResilience effect is spurious.\nConclusion: Claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0043",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Science of Science",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among published papers, researchers without prestigious affiliations have higher citation counts. Some argue prestige harms impact.\nPublication depends on both institutional prestige and paper quality.",
    "claim": "Among published papers, researchers without prestigious affiliations have higher citation counts",
    "variables": {
      "X": {
        "name": "Institutional prestige",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Citation impact",
        "role": "Outcome"
      },
      "Z": {
        "name": "Publication",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Publication that is determined after upstream factors affecting both Institutional prestige and Citation impact, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Publication): Associations between Institutional prestige and Citation impact can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Publication), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on publication induces spurious tradeoffs.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-43",
    "_original_title": "Academic Publication Bias",
    "_questions": "",
    "_expected_analysis": "Publication is a collider.\nEffect is spurious.\nConclusion: Interpretation is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0044",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Labor & Hiring",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among employees hired from elite firms, those with weaker resumes perform as well as strong candidates from non-elite firms. Managers infer elite firms overvalue credentials.\nHiring depends on both resume strength and firm pedigree.",
    "claim": "Among employees hired from elite firms, those with weaker resumes perform as well as strong candidates from non-elite firms",
    "variables": {
      "X": {
        "name": "Resume strength",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Job performance",
        "role": "Outcome"
      },
      "Z": {
        "name": "Hiring decision",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Hiring decision that is determined after upstream factors affecting both Resume strength and Job performance, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Hiring decision): Associations between Resume strength and Job performance can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Hiring decision), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on hiring distorts performance signals.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-44",
    "_original_title": "Hiring from Elite Firms",
    "_questions": "",
    "_expected_analysis": "Hiring is a collider.\nComparisons are biased.\nConclusion: Claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0045",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Sports Analytics",
    "difficulty": "Hard",
    "trap_type": "T3",
    "trap_family": "F2",
    "trap_subtype": "Conditioning on Compliance",
    "scenario": "Among professional athletes, those with poorer early training outperform peers later. Analysts argue early training is overrated.\nProfessional selection depends on both training quality and innate talent.",
    "claim": "Among professional athletes, those with poorer early training outperform peers later",
    "variables": {
      "X": {
        "name": "Early training quality",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Later performance",
        "role": "Outcome"
      },
      "Z": {
        "name": "Professional selection",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Is the analysis conditioning on Professional selection that is determined after upstream factors affecting both Early training quality and Later performance, potentially inducing a spurious association?",
    "conditional_answers": {
      "A": "Answer if you condition on a post-selection variable (conditioning on Professional selection): Associations between Early training quality and Later performance can be artifacts created by conditioning; do not interpret them causally.",
      "B": "Answer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.",
      "C": "Answer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Professional selection), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association.",
    "causal_structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
    "key_insight": "Conditioning on elite selection induces spurious inversions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-45",
    "_original_title": "The Athlete Selection Effect",
    "_questions": "",
    "_expected_analysis": "Selection is a collider.\nApparent reversal is spurious.\nConclusion: Claim is INVALID.\nL2-E (Base-Rate Distortion) — COMPLETE (9 cases)"
  },
  {
    "case_id": "T3-J1-L2-0046",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Public Health",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A news report states that most hospitalized COVID patients are vaccinated, leading commentators to claim that vaccines are ineffective.\nHowever, a large majority of the population is vaccinated, while only a small fraction is unvaccinated.",
    "claim": "vaccines are ineffective",
    "variables": {
      "X": {
        "name": "Vaccination status",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Hospitalization",
        "role": "Outcome"
      },
      "Z": {
        "name": "Population base rate of vaccination",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Hospitalization measured over the same time window as the base rate/denominator Population base rate of vaccination, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Hospitalization: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Hospitalization with denominator/base rate Population base rate of vaccination: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Hospitalization are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Population base rate of vaccination (and by subgroup/time) before concluding anything.",
    "causal_structure": "Hospitalization counts reflect population proportions rather than individual risk.",
    "key_insight": "High counts do not imply high risk without denominators.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-46",
    "_original_title": "Vaccine Hospitalization Reports",
    "_questions": "Does the hospitalization statistic imply vaccines are ineffective?\nWhy are base rates necessary for interpretation?",
    "_expected_analysis": "Vaccinated individuals dominate the population.\nRisk per person is lower among vaccinated.\nConclusion: Ineffectiveness claim is INVALID.\nWise refusal: Compare hospitalization rates, not counts."
  },
  {
    "case_id": "T3-J1-L2-0047",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "Police data shows that most arrests are from Neighborhood A, prompting claims that residents of Neighborhood A commit more crimes.\nHowever, Neighborhood A has twice the population of other neighborhoods.",
    "claim": "residents of Neighborhood A commit more crimes",
    "variables": {
      "X": {
        "name": "Neighborhood",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Arrest count",
        "role": "Outcome"
      },
      "Z": {
        "name": "Neighborhood population size",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Arrest count measured over the same time window as the base rate/denominator Neighborhood population size, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Arrest count: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Arrest count with denominator/base rate Neighborhood population size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Arrest count are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Neighborhood population size (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Counts must be normalized by population.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-47",
    "_original_title": "Crime Arrest Statistics",
    "_questions": "",
    "_expected_analysis": "Higher counts reflect larger population.\nConclusion: Crime propensity claim is INVALID.\nWise refusal: Use per-capita arrest rates."
  },
  {
    "case_id": "T3-J1-L2-0048",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Finance",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A bank reports that most loan defaults come from low-income borrowers, concluding they are riskier.\nHowever, most borrowers in the bank’s portfolio are low-income.",
    "claim": "most loan defaults come from low-income borrowers, concluding they are riskier",
    "variables": {
      "X": {
        "name": "Income group",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Loan default",
        "role": "Outcome"
      },
      "Z": {
        "name": "Borrower distribution",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Loan default measured over the same time window as the base rate/denominator Borrower distribution, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Loan default: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Loan default with denominator/base rate Borrower distribution: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Loan default are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Borrower distribution (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Default counts track borrower mix.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-48",
    "_original_title": "Loan Default Claims",
    "_questions": "",
    "_expected_analysis": "Risk requires conditional default rates.\nConclusion: Risk claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0049",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A report states that Group A accounts for more repeat offenses than Group B, suggesting harsher sentencing is needed for Group A.\nHowever, Group A represents a much larger share of the formerly incarcerated population.",
    "claim": "A report states that Group A accounts for more repeat offenses than Group B, suggesting harsher sentencing is needed for Group A",
    "variables": {
      "X": {
        "name": "Group membership",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Recidivism",
        "role": "Outcome"
      },
      "Z": {
        "name": "Released population size",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Recidivism measured over the same time window as the base rate/denominator Released population size, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Recidivism: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Recidivism with denominator/base rate Released population size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Recidivism are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Released population size (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Exposure determines opportunity for reoffense.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-49",
    "_original_title": "Recidivism Rate Comparison",
    "_questions": "",
    "_expected_analysis": "Compare rates per released individual.\nConclusion: Sentencing inference is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0050",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Occupational Safety",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A company reports that most workplace accidents occur in Factory X, concluding it is unsafe.\nFactory X employs far more workers than other sites.",
    "claim": "most workplace accidents occur in Factory X, concluding it is unsafe",
    "variables": {
      "X": {
        "name": "Factory site",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Accident count",
        "role": "Outcome"
      },
      "Z": {
        "name": "Workforce size",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Accident count measured over the same time window as the base rate/denominator Workforce size, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Accident count: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Accident count with denominator/base rate Workforce size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Accident count are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Workforce size (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Larger sites generate more incidents.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-50",
    "_original_title": "Workplace Accident Reports",
    "_questions": "",
    "_expected_analysis": "Normalize by hours worked.\nConclusion: Safety claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0051",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A university notes that most academic misconduct cases involve first-year students, implying they cheat more.\nHowever, first-year students comprise the largest enrollment cohort.",
    "claim": "A university notes that most academic misconduct cases involve first-year students, implying they cheat more",
    "variables": {
      "X": {
        "name": "Academic year",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Misconduct case",
        "role": "Outcome"
      },
      "Z": {
        "name": "Enrollment size",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Misconduct case measured over the same time window as the base rate/denominator Enrollment size, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Misconduct case: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Misconduct case with denominator/base rate Enrollment size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Misconduct case are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Enrollment size (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Case counts reflect cohort size.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-51",
    "_original_title": "Academic Misconduct Cases",
    "_questions": "",
    "_expected_analysis": "Compare misconduct rates by cohort.\nConclusion: Cheating inference is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0052",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Finance & Compliance",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "An algorithm flags more fraudulent transactions among online purchases than in-store purchases, leading to claims that online shopping is riskier.\nHowever, online transactions vastly outnumber in-store ones.",
    "claim": "online shopping is riskier",
    "variables": {
      "X": {
        "name": "Transaction type",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Fraud alert",
        "role": "Outcome"
      },
      "Z": {
        "name": "Transaction volume",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Fraud alert measured over the same time window as the base rate/denominator Transaction volume, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Fraud alert: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Fraud alert with denominator/base rate Transaction volume: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Fraud alert are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Transaction volume (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Alert counts track transaction volume.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-52",
    "_original_title": "Fraud Detection Alerts",
    "_questions": "",
    "_expected_analysis": "Compare fraud rates, not alerts.\nConclusion: Risk claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0053",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A district reports that most suspensions involve students from School A, concluding discipline problems are worse there.\nSchool A enrolls substantially more students.",
    "claim": "most suspensions involve students from School A, concluding discipline problems are worse there",
    "variables": {
      "X": {
        "name": "School",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Suspension",
        "role": "Outcome"
      },
      "Z": {
        "name": "Enrollment size",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Suspension measured over the same time window as the base rate/denominator Enrollment size, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Suspension: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Suspension with denominator/base rate Enrollment size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Suspension are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Enrollment size (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Discipline counts must be normalized.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-53",
    "_original_title": "School Discipline Disparities",
    "_questions": "",
    "_expected_analysis": "Use suspension rates per student.\nConclusion: Discipline severity claim is INVALID."
  },
  {
    "case_id": "T3-J1-L2-0054",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Healthcare",
    "difficulty": "Medium",
    "trap_type": "T6",
    "trap_family": "F3",
    "trap_subtype": "Prior Ignorance",
    "scenario": "A drug safety report shows that most side-effect reports involve Drug A, raising concerns about its safety.\nDrug A is prescribed far more frequently than alternatives.",
    "claim": "A drug safety report shows that most side-effect reports involve Drug A, raising concerns about its safety",
    "variables": {
      "X": {
        "name": "Drug type",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Side-effect report",
        "role": "Outcome"
      },
      "Z": {
        "name": "Prescription frequency",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Were the case counts for Side-effect report measured over the same time window as the base rate/denominator Prescription frequency, and are we comparing rates rather than raw counts?",
    "conditional_answers": {
      "A": "Answer if you only have raw counts of Side-effect report: You cannot infer risk or effectiveness because the base population sizes may differ.",
      "B": "Answer if you compute rates for Side-effect report with denominator/base rate Prescription frequency: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.",
      "C": "Answer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Side-effect report are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Prescription frequency (and by subgroup/time) before concluding anything.",
    "causal_structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
    "key_insight": "Report volume follows usage volume.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-54",
    "_original_title": "Medical Side-Effect Reports",
    "_questions": "",
    "_expected_analysis": "Compare side-effects per prescription.\nConclusion: Safety inference is INVALID.\nL2-F (Measurement / Denominator Mismatch) — COMPLETE (9 cases)"
  },
  {
    "case_id": "T3-J1-L2-0055",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education Statistics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A school district reports that students are in larger classes than ever before, even though the average class size per school has decreased. Officials argue that overcrowding is worsening.\nThe discrepancy arises because larger schools with many small classes enroll more students, while smaller schools with fewer large classes enroll fewer students.",
    "claim": "students are in larger classes than ever before, even though the average class size per school has decreased",
    "variables": {
      "X": {
        "name": "School",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Class size",
        "role": "Outcome"
      },
      "Z": {
        "name": "Weighting scheme (student-weighted vs. class-weighted)",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in School alter the composition (Weighting scheme (student-weighted vs. class-weighted)) of who is counted before Class size was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Class size after changing School can reflect a real outcome shift.",
      "B": "Answer if School changes who is counted via Weighting scheme (student-weighted vs. class-weighted): The aggregate Class size can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Class size may be moving because the denominator/population changed after School via composition variable Weighting scheme (student-weighted vs. class-weighted). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Different averaging schemes produce conflicting summaries without any underlying change.",
    "key_insight": "“Average” depends on what is being averaged.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-55",
    "_original_title": "The Class Size Paradox",
    "_questions": "Are students actually experiencing larger classes?\nWhy do the two averages disagree?",
    "_expected_analysis": "Class-weighted and student-weighted averages differ.\nNo contradiction exists.\nConclusion: Overcrowding claim is CONDITIONAL.\nWise refusal: Specify the unit of analysis."
  },
  {
    "case_id": "T3-J1-L2-0056",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Economics",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city reports that average income increased, while surveys show most residents feel poorer. Officials dismiss public concern.\nIncome growth is driven by gains among a small number of very high earners.",
    "claim": "average income increased, while surveys show most residents feel poorer",
    "variables": {
      "X": {
        "name": "Time period",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Income",
        "role": "Outcome"
      },
      "Z": {
        "name": "Mean vs median statistic",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Time period alter the composition (Mean vs median statistic) of who is counted before Income was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Income after changing Time period can reflect a real outcome shift.",
      "B": "Answer if Time period changes who is counted via Mean vs median statistic: The aggregate Income can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Income may be moving because the denominator/population changed after Time period via composition variable Mean vs median statistic. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Means are sensitive to outliers.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-56",
    "_original_title": "The Average Income Misinterpretation",
    "_questions": "",
    "_expected_analysis": "Median income better reflects typical experience.\nConclusion: Prosperity claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0057",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Education",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A university finds that average course ratings increased, and concludes teaching quality improved.\nHowever, high-enrollment courses received lower ratings, while small seminars received higher ratings.",
    "claim": "average course ratings increased, and concludes teaching quality improved",
    "variables": {
      "X": {
        "name": "Course",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Rating",
        "role": "Outcome"
      },
      "Z": {
        "name": "Enrollment weighting",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Course alter the composition (Enrollment weighting) of who is counted before Rating was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Rating after changing Course can reflect a real outcome shift.",
      "B": "Answer if Course changes who is counted via Enrollment weighting: The aggregate Rating can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Rating may be moving because the denominator/population changed after Course via composition variable Enrollment weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Course-weighted averages differ from student-weighted experience.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-57",
    "_original_title": "Course Rating Inflation",
    "_questions": "",
    "_expected_analysis": "Weighting determines interpretation.\nConclusion: Teaching improvement claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0058",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Healthcare Operations",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A hospital reports that average patient wait times decreased, while many patients report longer waits.\nShort visits dominate the average, masking longer waits for complex cases.",
    "claim": "average patient wait times decreased, while many patients report longer waits",
    "variables": {
      "X": {
        "name": "Visit type",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Wait time",
        "role": "Outcome"
      },
      "Z": {
        "name": "Visit weighting",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Visit type alter the composition (Visit weighting) of who is counted before Wait time was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Wait time after changing Visit type can reflect a real outcome shift.",
      "B": "Answer if Visit type changes who is counted via Visit weighting: The aggregate Wait time can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Wait time may be moving because the denominator/population changed after Visit type via composition variable Visit weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Aggregates hide heterogeneity.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-58",
    "_original_title": "Hospital Wait Time Reporting",
    "_questions": "",
    "_expected_analysis": "Typical patient experience differs by visit type.\nConclusion: Efficiency claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0059",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Organizational Behavior",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A firm reports rising average employee satisfaction, despite increased turnover.\nSatisfied long-tenured employees remain, while dissatisfied employees leave.",
    "claim": "A firm reports rising average employee satisfaction, despite increased turnover",
    "variables": {
      "X": {
        "name": "Employee tenure",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Satisfaction score",
        "role": "Outcome"
      },
      "Z": {
        "name": "Survivor weighting",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Employee tenure alter the composition (Survivor weighting) of who is counted before Satisfaction score was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Satisfaction score after changing Employee tenure can reflect a real outcome shift.",
      "B": "Answer if Employee tenure changes who is counted via Survivor weighting: The aggregate Satisfaction score can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Satisfaction score may be moving because the denominator/population changed after Employee tenure via composition variable Survivor weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Averages reflect who remains.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-59",
    "_original_title": "Employee Satisfaction Scores",
    "_questions": "",
    "_expected_analysis": "Satisfaction increase reflects composition of respondents.\nConclusion: Workplace improvement claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0060",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Transportation",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A city reports that average commute times decreased, while drivers complain of worse traffic.\nOff-peak trips increased, lowering the average.",
    "claim": "average commute times decreased, while drivers complain of worse traffic",
    "variables": {
      "X": {
        "name": "Time of travel",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Commute duration",
        "role": "Outcome"
      },
      "Z": {
        "name": "Trip distribution",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Time of travel alter the composition (Trip distribution) of who is counted before Commute duration was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Commute duration after changing Time of travel can reflect a real outcome shift.",
      "B": "Answer if Time of travel changes who is counted via Trip distribution: The aggregate Commute duration can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Commute duration may be moving because the denominator/population changed after Time of travel via composition variable Trip distribution. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Averages depend on trip timing.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-60",
    "_original_title": "Traffic Congestion Metrics",
    "_questions": "",
    "_expected_analysis": "Peak-hour experience worsened.\nConclusion: Traffic improvement claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0061",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Environmental Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A country reports lower per-capita emissions, claiming environmental progress.\nTotal emissions increased due to population growth.",
    "claim": "A country reports lower per-capita emissions, claiming environmental progress",
    "variables": {
      "X": {
        "name": "Population size",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Emissions metric",
        "role": "Outcome"
      },
      "Z": {
        "name": "Per-capita vs total denominator",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Population size alter the composition (Per-capita vs total denominator) of who is counted before Emissions metric was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Emissions metric after changing Population size can reflect a real outcome shift.",
      "B": "Answer if Population size changes who is counted via Per-capita vs total denominator: The aggregate Emissions metric can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Emissions metric may be moving because the denominator/population changed after Population size via composition variable Per-capita vs total denominator. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Different denominators answer different questions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-61",
    "_original_title": "Carbon Emissions Reporting",
    "_questions": "",
    "_expected_analysis": "Both statements can be true.\nConclusion: Progress claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0062",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Energy Policy",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "Appliance A is rated as more energy efficient per use, while households using it consume more total energy.\nAppliance A is used more frequently.",
    "claim": "Appliance A is rated as more energy efficient per use, while households using it consume more total energy",
    "variables": {
      "X": {
        "name": "Appliance type",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Energy use",
        "role": "Outcome"
      },
      "Z": {
        "name": "Usage frequency",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Appliance type alter the composition (Usage frequency) of who is counted before Energy use was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Energy use after changing Appliance type can reflect a real outcome shift.",
      "B": "Answer if Appliance type changes who is counted via Usage frequency: The aggregate Energy use can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Energy use may be moving because the denominator/population changed after Appliance type via composition variable Usage frequency. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Efficiency does not equal total consumption.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-62",
    "_original_title": "Energy Efficiency Ratings",
    "_questions": "",
    "_expected_analysis": "Usage drives totals.\nConclusion: Efficiency claim is CONDITIONAL."
  },
  {
    "case_id": "T3-J1-L2-0063",
    "pearl_level": "L2",
    "domain": "D10",
    "subdomain": "Digital Media",
    "difficulty": "Medium",
    "trap_type": "T1",
    "trap_family": "F1",
    "trap_subtype": "Post-intervention Selection",
    "scenario": "A platform reports higher average engagement per post, while users report declining reach.\nHigh-engagement posts dominate the average, while most posts perform poorly.",
    "claim": "A platform reports higher average engagement per post, while users report declining reach",
    "variables": {
      "X": {
        "name": "Post type",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Engagement metric",
        "role": "Outcome"
      },
      "Z": {
        "name": "Distribution skew",
        "role": "Confounder/Mediator"
      }
    },
    "label": "NO",
    "hidden_question": "Did the intervention/change in Post type alter the composition (Distribution skew) of who is counted before Engagement metric was computed?",
    "conditional_answers": {
      "A": "Answer if the population/denominator is stable: A change in Engagement metric after changing Post type can reflect a real outcome shift.",
      "B": "Answer if Post type changes who is counted via Distribution skew: The aggregate Engagement metric can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.",
      "C": "Answer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Engagement metric may be moving because the denominator/population changed after Post type via composition variable Distribution skew. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement.",
    "causal_structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
    "key_insight": "Skewed distributions distort averages.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-63",
    "_original_title": "Social Media Engagement Rates",
    "_questions": "",
    "_expected_analysis": "Median engagement is more informative.\nConclusion: Platform success claim is CONDITIONAL.\nL3-A (Individual Counterfactuals) — 9 cases"
  },
  {
    "case_id": "T3-J1-L3-0064",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Outcomes",
    "difficulty": "Medium",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A student failed a qualifying exam and chose not to retake it, later leaving the program. The student claims that retaking the exam would not have changed the outcome.\nHowever, historical data shows that students with similar initial scores who retook the exam often passed.",
    "claim": "retaking the exam would not have changed the outcome",
    "variables": {
      "X": {
        "name": "Decision to retake the exam (yes / no)",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Program continuation",
        "role": "Outcome"
      },
      "Z": {
        "name": "Student preparation and ability",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Program continuation occurred—and is selection related to Student preparation and ability or Program continuation?",
    "conditional_answers": {
      "A": "Answer if Decision to retake the exam (yes / no) is randomly assigned: A difference in Program continuation across Decision to retake the exam (yes / no) groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Student preparation and ability): The Decision to retake the exam (yes / no) vs not-Decision to retake the exam (yes / no) difference in Program continuation is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Student preparation and ability) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Student preparation and ability); otherwise Decision to retake the exam (yes / no)–Program continuation differences may reflect selection rather than effect.",
    "causal_structure": "Only one action (retake or not) is observed for the student; the alternative outcome is unobserved.",
    "key_insight": "Individual-level causal claims require counterfactual comparison.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-64",
    "_original_title": "The Missed Exam Retake",
    "_questions": "Can the student conclude that retaking the exam would not have mattered?\nWhy is this counterfactual inherently unobservable?",
    "_expected_analysis": "The student’s outcome under retaking (Y₁) is unobserved.\nSimilar students provide suggestive but imperfect evidence.\nConclusion: The student’s claim is UNDETERMINED.\nWise refusal: Individual causal effects cannot be known without assumptions or experimental design."
  },
  {
    "case_id": "T3-J1-L3-0065",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T10",
    "trap_family": "",
    "trap_subtype": "Outcome-dependent Worlds",
    "scenario": "An individual declined a job offer and later experienced slower career progression. They claim that accepting the offer would not have improved their career.\nComparable candidates who accepted similar offers often advanced more quickly.",
    "claim": "accepting the offer would not have improved their career",
    "variables": {
      "X": {
        "name": "Job offer acceptance",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Career progression",
        "role": "Outcome"
      },
      "Z": {
        "name": "Skill level and career ambition",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Career progression occurred—and is selection related to Skill level and career ambition or Career progression?",
    "conditional_answers": {
      "A": "Answer if Job offer acceptance is randomly assigned: A difference in Career progression across Job offer acceptance groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Skill level and career ambition): The Job offer acceptance vs not-Job offer acceptance difference in Career progression is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Skill level and career ambition) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Skill level and career ambition); otherwise Job offer acceptance–Career progression differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Personal counterfactual outcomes are unobserved.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-65",
    "_original_title": "The Declined Job Offer",
    "_questions": "",
    "_expected_analysis": "Only one trajectory is observed.\nPeer comparisons are imperfect substitutes.\nConclusion: Claim is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0066",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Healthcare",
    "difficulty": "Hard",
    "trap_type": "T10",
    "trap_family": "",
    "trap_subtype": "Outcome-dependent Worlds",
    "scenario": "A patient declined a recommended medical treatment and later recovered naturally. They conclude the treatment was unnecessary.\nClinical evidence suggests that many patients who refused treatment deteriorated.",
    "claim": "A patient declined a recommended medical treatment and later recovered naturally",
    "variables": {
      "X": {
        "name": "Treatment acceptance",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Health outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Disease severity",
        "role": "Confounder/Mediator"
      }
    },
    "label": "INVALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Health outcome occurred—and is selection related to Disease severity or Health outcome?",
    "conditional_answers": {
      "A": "Answer if Treatment acceptance is randomly assigned: A difference in Health outcome across Treatment acceptance groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Disease severity): The Treatment acceptance vs not-Treatment acceptance difference in Health outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Disease severity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Disease severity); otherwise Treatment acceptance–Health outcome differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Recovery does not reveal what would have happened under treatment.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-66",
    "_original_title": "The Medical Treatment Refusal",
    "_questions": "",
    "_expected_analysis": "Natural recovery does not imply treatment uselessness.\nConclusion: Claim is INVALID."
  },
  {
    "case_id": "T3-J1-L3-0067",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T10",
    "trap_family": "",
    "trap_subtype": "Outcome-dependent Worlds",
    "scenario": "A parole board denies parole to an inmate, who later reoffends after release. The board claims parole denial was justified.\nThe counterfactual—what would have happened if parole had been granted earlier—is unobserved.",
    "claim": "A parole board denies parole to an inmate, who later reoffends after release",
    "variables": {
      "X": {
        "name": "Parole decision",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Recidivism",
        "role": "Outcome"
      },
      "Z": {
        "name": "Risk profile",
        "role": "Confounder/Mediator"
      }
    },
    "label": "INVALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Recidivism occurred—and is selection related to Risk profile or Recidivism?",
    "conditional_answers": {
      "A": "Answer if Parole decision is randomly assigned: A difference in Recidivism across Parole decision groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Risk profile): The Parole decision vs not-Parole decision difference in Recidivism is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Risk profile) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Risk profile); otherwise Parole decision–Recidivism differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Observed outcomes do not validate the decision taken.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-67",
    "_original_title": "The Parole Board Decision",
    "_questions": "",
    "_expected_analysis": "Reoffending does not prove denial was optimal.\nConclusion: Claim is INVALID."
  },
  {
    "case_id": "T3-J1-L3-0068",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A student narrowly missed a scholarship cutoff and later struggled financially. Administrators argue the cutoff was fair.\nWhether the student would have succeeded with the scholarship is unobserved.",
    "claim": "A student narrowly missed a scholarship cutoff and later struggled financially",
    "variables": {
      "X": {
        "name": "Scholarship receipt",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Academic success",
        "role": "Outcome"
      },
      "Z": {
        "name": "Financial stability",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Academic success occurred—and is selection related to Financial stability or Academic success?",
    "conditional_answers": {
      "A": "Answer if Scholarship receipt is randomly assigned: A difference in Academic success across Scholarship receipt groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Financial stability): The Scholarship receipt vs not-Scholarship receipt difference in Academic success is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Financial stability) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Financial stability); otherwise Scholarship receipt–Academic success differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Small differences around cutoffs hide large causal uncertainty.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-68",
    "_original_title": "The Scholarship Cutoff",
    "_questions": "",
    "_expected_analysis": "Near-threshold comparisons are suggestive but uncertain.\nConclusion: Claim is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0069",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Psychology",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A patient drops out of therapy early and does not improve. They claim therapy was ineffective for them.\nThe outcome had they completed therapy is unknown.",
    "claim": "A patient drops out of therapy early and does not improve",
    "variables": {
      "X": {
        "name": "Therapy completion",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Mental health outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Adherence capacity",
        "role": "Confounder/Mediator"
      }
    },
    "label": "INVALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Mental health outcome occurred—and is selection related to Adherence capacity or Mental health outcome?",
    "conditional_answers": {
      "A": "Answer if Therapy completion is randomly assigned: A difference in Mental health outcome across Therapy completion groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Adherence capacity): The Therapy completion vs not-Therapy completion difference in Mental health outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Adherence capacity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Adherence capacity); otherwise Therapy completion–Mental health outcome differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Dropout obscures treatment counterfactual.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-69",
    "_original_title": "The Therapy Dropout",
    "_questions": "",
    "_expected_analysis": "Non-completion masks potential benefit.\nConclusion: Claim is INVALID."
  },
  {
    "case_id": "T3-J1-L3-0070",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Finance",
    "difficulty": "Hard",
    "trap_type": "T10",
    "trap_family": "",
    "trap_subtype": "Outcome-dependent Worlds",
    "scenario": "An investor chose not to invest in a startup that later succeeded. They conclude investing would have yielded large returns.\nThe outcome had they invested—including dilution, exit timing, or failure—is unknowable.",
    "claim": "An investor chose not to invest in a startup that later succeeded",
    "variables": {
      "X": {
        "name": "Investment decision",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Financial return",
        "role": "Outcome"
      },
      "Z": {
        "name": "Market volatility",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Financial return occurred—and is selection related to Market volatility or Financial return?",
    "conditional_answers": {
      "A": "Answer if Investment decision is randomly assigned: A difference in Financial return across Investment decision groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Market volatility): The Investment decision vs not-Investment decision difference in Financial return is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Market volatility) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Market volatility); otherwise Investment decision–Financial return differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Observing success does not reveal individual counterfactual payoff.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-70",
    "_original_title": "The Missed Investment Opportunity",
    "_questions": "",
    "_expected_analysis": "Success path is not deterministic.\nConclusion: Claim is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0071",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A parent chose a private school for their child, who later excelled academically. The parent claims public school would have led to worse outcomes.\nThe child’s public-school trajectory is unobserved.",
    "claim": "A parent chose a private school for their child, who later excelled academically",
    "variables": {
      "X": {
        "name": "School choice",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Academic outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Family support",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Academic outcome occurred—and is selection related to Family support or Academic outcome?",
    "conditional_answers": {
      "A": "Answer if School choice is randomly assigned: A difference in Academic outcome across School choice groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Family support): The School choice vs not-School choice difference in Academic outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Family support) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Family support); otherwise School choice–Academic outcome differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Success does not validate the chosen path.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-71",
    "_original_title": "The Alternative School Choice",
    "_questions": "",
    "_expected_analysis": "Only one schooling path observed.\nConclusion: Claim is UNDETERMINED.\nL3-B (Policy Counterfactuals) — 6 cases"
  },
  {
    "case_id": "T3-J1-L3-0072",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Labor Economics",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A city raises the minimum wage, after which employment levels remain stable. Officials claim the policy had no negative employment effects.\nHowever, the counterfactual—what employment would have been without the wage increase—is unobserved. Economic conditions were improving during the same period.",
    "claim": "A city raises the minimum wage, after which employment levels remain stable",
    "variables": {
      "X": {
        "name": "Minimum wage policy",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Employment level",
        "role": "Outcome"
      },
      "Z": {
        "name": "Economic trend",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Employment level occurred—and is selection related to Economic trend or Employment level?",
    "conditional_answers": {
      "A": "Answer if Minimum wage policy is randomly assigned: A difference in Employment level across Minimum wage policy groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Economic trend): The Minimum wage policy vs not-Minimum wage policy difference in Employment level is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Economic trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Economic trend); otherwise Minimum wage policy–Employment level differences may reflect selection rather than effect.",
    "causal_structure": "Observed employment reflects both policy and macroeconomic forces.",
    "key_insight": "Stable outcomes do not imply zero policy effect.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-72",
    "_original_title": "The Minimum Wage Increase",
    "_questions": "Can we conclude the wage increase had no effect on employment?\nWhy is the counterfactual employment trajectory unobservable?",
    "_expected_analysis": "Employment without the policy is unobserved.\nEconomic growth may mask negative effects.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires synthetic control or difference-in-differences."
  },
  {
    "case_id": "T3-J1-L3-0073",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Criminal Justice",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A city shifts to a community policing strategy and observes a decline in crime. Officials credit the strategy.\nNeighboring cities without the policy also experienced crime declines.",
    "claim": "A city shifts to a community policing strategy and observes a decline in crime",
    "variables": {
      "X": {
        "name": "Policing strategy",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Crime rate",
        "role": "Outcome"
      },
      "Z": {
        "name": "Regional crime trend",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Crime rate occurred—and is selection related to Regional crime trend or Crime rate?",
    "conditional_answers": {
      "A": "Answer if Policing strategy is randomly assigned: A difference in Crime rate across Policing strategy groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Regional crime trend): The Policing strategy vs not-Policing strategy difference in Crime rate is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Regional crime trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Regional crime trend); otherwise Policing strategy–Crime rate differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Common trends obscure counterfactual outcomes.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-73",
    "_original_title": "The Policing Strategy Shift",
    "_questions": "",
    "_expected_analysis": "Crime may have declined anyway.\nConclusion: Policy impact claim is UNDETERMINED.\nWise refusal: Requires comparison cities."
  },
  {
    "case_id": "T3-J1-L3-0074",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Education Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A state increases education funding, and student outcomes improve. Legislators claim success.\nOther reforms were implemented simultaneously.",
    "claim": "A state increases education funding, and student outcomes improve",
    "variables": {
      "X": {
        "name": "Funding reform",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Student outcomes",
        "role": "Outcome"
      },
      "Z": {
        "name": "Concurrent reforms",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Student outcomes occurred—and is selection related to Concurrent reforms or Student outcomes?",
    "conditional_answers": {
      "A": "Answer if Funding reform is randomly assigned: A difference in Student outcomes across Funding reform groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Concurrent reforms): The Funding reform vs not-Funding reform difference in Student outcomes is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Concurrent reforms) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Concurrent reforms); otherwise Funding reform–Student outcomes differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Isolating policy effects requires disentangling reforms.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-74",
    "_original_title": "Education Funding Reform",
    "_questions": "",
    "_expected_analysis": "Counterfactual funding-only effect is unknown.\nConclusion: Claim is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0075",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Housing Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A city imposes a housing construction ban, after which housing prices stabilize. Officials argue the ban prevented price increases.\nPrices might have stabilized regardless due to slowing demand.",
    "claim": "A city imposes a housing construction ban, after which housing prices stabilize",
    "variables": {
      "X": {
        "name": "Construction ban",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Housing prices",
        "role": "Outcome"
      },
      "Z": {
        "name": "Demand trend",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Housing prices occurred—and is selection related to Demand trend or Housing prices?",
    "conditional_answers": {
      "A": "Answer if Construction ban is randomly assigned: A difference in Housing prices across Construction ban groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Demand trend): The Construction ban vs not-Construction ban difference in Housing prices is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Demand trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Demand trend); otherwise Construction ban–Housing prices differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Observed stability does not reveal prevented change.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-75",
    "_original_title": "Housing Construction Ban",
    "_questions": "",
    "_expected_analysis": "Counterfactual price trajectory unknown.\nConclusion: Claim is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0076",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Environmental Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A country introduces emissions regulations, and emissions fall. Leaders credit regulation.\nGlobal energy prices also rose sharply.",
    "claim": "A country introduces emissions regulations, and emissions fall",
    "variables": {
      "X": {
        "name": "Regulation",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Emissions",
        "role": "Outcome"
      },
      "Z": {
        "name": "Energy price shocks",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Emissions occurred—and is selection related to Energy price shocks or Emissions?",
    "conditional_answers": {
      "A": "Answer if Regulation is randomly assigned: A difference in Emissions across Regulation groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Energy price shocks): The Regulation vs not-Regulation difference in Emissions is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Energy price shocks) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Energy price shocks); otherwise Regulation–Emissions differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "External forces affect counterfactual emissions.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-76",
    "_original_title": "Environmental Regulation Rollout",
    "_questions": "",
    "_expected_analysis": "Cannot isolate regulation effect.\nConclusion: Claim is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0077",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Transportation Policy",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A city expands public transit, after which traffic congestion eases. Officials credit the expansion.\nRemote work adoption increased during the same period.",
    "claim": "A city expands public transit, after which traffic congestion eases",
    "variables": {
      "X": {
        "name": "Transit expansion",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Congestion",
        "role": "Outcome"
      },
      "Z": {
        "name": "Remote work prevalence",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Congestion occurred—and is selection related to Remote work prevalence or Congestion?",
    "conditional_answers": {
      "A": "Answer if Transit expansion is randomly assigned: A difference in Congestion across Transit expansion groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Remote work prevalence): The Transit expansion vs not-Transit expansion difference in Congestion is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Remote work prevalence) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Remote work prevalence); otherwise Transit expansion–Congestion differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Multiple causes obscure the counterfactual.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-77",
    "_original_title": "Public Transit Expansion",
    "_questions": "",
    "_expected_analysis": "Congestion might have declined anyway.\nConclusion: Claim is UNDETERMINED.\nL3-C (Fairness & Causal Attribution) — 2 cases"
  },
  {
    "case_id": "T3-J1-L3-0078",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "N/A",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A hiring algorithm results in fewer hires from Group A. The company claims the algorithm is discriminatory.\nHowever, the counterfactual—how many from Group A would have been hired without the algorithm—is unobserved.",
    "claim": "A hiring algorithm results in fewer hires from Group A",
    "variables": {
      "X": {
        "name": "Algorithmic hiring",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Hiring outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Applicant qualification distribution",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Hiring outcome occurred—and is selection related to Applicant qualification distribution or Hiring outcome?",
    "conditional_answers": {
      "A": "Answer if Algorithmic hiring is randomly assigned: A difference in Hiring outcome across Algorithmic hiring groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Applicant qualification distribution): The Algorithmic hiring vs not-Algorithmic hiring difference in Hiring outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Applicant qualification distribution) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Applicant qualification distribution); otherwise Algorithmic hiring–Hiring outcome differences may reflect selection rather than effect.",
    "causal_structure": "Disparity does not imply discrimination without a counterfactual baseline.",
    "key_insight": "Fairness claims require causal comparisons.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-78",
    "_original_title": "Algorithmic Hiring Fairness",
    "_questions": "Does the hiring disparity prove discrimination?\nWhat counterfactual is missing?",
    "_expected_analysis": "Need hiring outcomes under alternative process.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires causal fairness evaluation."
  },
  {
    "case_id": "T3-J1-L3-0079",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Sociology & Law",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "A wage gap is observed between two demographic groups. Analysts claim a specific percentage is due to discrimination.\nThe fraction attributable to discrimination depends on the counterfactual wage distribution absent discrimination.",
    "claim": "A wage gap is observed between two demographic groups",
    "variables": {
      "X": {
        "name": "Group membership",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Wages",
        "role": "Outcome"
      },
      "Z": {
        "name": "Job characteristics",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Wages occurred—and is selection related to Job characteristics or Wages?",
    "conditional_answers": {
      "A": "Answer if Group membership is randomly assigned: A difference in Wages across Group membership groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Job characteristics): The Group membership vs not-Group membership difference in Wages is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Job characteristics) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Job characteristics); otherwise Group membership–Wages differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Attribution requires assumptions about counterfactual worlds.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-79",
    "_original_title": "Discrimination Attribution Fraction",
    "_questions": "",
    "_expected_analysis": "Multiple valid counterfactuals exist.\nConclusion: Attribution percentage is UNDETERMINED."
  },
  {
    "case_id": "T3-J1-L3-0080",
    "pearl_level": "L3",
    "domain": "D10",
    "subdomain": "Political Science",
    "difficulty": "Hard",
    "trap_type": "T7",
    "trap_family": "",
    "trap_subtype": "Cross-world Confounder",
    "scenario": "An election was decided by a narrow margin. Commentators claim a specific policy decision “caused” the loss.\nThe election outcome absent that policy is unknowable.",
    "claim": "An election was decided by a narrow margin",
    "variables": {
      "X": {
        "name": "Policy decision",
        "role": "Treatment/Factor"
      },
      "Y": {
        "name": "Election outcome",
        "role": "Outcome"
      },
      "Z": {
        "name": "Voter preferences",
        "role": "Confounder/Mediator"
      }
    },
    "label": "VALID",
    "hidden_question": "At what point were units selected into the observed sample—before or after Election outcome occurred—and is selection related to Voter preferences or Election outcome?",
    "conditional_answers": {
      "A": "Answer if Policy decision is randomly assigned: A difference in Election outcome across Policy decision groups can be interpreted causally.",
      "B": "Answer if participation/exposure is voluntary or selected (e.g., Voter preferences): The Policy decision vs not-Policy decision difference in Election outcome is biased by who ends up observed/treated.",
      "C": "Answer if you can measure the selection drivers (e.g., Voter preferences) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality."
    },
    "wise_refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Voter preferences); otherwise Policy decision–Election outcome differences may reflect selection rather than effect.",
    "causal_structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
    "key_insight": "Singular historical events lack observable counterfactuals.",
    "initial_author": "Sreya Vangara",
    "validator": "Fernando Torres",
    "final_score": 8.5,
    "_original_id": "T3-BucketJ-80",
    "_original_title": "Election Outcome Counterfactual",
    "_questions": "",
    "_expected_analysis": "Many plausible counterfactual worlds.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires formal causal modeling assumptions."
  }
]