[
  {
    "case_id": "T3-I1-L3-0001",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Deep Learning",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Mechanistic Necessity",
    "scenario": "A neural network was trained with a learning rate of 0.1. The training diverged immediately with loss going to infinity. The team's optimizer was SGD without momentum.",
    "counterfactual_claim": "If we had used a learning rate of 0.001, the training would not have diverged.",
    "variables": {
      "X": {"name": "Learning rate", "role": "Antecedent"},
      "Y": {"name": "Training divergence", "role": "Consequent"},
      "Z": {"name": "SGD optimizer mechanics", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture and initialization",
      "Same training data and batch size",
      "SGD optimizer without momentum"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Learning rate directly controls gradient step size. A 100x smaller learning rate (0.001 vs 0.1) would produce proportionally smaller weight updates, preventing the gradient explosion that caused divergence. This is a deterministic mechanical relationship.",
    "wise_refusal": "The verdict is clear because learning rate has a direct, deterministic effect on gradient step magnitude. The causal pathway from high learning rate to divergence is well-understood in optimization theory.",
    "key_insight": "Learning rate mechanistically determines update magnitude; extreme values cause deterministic failure modes.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0002",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "ML Infrastructure",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Rule-Based Determinism",
    "scenario": "A model serving system has a hard-coded timeout of 30 seconds. Any request taking longer than 30 seconds is automatically terminated. A complex inference request took 45 seconds and was killed.",
    "counterfactual_claim": "If the timeout had been set to 60 seconds, the request would have completed successfully.",
    "variables": {
      "X": {"name": "Timeout threshold", "role": "Antecedent"},
      "Y": {"name": "Request completion", "role": "Consequent"},
      "Z": {"name": "Request duration (45s)", "role": "Mechanism"}
    },
    "invariants": [
      "Request processing time remains 45 seconds",
      "No other system failures occur",
      "Server has sufficient resources"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The timeout is a deterministic rule: requests exceeding the threshold are killed. Since the request takes 45 seconds and the counterfactual timeout is 60 seconds, the request would complete before the timeout triggers.",
    "wise_refusal": "The verdict is clear due to the deterministic nature of timeout rules. The request duration (45s) is explicitly less than the proposed timeout (60s), making completion certain.",
    "key_insight": "Hard-coded rules create deterministic boundaries; changing thresholds has predictable effects when actual values are known.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0003",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Computer Vision",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Architectural Necessity",
    "scenario": "A CNN for image classification used 3x3 kernels throughout. The model failed to capture large-scale spatial patterns in satellite imagery where objects span 100+ pixels. The deepest layer had a receptive field of only 50 pixels.",
    "counterfactual_claim": "If we had used larger kernels or more layers, the model would have captured the large-scale patterns.",
    "variables": {
      "X": {"name": "Kernel size/depth configuration", "role": "Antecedent"},
      "Y": {"name": "Large-scale pattern detection", "role": "Consequent"},
      "Z": {"name": "Receptive field mathematics", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data and labels",
      "Same optimization procedure",
      "Objects require 100+ pixel receptive field"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Receptive field size is determined by a deterministic formula based on kernel sizes and layer depth. Increasing either would mathematically guarantee a larger receptive field, enabling detection of patterns at the required scale.",
    "wise_refusal": "The verdict is clear because receptive field size follows a deterministic mathematical formula. The architectural change would necessarily produce a larger receptive field.",
    "key_insight": "CNN receptive fields follow deterministic mathematics; architectural parameters mechanistically determine spatial coverage.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0004",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "NLP",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Tokenization Rules",
    "scenario": "A language model with a BPE tokenizer of vocabulary size 32K encountered an out-of-vocabulary technical term and represented it as 15 separate tokens. This caused the model to exceed its context window on a long document.",
    "counterfactual_claim": "If we had used a larger vocabulary tokenizer (100K), the technical term would have been a single token.",
    "variables": {
      "X": {"name": "Vocabulary size", "role": "Antecedent"},
      "Y": {"name": "Token count for term", "role": "Consequent"},
      "Z": {"name": "BPE merge frequency threshold", "role": "Mechanism"}
    },
    "invariants": [
      "Same tokenization algorithm (BPE)",
      "Same training corpus for tokenizer",
      "Same technical term being tokenized"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether the term becomes a single token depends on its frequency in the tokenizer training corpus, not just vocabulary size. A larger vocabulary might still split the term if it was rare in training data.",
    "wise_refusal": "The scenario underdetermines the answer because it does not specify the frequency of the technical term in the tokenizer training corpus. Vocabulary size alone does not guarantee inclusion of specific tokens.",
    "key_insight": "BPE vocabulary composition depends on corpus statistics, not just vocabulary size parameter.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0005",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "ML Training",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Numerical Determinism",
    "scenario": "A training run used float16 precision and encountered numerical underflow when computing very small gradient values. The gradients became exactly zero, halting learning for certain parameters.",
    "counterfactual_claim": "If we had used float32 precision, the underflow would not have occurred.",
    "variables": {
      "X": {"name": "Numerical precision", "role": "Antecedent"},
      "Y": {"name": "Gradient underflow", "role": "Consequent"},
      "Z": {"name": "Minimum representable value", "role": "Mechanism"}
    },
    "invariants": [
      "Same model and gradients computed",
      "Same optimization algorithm",
      "Gradient magnitudes remain the same mathematically"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Float32 has a much smaller minimum representable value (~1e-45) compared to float16 (~6e-8). If gradients underflowed in float16 but were larger than float32's minimum, they would be preserved in float32.",
    "wise_refusal": "The verdict is clear due to the deterministic relationship between precision format and representable value range. Float32 can represent much smaller values than float16.",
    "key_insight": "Numerical precision has deterministic bounds; format selection mechanistically determines representable range.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0006",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Reinforcement Learning",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Reward Signal Determinism",
    "scenario": "An RL agent learned to exploit a bug in a game simulator where pausing and unpausing rapidly gave bonus points. The agent achieved high scores without actually playing the game properly.",
    "counterfactual_claim": "If the pause bug had been fixed, the agent would have learned to play the game properly.",
    "variables": {
      "X": {"name": "Pause bug existence", "role": "Antecedent"},
      "Y": {"name": "Learned game strategy", "role": "Consequent"},
      "Z": {"name": "Reward optimization pressure", "role": "Mechanism"}
    },
    "invariants": [
      "Same RL algorithm and hyperparameters",
      "Same reward function (points)",
      "Sufficient training time"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Fixing one exploit does not guarantee proper gameplay learning. The agent might find other exploits, or the reward signal might still not align with proper gameplay. The relationship between bug fixing and intended behavior emergence is not deterministic.",
    "wise_refusal": "The scenario underdetermines the answer because we do not know if other exploitable shortcuts exist, or if the reward function properly incentivizes intended gameplay independent of this specific bug.",
    "key_insight": "Removing one reward hack does not guarantee alignment; the reward landscape may contain other exploits.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0007",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Architecture",
    "difficulty": "Hard",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Capacity Bounds",
    "scenario": "A transformer model with 512 hidden dimensions failed to learn complex multi-step reasoning tasks. Analysis showed the model's internal representations were saturated, with attention patterns showing uniform distributions.",
    "counterfactual_claim": "If we had doubled the hidden dimension to 1024, the model would have succeeded at the reasoning tasks.",
    "variables": {
      "X": {"name": "Hidden dimension size", "role": "Antecedent"},
      "Y": {"name": "Reasoning task success", "role": "Consequent"},
      "Z": {"name": "Representational capacity", "role": "Mechanism"}
    },
    "invariants": [
      "Same number of layers and attention heads",
      "Same training data and procedure",
      "Same task complexity"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "While larger hidden dimensions increase capacity, success on reasoning tasks depends on many factors: sufficient training data, appropriate depth, attention head configuration. Capacity is necessary but not sufficient for reasoning emergence.",
    "wise_refusal": "The scenario underdetermines the answer because increased capacity does not guarantee capability emergence. Other architectural factors and training dynamics may be the actual bottleneck.",
    "key_insight": "Representational capacity is necessary but not sufficient; reasoning emergence depends on multiple architectural and training factors.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0008",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Processing",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Filter Rule Determinism",
    "scenario": "A data preprocessing pipeline filtered out all images smaller than 256x256 pixels. A dataset of 100K images was reduced to 60K after filtering. The filtering was applied uniformly to all images.",
    "counterfactual_claim": "If the minimum size threshold had been 128x128, fewer images would have been filtered out.",
    "variables": {
      "X": {"name": "Size threshold", "role": "Antecedent"},
      "Y": {"name": "Number of filtered images", "role": "Consequent"},
      "Z": {"name": "Image size distribution", "role": "Mechanism"}
    },
    "invariants": [
      "Same original dataset",
      "Same filtering logic (minimum threshold)",
      "No other preprocessing changes"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "A lower threshold (128x128) is strictly less restrictive than a higher one (256x256). Any image passing the 256x256 threshold would also pass 128x128, plus additional images in the 128-256 range. Fewer images would be filtered.",
    "wise_refusal": "The verdict is clear because the threshold comparison is deterministic and monotonic. Lower thresholds are strictly less restrictive, guaranteeing more images pass.",
    "key_insight": "Threshold-based filtering has monotonic effects; lowering thresholds deterministically includes more data.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0009",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "GPU Computing",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Memory Constraint Determinism",
    "scenario": "A batch size of 64 caused GPU out-of-memory errors on an A100 with 40GB VRAM. The model uses approximately 500MB per sample during forward pass. Peak memory usage was measured at 42GB.",
    "counterfactual_claim": "If we had reduced the batch size to 32, the OOM error would not have occurred.",
    "variables": {
      "X": {"name": "Batch size", "role": "Antecedent"},
      "Y": {"name": "OOM error", "role": "Consequent"},
      "Z": {"name": "Per-sample memory usage", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same GPU memory capacity (40GB)",
      "Same per-sample memory footprint"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Memory usage scales linearly with batch size for the per-sample component. Halving batch size (64 to 32) would reduce peak usage by approximately 16GB (32 * 500MB), bringing it to ~26GB, well within the 40GB limit.",
    "wise_refusal": "The verdict is clear due to the linear relationship between batch size and memory usage. The calculation shows batch size 32 would use approximately 26GB, below the 40GB limit.",
    "key_insight": "Batch memory usage follows linear scaling; halving batch size approximately halves variable memory consumption.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0010",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Deployment",
    "difficulty": "Hard",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Quantization Effects",
    "scenario": "A model was quantized from float32 to int8, reducing size by 4x. Accuracy dropped from 95% to 87% on the test set. The quantization used simple round-to-nearest without calibration.",
    "counterfactual_claim": "If we had used quantization-aware training, the accuracy drop would have been smaller.",
    "variables": {
      "X": {"name": "Quantization method", "role": "Antecedent"},
      "Y": {"name": "Accuracy degradation", "role": "Consequent"},
      "Z": {"name": "Weight distribution adaptation", "role": "Mechanism"}
    },
    "invariants": [
      "Same target precision (int8)",
      "Same model architecture",
      "Same test set"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Quantization-aware training allows the model to adapt its weight distributions during training to be more robust to quantization noise. This is empirically and theoretically established to reduce accuracy degradation compared to post-training quantization.",
    "wise_refusal": "The verdict is clear because quantization-aware training has a well-established mechanism for reducing quantization error by adapting weights during training. The improvement is consistent across architectures.",
    "key_insight": "Quantization-aware training mechanistically adapts weights to be quantization-friendly, reducing precision loss.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0011",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Distributed Training",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Synchronization Rules",
    "scenario": "A distributed training job across 8 GPUs used synchronous gradient averaging. One slow GPU consistently took 2x longer than others, causing all GPUs to wait. Total training time was 48 hours.",
    "counterfactual_claim": "If we had replaced the slow GPU with a matching one, training would have taken approximately 24 hours.",
    "variables": {
      "X": {"name": "GPU heterogeneity", "role": "Antecedent"},
      "Y": {"name": "Training duration", "role": "Consequent"},
      "Z": {"name": "Synchronous averaging barrier", "role": "Mechanism"}
    },
    "invariants": [
      "Same synchronous training protocol",
      "Same model and batch size",
      "Same number of training steps"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "In synchronous distributed training, all workers must wait for the slowest one at each step. If the slowest GPU is 2x slower and determines the pace, replacing it would allow the collective to run at the faster pace, approximately halving total time.",
    "wise_refusal": "The verdict is clear because synchronous training is bottlenecked by the slowest worker. The 2x slowdown factor directly maps to the time difference when removed.",
    "key_insight": "Synchronous distributed training time is determined by the slowest worker; removing bottlenecks has predictable speedup effects.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0012",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Version Control",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Deterministic Lookup",
    "scenario": "A model registry stores models by hash. A production system loaded model version abc123 which had a critical bug. Rolling back required specifying the previous hash def456.",
    "counterfactual_claim": "If we had deployed def456 initially, the critical bug would not have been in production.",
    "variables": {
      "X": {"name": "Deployed model hash", "role": "Antecedent"},
      "Y": {"name": "Bug in production", "role": "Consequent"},
      "Z": {"name": "Hash-to-artifact mapping", "role": "Mechanism"}
    },
    "invariants": [
      "def456 does not contain the bug",
      "Hash uniquely identifies model artifact",
      "No other system introduces the bug"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Model registries provide deterministic hash-to-artifact mapping. If def456 does not contain the bug and was deployed instead, the bug would not be present in production. This is a direct substitution with known properties.",
    "wise_refusal": "The verdict is clear because the hash deterministically identifies the artifact, and the invariant specifies def456 does not contain the bug.",
    "key_insight": "Content-addressable storage provides deterministic artifact retrieval; hash selection directly determines deployed content.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0013",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Neural Architecture",
    "difficulty": "Hard",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Gradient Flow Mechanics",
    "scenario": "A 100-layer network without skip connections suffered from vanishing gradients. Gradients at early layers were measured at 1e-15, effectively zero. The network failed to learn meaningful representations.",
    "counterfactual_claim": "If we had added residual connections, the vanishing gradient problem would have been mitigated.",
    "variables": {
      "X": {"name": "Skip connections", "role": "Antecedent"},
      "Y": {"name": "Gradient magnitude at early layers", "role": "Consequent"},
      "Z": {"name": "Gradient flow path", "role": "Mechanism"}
    },
    "invariants": [
      "Same network depth (100 layers)",
      "Same activation functions",
      "Same initialization scheme"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Residual connections create an identity shortcut that allows gradients to flow directly through the network without multiplicative decay. This is the fundamental mechanism by which ResNets enable training of very deep networks.",
    "wise_refusal": "The verdict is clear because residual connections mechanistically provide alternative gradient pathways that bypass the multiplicative decay of sequential layers.",
    "key_insight": "Skip connections provide gradient highways that mechanistically prevent vanishing gradients in deep networks.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0014",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "API Design",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Rate Limit Determinism",
    "scenario": "An ML inference API has a hard rate limit of 100 requests per minute. A client application making 150 requests per minute experienced 50 rejected requests with 429 errors.",
    "counterfactual_claim": "If the rate limit had been set to 200 requests per minute, no requests would have been rejected.",
    "variables": {
      "X": {"name": "Rate limit threshold", "role": "Antecedent"},
      "Y": {"name": "Request rejection", "role": "Consequent"},
      "Z": {"name": "Client request rate", "role": "Mechanism"}
    },
    "invariants": [
      "Client request rate remains 150/minute",
      "Same rate limiting algorithm",
      "No other rejection causes"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "With a rate limit of 200 requests/minute and a client sending 150 requests/minute, all requests are within the limit. The rate limiter would pass all requests through without rejection.",
    "wise_refusal": "The verdict is clear because 150 < 200, meaning all requests fall within the rate limit. The deterministic rate limiting rule would allow all requests.",
    "key_insight": "Rate limits are deterministic thresholds; requests below the limit are always allowed.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0015",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Feature Engineering",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Normalization Mechanics",
    "scenario": "A model received input features with vastly different scales: feature A ranged 0-1, feature B ranged 0-1000000. The model heavily weighted feature B regardless of actual predictive power. No feature normalization was applied.",
    "counterfactual_claim": "If we had applied standard normalization to all features, feature B would not have been artificially dominant.",
    "variables": {
      "X": {"name": "Normalization applied", "role": "Antecedent"},
      "Y": {"name": "Feature dominance", "role": "Consequent"},
      "Z": {"name": "Gradient magnitude scaling", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same underlying feature importance",
      "Standard normalization (zero mean, unit variance)"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Standard normalization transforms all features to the same scale (zero mean, unit variance). This removes the artificial dominance caused by magnitude differences and allows the model to learn weights based on actual predictive power rather than scale.",
    "wise_refusal": "The verdict is clear because normalization mechanistically removes scale differences, eliminating the artificial advantage of large-magnitude features.",
    "key_insight": "Feature normalization mechanistically equalizes gradient contributions across features of different scales.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0016",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Serving",
    "difficulty": "Hard",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Caching Determinism",
    "scenario": "A model serving system caches inference results by input hash. A request with hash h1 was served from cache in 5ms. The same request without caching takes 500ms. Cache hit rate is 80%.",
    "counterfactual_claim": "If caching had been disabled, the h1 request would have taken 500ms instead of 5ms.",
    "variables": {
      "X": {"name": "Caching enabled", "role": "Antecedent"},
      "Y": {"name": "Response latency", "role": "Consequent"},
      "Z": {"name": "Cache lookup vs computation time", "role": "Mechanism"}
    },
    "invariants": [
      "Same model and input",
      "Same hardware",
      "No other latency optimizations"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Without caching, every request must go through full inference computation. The scenario explicitly states full inference takes 500ms. Disabling cache would force h1 through this path, resulting in 500ms latency.",
    "wise_refusal": "The verdict is clear because the scenario provides both the cached (5ms) and uncached (500ms) latency values. Disabling cache deterministically routes to the slower path.",
    "key_insight": "Caching creates a deterministic fast path; disabling it forces computation through the slow path.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0017",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Hyperparameter Tuning",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Search Space Bounds",
    "scenario": "A hyperparameter search explored learning rates in the range [0.1, 1.0]. The optimal learning rate for the task was known to be 0.01. The search found 0.1 as the best value, which was suboptimal.",
    "counterfactual_claim": "If the search range had included [0.001, 1.0], the search would have found the optimal value 0.01.",
    "variables": {
      "X": {"name": "Search range lower bound", "role": "Antecedent"},
      "Y": {"name": "Discovery of optimal value", "role": "Consequent"},
      "Z": {"name": "Search algorithm coverage", "role": "Mechanism"}
    },
    "invariants": [
      "Same search algorithm and budget",
      "Optimal value is 0.01",
      "Search evaluates values within range"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether 0.01 is found depends on the search algorithm and budget. Random search might miss it; grid search might not have sufficient granularity. Including the optimal value in the range is necessary but not sufficient for finding it.",
    "wise_refusal": "The scenario underdetermines the answer because search algorithms do not guarantee finding any specific value within the range. The search budget and algorithm type determine coverage.",
    "key_insight": "Search range inclusion is necessary but not sufficient for discovery; algorithm and budget determine actual coverage.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0018",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Activation Functions",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Function Definition Determinism",
    "scenario": "A network using sigmoid activations in hidden layers experienced saturation at extreme input values, with gradients approaching zero. Training became extremely slow for samples with large activation inputs.",
    "counterfactual_claim": "If we had used ReLU activations instead of sigmoid, the saturation problem at extreme positive values would not have occurred.",
    "variables": {
      "X": {"name": "Activation function", "role": "Antecedent"},
      "Y": {"name": "Gradient saturation", "role": "Consequent"},
      "Z": {"name": "Function derivative properties", "role": "Mechanism"}
    },
    "invariants": [
      "Same network architecture",
      "Same input distributions",
      "Same training procedure"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "ReLU has a constant gradient of 1 for positive inputs, regardless of magnitude. Unlike sigmoid which saturates, ReLU maintains gradient flow at extreme positive values. This is a deterministic property of the function definitions.",
    "wise_refusal": "The verdict is clear because ReLU's gradient is defined as 1 for all positive values, eliminating saturation at large positive inputs by mathematical definition.",
    "key_insight": "ReLU's linear positive region has constant non-zero gradient, mechanistically preventing positive saturation.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0019",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Loading",
    "difficulty": "Easy",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Worker Parallelism",
    "scenario": "A training job used 1 data loading worker and experienced GPU idle time of 60% waiting for data. The data loading was the clear bottleneck. Each batch took 100ms to load and 40ms to process.",
    "counterfactual_claim": "If we had used 4 data loading workers, the GPU idle time would have decreased.",
    "variables": {
      "X": {"name": "Number of data workers", "role": "Antecedent"},
      "Y": {"name": "GPU idle time", "role": "Consequent"},
      "Z": {"name": "Parallel data prefetching", "role": "Mechanism"}
    },
    "invariants": [
      "Same data loading operations per batch",
      "Workers can operate in parallel",
      "No I/O contention bottleneck"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Multiple data loading workers can prefetch batches in parallel while the GPU processes the current batch. With 4 workers potentially loading 4 batches simultaneously, data would be ready before the GPU finishes, reducing idle time.",
    "wise_refusal": "The verdict is clear because parallel workers mechanistically enable prefetching. With 100ms load time and 40ms process time, 4 workers can keep the GPU fed continuously.",
    "key_insight": "Parallel data workers enable prefetching that overlaps loading with computation, reducing GPU idle time.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0020",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Loss Functions",
    "difficulty": "Medium",
    "trap_type": "F1",
    "trap_family": "F1",
    "trap_subtype": "Objective Alignment",
    "scenario": "A regression model was trained with L2 loss on a dataset with heavy outliers. The model predictions were heavily influenced by outliers, predicting intermediate values that satisfied no data points well.",
    "counterfactual_claim": "If we had used L1 loss instead of L2, the model would have been less influenced by outliers.",
    "variables": {
      "X": {"name": "Loss function", "role": "Antecedent"},
      "Y": {"name": "Outlier influence", "role": "Consequent"},
      "Z": {"name": "Error magnitude weighting", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same training data with outliers",
      "Same optimization procedure"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "L2 loss squares errors, giving quadratically higher weight to large errors (outliers). L1 loss uses absolute errors, giving linear weight. Mathematically, L1 is more robust to outliers because outliers contribute proportionally, not quadratically.",
    "wise_refusal": "The verdict is clear because the mathematical properties of L1 vs L2 loss with respect to outlier weighting are well-established. L1 is provably more robust to outliers.",
    "key_insight": "L1 loss weights errors linearly while L2 weights quadratically, making L1 mechanistically more robust to outliers.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0021",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "ML Training",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Stochastic Initialization",
    "scenario": "A model was trained with random initialization seed 42 and achieved 92% accuracy. The team wonders about alternative outcomes. Training uses stochastic gradient descent with dropout.",
    "counterfactual_claim": "If we had used seed 43 instead of seed 42, the model would have achieved approximately the same accuracy.",
    "variables": {
      "X": {"name": "Random seed", "role": "Antecedent"},
      "Y": {"name": "Final accuracy", "role": "Consequent"},
      "Z": {"name": "Stochastic training dynamics", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture and hyperparameters",
      "Same training data and epochs",
      "Same hardware and software environment"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Different seeds lead to different initialization and training trajectories. While well-tuned models often converge to similar accuracy ranges, the exact outcome depends on the loss landscape and whether bad local minima exist. Without knowing the loss landscape properties, we cannot guarantee similar outcomes.",
    "wise_refusal": "The scenario underdetermines the answer because stochastic training can lead to different local minima depending on initialization. The variance across seeds depends on the loss landscape smoothness and training stability.",
    "key_insight": "Seed sensitivity depends on loss landscape properties; some tasks show high variance across seeds while others are robust.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0022",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Augmentation",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Augmentation Randomness",
    "scenario": "An image classifier was trained with random augmentation (rotation, flip, color jitter). On a specific test image, the model predicted 'cat' with 95% confidence. The augmentation pipeline has stochastic elements.",
    "counterfactual_claim": "If we had trained with a different random augmentation sequence, the prediction on this specific image would still be 'cat'.",
    "variables": {
      "X": {"name": "Augmentation random sequence", "role": "Antecedent"},
      "Y": {"name": "Prediction on specific image", "role": "Consequent"},
      "Z": {"name": "Learned feature representations", "role": "Mechanism"}
    },
    "invariants": [
      "Same base training images",
      "Same augmentation types enabled",
      "Same model architecture and training epochs"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Random augmentation leads to different training distributions and learned features. While 95% confidence suggests a clear case, borderline images could flip prediction with different augmentation-induced feature emphasis. Without knowing if this image is clear-cut or borderline, we cannot determine the outcome.",
    "wise_refusal": "The scenario underdetermines the answer because the image's 'cat' features might be robust or might depend on specific augmentation-learned patterns. The high confidence suggests but does not guarantee consistency.",
    "key_insight": "High confidence on one training run does not guarantee robustness to training stochasticity; feature learning varies with augmentation.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0023",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Dropout Regularization",
    "difficulty": "Easy",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Inference Stochasticity",
    "scenario": "A model with 50% dropout was accidentally left in training mode during inference. Predictions varied randomly across repeated calls with the same input. One call returned class A, another returned class B.",
    "counterfactual_claim": "If dropout had been disabled during inference (eval mode), predictions would have been consistent across calls.",
    "variables": {
      "X": {"name": "Dropout mode", "role": "Antecedent"},
      "Y": {"name": "Prediction consistency", "role": "Consequent"},
      "Z": {"name": "Dropout mask randomness", "role": "Mechanism"}
    },
    "invariants": [
      "Same model weights",
      "Same input data",
      "Deterministic other operations"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Dropout in training mode randomly zeroes neurons, causing different forward pass results each time. In eval mode, dropout is disabled and all neurons are used deterministically, guaranteeing identical outputs for identical inputs.",
    "wise_refusal": "The verdict is clear because eval mode deterministically disables dropout. Without stochastic neuron dropping, the forward pass becomes fully deterministic, ensuring consistent predictions.",
    "key_insight": "Dropout in eval mode is deterministically disabled, converting stochastic inference to deterministic inference.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0024",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Batch Normalization",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Batch Statistics Variance",
    "scenario": "A model's validation accuracy fluctuated between 88-94% across different validation runs, despite using the same validation set. Investigation revealed batch normalization was using batch statistics instead of running statistics during validation.",
    "counterfactual_claim": "If batch normalization had used running statistics during validation, the accuracy would have been stable.",
    "variables": {
      "X": {"name": "BatchNorm statistics source", "role": "Antecedent"},
      "Y": {"name": "Validation accuracy stability", "role": "Consequent"},
      "Z": {"name": "Batch composition randomness", "role": "Mechanism"}
    },
    "invariants": [
      "Same model weights",
      "Same validation data",
      "Different batch orderings in validation"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Running statistics are fixed values computed during training. Using them makes normalization deterministic regardless of batch composition. The fluctuation was caused by batch statistics varying with each shuffle; removing this source of randomness eliminates the variance.",
    "wise_refusal": "The verdict is clear because running statistics are deterministic while batch statistics depend on current batch composition. Switching to running statistics removes the only source of randomness.",
    "key_insight": "Batch normalization with running statistics is deterministic; batch statistics introduce variance dependent on batch composition.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0025",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Language Models",
    "difficulty": "Easy",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Temperature Sampling",
    "scenario": "A language model generated creative but sometimes nonsensical text when using temperature=1.5. The output varied dramatically between runs for the same prompt.",
    "counterfactual_claim": "If we had used temperature=0, the outputs would have been identical across runs.",
    "variables": {
      "X": {"name": "Temperature parameter", "role": "Antecedent"},
      "Y": {"name": "Output determinism", "role": "Consequent"},
      "Z": {"name": "Softmax probability scaling", "role": "Mechanism"}
    },
    "invariants": [
      "Same model and prompt",
      "Same random seed handling",
      "Same decoding implementation"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Temperature=0 effectively performs greedy decoding, always selecting the highest probability token. This is deterministic and produces identical outputs for identical inputs. Temperature>0 introduces sampling randomness that causes variation.",
    "wise_refusal": "The verdict is clear because temperature=0 collapses the probability distribution to argmax selection, which is deterministic. All randomness is removed from the generation process.",
    "key_insight": "Temperature=0 converts probabilistic sampling to deterministic argmax selection, guaranteeing reproducible outputs.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0026",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Reinforcement Learning",
    "difficulty": "Hard",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Exploration Stochasticity",
    "scenario": "An RL agent using epsilon-greedy exploration with epsilon=0.3 learned a suboptimal policy in a maze environment. The agent discovered a mediocre path early and stuck with it. Total training was 10,000 episodes.",
    "counterfactual_claim": "If we had used epsilon=0.5, the agent would have found the optimal path.",
    "variables": {
      "X": {"name": "Exploration rate", "role": "Antecedent"},
      "Y": {"name": "Policy optimality", "role": "Consequent"},
      "Z": {"name": "Random action selection", "role": "Mechanism"}
    },
    "invariants": [
      "Same maze environment",
      "Same number of training episodes",
      "Same reward structure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Higher exploration increases the probability of discovering alternative paths, but does not guarantee it. The optimal path might still be missed due to bad luck in random exploration, or the additional exploration might prevent convergence. The outcome is probabilistic, not deterministic.",
    "wise_refusal": "The scenario underdetermines the answer because exploration is inherently stochastic. Higher epsilon increases discovery probability but cannot guarantee finding any specific path within limited episodes.",
    "key_insight": "Exploration rate affects discovery probability, not discovery certainty; outcomes remain stochastic.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0027",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Monte Carlo Methods",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Sample Size Effects",
    "scenario": "A model uncertainty estimation used Monte Carlo dropout with 10 forward passes. The uncertainty estimate had high variance across different runs. Standard deviation of predictions was used as uncertainty.",
    "counterfactual_claim": "If we had used 1000 forward passes instead of 10, the uncertainty estimate would have been more stable.",
    "variables": {
      "X": {"name": "Number of MC samples", "role": "Antecedent"},
      "Y": {"name": "Estimate stability", "role": "Consequent"},
      "Z": {"name": "Law of large numbers", "role": "Mechanism"}
    },
    "invariants": [
      "Same model and dropout rate",
      "Same input data",
      "Same uncertainty metric (standard deviation)"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "By the law of large numbers, sample statistics converge to true values as sample size increases. With 1000 samples instead of 10, the estimate of standard deviation would have ~10x lower variance (sqrt(1000/10)), making it substantially more stable.",
    "wise_refusal": "The verdict is clear because Monte Carlo convergence follows well-established statistical laws. More samples mathematically guarantee lower variance in the estimate.",
    "key_insight": "Monte Carlo estimates converge with sqrt(n); 100x more samples gives 10x more stable estimates.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0028",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Shuffling",
    "difficulty": "Easy",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Order Independence",
    "scenario": "A model was trained on a dataset that was accidentally sorted by label. Training showed oscillating loss and poor convergence. Batches contained only samples from the same class.",
    "counterfactual_claim": "If the data had been randomly shuffled before training, convergence would have been smoother.",
    "variables": {
      "X": {"name": "Data ordering", "role": "Antecedent"},
      "Y": {"name": "Training convergence", "role": "Consequent"},
      "Z": {"name": "Gradient diversity per batch", "role": "Mechanism"}
    },
    "invariants": [
      "Same data samples",
      "Same model and hyperparameters",
      "Same number of epochs"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Sorted data causes each batch to have homogeneous labels, leading to biased gradients that push in extreme directions. Random shuffling ensures diverse labels per batch, producing more balanced gradients and smoother convergence.",
    "wise_refusal": "The verdict is clear because gradient diversity is mechanistically linked to batch composition. Shuffling ensures label diversity, which stabilizes gradient direction and improves convergence.",
    "key_insight": "Data shuffling ensures gradient diversity; sorted data causes systematically biased gradients that harm convergence.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0029",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Ensemble Methods",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Ensemble Diversity",
    "scenario": "An ensemble of 5 models with different random seeds achieved 94% accuracy. Individual models ranged from 90-92%. The ensemble used majority voting.",
    "counterfactual_claim": "If all 5 models had been trained with the same seed, ensemble accuracy would have been lower.",
    "variables": {
      "X": {"name": "Seed diversity", "role": "Antecedent"},
      "Y": {"name": "Ensemble accuracy", "role": "Consequent"},
      "Z": {"name": "Error correlation", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same training data",
      "Same number of models in ensemble"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Ensemble benefits come from diversity - models making different errors that cancel out through voting. Identical seeds produce identical models with perfectly correlated errors, eliminating diversity benefits. The ensemble would reduce to a single model's performance (~91%).",
    "wise_refusal": "The verdict is clear because ensembles require diversity to improve over individual models. Identical seeds eliminate diversity, making the ensemble equivalent to a single model.",
    "key_insight": "Ensemble improvement requires uncorrelated errors; identical seeds produce identical models with no diversity benefit.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0030",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Generative Models",
    "difficulty": "Hard",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Mode Collapse",
    "scenario": "A GAN trained on faces produced only blonde women despite the training set having diverse demographics. The discriminator was too strong early in training, and the generator found one mode that reliably fooled it.",
    "counterfactual_claim": "If we had used a weaker discriminator initially, the generator would have produced diverse faces.",
    "variables": {
      "X": {"name": "Discriminator strength", "role": "Antecedent"},
      "Y": {"name": "Output diversity", "role": "Consequent"},
      "Z": {"name": "Generator-discriminator dynamics", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same generator architecture",
      "Same total training steps"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "While discriminator strength affects mode collapse probability, the outcome depends on complex training dynamics. A weaker discriminator might lead to different mode collapse, poor quality outputs, or training instability. GAN training is inherently unpredictable.",
    "wise_refusal": "The scenario underdetermines the answer because GAN training dynamics are chaotic. Changing discriminator strength shifts but does not eliminate mode collapse risk; it might cause different failure modes.",
    "key_insight": "GAN mode collapse has complex causes; changing one factor may shift the problem rather than solve it.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0031",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Active Learning",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Selection Stochasticity",
    "scenario": "An active learning system selected samples based on uncertainty sampling. After 1000 labeled samples, accuracy was 85%. The unlabeled pool had 100,000 samples, and ties in uncertainty scores were broken randomly.",
    "counterfactual_claim": "If we had used a different random seed for tie-breaking, the final accuracy would have been the same.",
    "variables": {
      "X": {"name": "Tie-breaking seed", "role": "Antecedent"},
      "Y": {"name": "Final accuracy", "role": "Consequent"},
      "Z": {"name": "Sample selection path", "role": "Mechanism"}
    },
    "invariants": [
      "Same uncertainty metric",
      "Same labeling budget (1000)",
      "Same base model"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Tie-breaking affects which samples are selected when uncertainties are equal. Different selections lead to different model updates, which affect future uncertainty calculations. This cascading effect makes final accuracy dependent on early random choices.",
    "wise_refusal": "The scenario underdetermines the answer because early tie-breaking decisions cascade through the active learning loop. Small initial differences can compound into different final outcomes.",
    "key_insight": "Active learning has path dependence; early random choices cascade through the selection-training loop.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0032",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Bayesian Inference",
    "difficulty": "Hard",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Prior Sensitivity",
    "scenario": "A Bayesian neural network with a standard normal prior on weights produced uncertain predictions on out-of-distribution data. The posterior predictive showed high variance for inputs far from training data.",
    "counterfactual_claim": "If we had used a more informative prior based on domain knowledge, predictions would have been more confident.",
    "variables": {
      "X": {"name": "Prior distribution", "role": "Antecedent"},
      "Y": {"name": "Prediction confidence", "role": "Consequent"},
      "Z": {"name": "Posterior concentration", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same model architecture",
      "Same inference method"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "More informative priors can increase confidence, but whether this is appropriate depends on prior accuracy. A confident but wrong prior leads to confidently wrong predictions. Without knowing if the domain knowledge encoded in the prior is correct, we cannot determine if increased confidence is beneficial.",
    "wise_refusal": "The scenario underdetermines the answer because informative priors increase confidence only when they encode accurate knowledge. The effect depends on prior quality, which is not specified.",
    "key_insight": "Informative priors increase confidence but not necessarily accuracy; prior quality determines whether this is beneficial.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0033",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Federated Learning",
    "difficulty": "Hard",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Client Sampling Variance",
    "scenario": "A federated learning system trained across 1000 clients, sampling 10 clients per round. After 100 rounds, the global model had 88% accuracy. Client selection was uniformly random each round.",
    "counterfactual_claim": "If we had sampled 100 clients per round instead of 10, the final accuracy would have been higher.",
    "variables": {
      "X": {"name": "Clients per round", "role": "Antecedent"},
      "Y": {"name": "Final accuracy", "role": "Consequent"},
      "Z": {"name": "Gradient estimate variance", "role": "Mechanism"}
    },
    "invariants": [
      "Same total clients and data distribution",
      "Same number of rounds",
      "Same local training procedure"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "More clients per round reduces variance in the aggregated gradient estimate, leading to more stable optimization. This is analogous to larger batch sizes in centralized training. With 10x more clients, gradient estimates are more representative, improving convergence.",
    "wise_refusal": "The verdict is clear because sampling more clients reduces gradient estimate variance by the law of large numbers. More representative gradients lead to better optimization.",
    "key_insight": "Client sampling follows law of large numbers; more clients per round reduces variance and improves convergence.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0034",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Neural Architecture Search",
    "difficulty": "Hard",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Search Randomness",
    "scenario": "A neural architecture search using random search found architecture A with 93% accuracy after 100 trials. The search space contained 10^6 possible architectures.",
    "counterfactual_claim": "If we had run the same search again with a different seed, we would have found an architecture with similar accuracy.",
    "variables": {
      "X": {"name": "Search random seed", "role": "Antecedent"},
      "Y": {"name": "Best found accuracy", "role": "Consequent"},
      "Z": {"name": "Random sampling of architecture space", "role": "Mechanism"}
    },
    "invariants": [
      "Same search space",
      "Same number of trials (100)",
      "Same evaluation procedure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "100 trials from 10^6 architectures samples only 0.01% of the space. Whether a similar-quality architecture is found depends on the distribution of good architectures in the space. If good architectures are rare, different seeds may find significantly different results.",
    "wise_refusal": "The scenario underdetermines the answer because we do not know the distribution of architecture quality in the search space. Sparse good solutions lead to high variance across seeds.",
    "key_insight": "Random search variance depends on good solution density; sparse optima cause high variance across seeds.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0035",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Contrastive Learning",
    "difficulty": "Medium",
    "trap_type": "F2",
    "trap_family": "F2",
    "trap_subtype": "Negative Sampling",
    "scenario": "A contrastive learning model used random negative sampling from the batch. With batch size 256, the model learned good representations. Negative pairs were selected uniformly at random.",
    "counterfactual_claim": "If we had used hard negative mining instead of random sampling, the representations would have been better.",
    "variables": {
      "X": {"name": "Negative sampling strategy", "role": "Antecedent"},
      "Y": {"name": "Representation quality", "role": "Consequent"},
      "Z": {"name": "Contrastive loss gradient signal", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same model architecture",
      "Same batch size and epochs"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Hard negative mining can improve learning by providing more informative gradients, but it can also lead to collapse if negatives are too hard. The benefit depends on the difficulty threshold and data characteristics. Without knowing these, the outcome is uncertain.",
    "wise_refusal": "The scenario underdetermines the answer because hard negative mining has a complex effect that depends on the hardness threshold. Too easy negatives provide weak signal; too hard negatives cause collapse.",
    "key_insight": "Hard negative mining is a double-edged sword; benefits depend on careful calibration of difficulty.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0036",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Redundancy",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Multiple Sufficient Causes",
    "scenario": "A production ML system had both input validation and model-level anomaly detection. A malformed input was caught and rejected. Both systems independently flagged the input as problematic.",
    "counterfactual_claim": "If the input validation had been disabled, the malformed input would still have been rejected.",
    "variables": {
      "X": {"name": "Input validation", "role": "Antecedent"},
      "Y": {"name": "Input rejection", "role": "Consequent"},
      "Z": {"name": "Anomaly detection backup", "role": "Mechanism"}
    },
    "invariants": [
      "Anomaly detection remains active",
      "Same malformed input",
      "Both systems independently sufficient for rejection"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The scenario states both systems independently flagged the input. Since anomaly detection would have caught and rejected the input regardless of input validation, the counterfactual holds. This is classic overdetermination where either cause alone is sufficient.",
    "wise_refusal": "The verdict is clear because the scenario specifies both systems independently identified the problem. The anomaly detection alone would have produced the same outcome.",
    "key_insight": "When redundant safety systems each independently suffice, removing one does not change the outcome.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0037",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Training Failure",
    "difficulty": "Hard",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Coincidental Overdetermination",
    "scenario": "A training run failed at epoch 50. Investigation found two independent issues: a learning rate schedule bug that would cause divergence at epoch 50, and corrupted training data that would cause NaN at epoch 52. The run crashed from the learning rate bug.",
    "counterfactual_claim": "If the learning rate bug had been fixed, the training would have completed successfully.",
    "variables": {
      "X": {"name": "Learning rate bug", "role": "Antecedent"},
      "Y": {"name": "Training completion", "role": "Consequent"},
      "Z": {"name": "Data corruption", "role": "Mechanism"}
    },
    "invariants": [
      "Corrupted data remains in dataset",
      "Same training procedure otherwise",
      "Training continues past epoch 50"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Even with the learning rate bug fixed, the corrupted data would cause failure at epoch 52. The training would not complete successfully because a second, independent cause of failure exists. This is overdetermination where removing one cause still leaves another.",
    "wise_refusal": "The verdict is clear because the scenario specifies an independent failure cause (corrupted data) that would trigger shortly after. Fixing the first bug does not prevent the second failure.",
    "key_insight": "Multiple independent failure modes create overdetermination; fixing one does not guarantee success.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0038",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Ensemble Decisions",
    "difficulty": "Easy",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Voting Overdetermination",
    "scenario": "An ensemble of 5 classifiers uses majority voting. On a specific input, all 5 models predicted 'spam'. The final ensemble prediction was 'spam' (5 votes to 0).",
    "counterfactual_claim": "If model 3 had predicted 'not spam' instead, the ensemble would still have predicted 'spam'.",
    "variables": {
      "X": {"name": "Model 3 prediction", "role": "Antecedent"},
      "Y": {"name": "Ensemble prediction", "role": "Consequent"},
      "Z": {"name": "Majority voting threshold", "role": "Mechanism"}
    },
    "invariants": [
      "Other 4 models still predict 'spam'",
      "Majority voting rule unchanged",
      "Same input"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "With majority voting and 5 models, 3 votes are needed for a decision. If model 3 changes to 'not spam', the vote becomes 4-1 for 'spam', still a majority. The ensemble prediction remains unchanged.",
    "wise_refusal": "The verdict is clear because 4 out of 5 votes (80%) still exceeds the majority threshold of 3 votes (60%). The changed vote does not flip the outcome.",
    "key_insight": "In majority voting, changing one vote from a unanimous decision cannot flip the outcome.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0039",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Feature Importance",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Redundant Features",
    "scenario": "A fraud detection model used both 'transaction amount' and 'transaction amount in USD' (identical values due to USD-only transactions). Removing 'transaction amount' from the model had no effect on predictions.",
    "counterfactual_claim": "If 'transaction amount' had been a critical feature, removing it would have degraded performance.",
    "variables": {
      "X": {"name": "Feature removal", "role": "Antecedent"},
      "Y": {"name": "Model performance", "role": "Consequent"},
      "Z": {"name": "Redundant feature availability", "role": "Mechanism"}
    },
    "invariants": [
      "Redundant feature remains",
      "Same model architecture",
      "Same test data"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The counterfactual premise is not met in this scenario. The feature might be critical in the sense of carrying important information, but the redundant feature provides identical information. Removal has no effect because the information is duplicated, not because the information is unimportant.",
    "wise_refusal": "The verdict is clear because feature importance cannot be assessed when redundant copies exist. The lack of performance drop reflects redundancy, not irrelevance.",
    "key_insight": "Feature importance analysis is confounded by redundant features that carry the same information.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0040",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "System Reliability",
    "difficulty": "Easy",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Backup System",
    "scenario": "A model serving system has primary and backup servers. Both are always running and synchronized. During an outage, the primary server failed, and the backup immediately took over. Users experienced no downtime.",
    "counterfactual_claim": "If the backup server had also failed simultaneously, users would have experienced downtime.",
    "variables": {
      "X": {"name": "Backup server availability", "role": "Antecedent"},
      "Y": {"name": "User downtime", "role": "Consequent"},
      "Z": {"name": "Failover mechanism", "role": "Mechanism"}
    },
    "invariants": [
      "Primary server fails",
      "No tertiary backup exists",
      "Same user requests"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "With no backup to fail over to, the system has no capacity to serve requests. Users would experience downtime until the primary is restored or a backup is brought online. The backup's availability was causally necessary for zero downtime.",
    "wise_refusal": "The verdict is clear because without any functioning server, requests cannot be served. The backup was the sole remaining capacity after primary failure.",
    "key_insight": "Backup systems are causally necessary for reliability when primary fails; both failing together guarantees downtime.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0041",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Pipeline",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Duplicate Validation",
    "scenario": "A data pipeline has schema validation at ingestion and again before model training. Malformed data was caught at ingestion. The same data would have been caught at the training stage validation.",
    "counterfactual_claim": "If ingestion validation had been skipped, the malformed data would have corrupted the model.",
    "variables": {
      "X": {"name": "Ingestion validation", "role": "Antecedent"},
      "Y": {"name": "Model corruption", "role": "Consequent"},
      "Z": {"name": "Pre-training validation", "role": "Mechanism"}
    },
    "invariants": [
      "Pre-training validation remains active",
      "Same malformed data",
      "Same validation rules"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The pre-training validation would catch the same malformed data before it reaches the model. Skipping ingestion validation changes when the data is caught but not whether it corrupts the model. The outcome is overdetermined by redundant checks.",
    "wise_refusal": "The verdict is clear because the scenario specifies identical validation at both stages. The pre-training check provides complete protection against this specific corruption.",
    "key_insight": "Redundant validation at multiple stages creates overdetermination; removing one stage does not expose the system.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0042",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Compression",
    "difficulty": "Hard",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Compensatory Mechanisms",
    "scenario": "A neural network was pruned by removing 50% of weights. Surprisingly, accuracy remained unchanged. Analysis showed the remaining weights had adapted during fine-tuning to compensate for the removed weights.",
    "counterfactual_claim": "If the pruned weights had been important, accuracy would have dropped.",
    "variables": {
      "X": {"name": "Pruned weight importance", "role": "Antecedent"},
      "Y": {"name": "Accuracy drop", "role": "Consequent"},
      "Z": {"name": "Weight adaptation", "role": "Mechanism"}
    },
    "invariants": [
      "Fine-tuning allowed after pruning",
      "Same test set",
      "Sufficient model capacity in remaining weights"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The counterfactual reasoning is flawed because fine-tuning allows remaining weights to compensate. The pruned weights might have been important before pruning, but their removal triggers adaptation. Stable accuracy does not imply the weights were unimportant.",
    "wise_refusal": "The verdict is clear because the scenario explicitly describes compensation by remaining weights. Importance before pruning and importance after adaptation are different concepts.",
    "key_insight": "Neural network plasticity allows compensation after pruning; stable accuracy does not prove original weights were unimportant.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0043",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Prompt Engineering",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Instruction Redundancy",
    "scenario": "A prompt contained both explicit instructions ('Respond only in JSON') and a JSON schema example. The model responded in JSON format. Both the instruction and the example independently would have elicited JSON output.",
    "counterfactual_claim": "If the explicit instruction had been removed, the model would have responded in plain text.",
    "variables": {
      "X": {"name": "Explicit JSON instruction", "role": "Antecedent"},
      "Y": {"name": "Response format", "role": "Consequent"},
      "Z": {"name": "JSON example in prompt", "role": "Mechanism"}
    },
    "invariants": [
      "JSON schema example remains",
      "Same model",
      "Same query"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The JSON schema example provides strong implicit instruction for JSON output. Language models follow patterns from examples. With the example present, the model would still output JSON even without explicit instruction.",
    "wise_refusal": "The verdict is clear because in-context examples strongly influence output format. The JSON schema example independently suffices to elicit JSON responses.",
    "key_insight": "Prompt examples provide implicit instructions; explicit instructions may be redundant when examples are present.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0044",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Distributed Systems",
    "difficulty": "Hard",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Consensus Overdetermination",
    "scenario": "A distributed ML training job uses 10 nodes with consensus requiring 7 nodes to agree. A gradient update was approved with 9 nodes agreeing. One agreeing node had a subtle bug that would have caused it to disagree if fixed.",
    "counterfactual_claim": "If the buggy node had been fixed, the gradient update would not have been approved.",
    "variables": {
      "X": {"name": "Buggy node fix", "role": "Antecedent"},
      "Y": {"name": "Update approval", "role": "Consequent"},
      "Z": {"name": "Consensus threshold", "role": "Mechanism"}
    },
    "invariants": [
      "Other 8 agreeing nodes unchanged",
      "Consensus threshold remains 7",
      "Same gradient update"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "With 9 nodes agreeing and threshold of 7, losing one agreement still leaves 8 agreeing nodes, which exceeds the threshold. The update would still be approved. The buggy node's vote is overdetermined.",
    "wise_refusal": "The verdict is clear because 8 remaining agreements still exceed the 7-node threshold. The buggy node's vote was not necessary for consensus.",
    "key_insight": "In threshold consensus, excess agreements create overdetermination; losing one vote above threshold does not change outcome.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0045",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Security",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Defense in Depth",
    "scenario": "An ML API has rate limiting, input sanitization, and output filtering. An adversarial attack was blocked by the rate limiter. The attack would also have been caught by input sanitization if it had passed rate limiting.",
    "counterfactual_claim": "If rate limiting had been disabled, the attack would have succeeded.",
    "variables": {
      "X": {"name": "Rate limiting", "role": "Antecedent"},
      "Y": {"name": "Attack success", "role": "Consequent"},
      "Z": {"name": "Input sanitization layer", "role": "Mechanism"}
    },
    "invariants": [
      "Input sanitization remains active",
      "Same attack pattern",
      "Same defense rules"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The scenario states input sanitization would have independently caught the attack. Disabling rate limiting allows the attack to reach the next layer, where it is still blocked. Defense in depth means multiple layers must fail for an attack to succeed.",
    "wise_refusal": "The verdict is clear because the scenario explicitly states the backup defense would catch this attack. The attack's failure is overdetermined by multiple defenses.",
    "key_insight": "Defense in depth creates overdetermination; attacks must bypass all layers, not just the first encountered.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0046",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Selection",
    "difficulty": "Easy",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Tied Rankings",
    "scenario": "A model selection process compared 5 models. Models A and B tied for best performance at 95% accuracy. Model A was selected due to alphabetical tie-breaking. Both would have been acceptable choices.",
    "counterfactual_claim": "If the tie-breaking rule had been reversed (Z before A), the production system would have performed worse.",
    "variables": {
      "X": {"name": "Tie-breaking rule", "role": "Antecedent"},
      "Y": {"name": "Production performance", "role": "Consequent"},
      "Z": {"name": "Model B's equal performance", "role": "Mechanism"}
    },
    "invariants": [
      "Both models have 95% accuracy",
      "Same production data distribution",
      "Same deployment infrastructure"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Both models have identical accuracy by the scenario's statement. Reversing tie-breaking would select model B, which has the same performance. Production would perform identically at 95% accuracy.",
    "wise_refusal": "The verdict is clear because the models are stated to have equal performance. Different tie-breaking selects an equally good model.",
    "key_insight": "Tie-breaking between equal options does not affect outcome quality; the alternatives are equivalent.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0047",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Attention Mechanisms",
    "difficulty": "Hard",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Redundant Attention Heads",
    "scenario": "A transformer has 12 attention heads. Ablation studies showed that removing any single head had minimal impact on performance. The model appeared to have learned redundant representations across heads.",
    "counterfactual_claim": "If head 5 had not learned its specific pattern, the model would have performed worse.",
    "variables": {
      "X": {"name": "Head 5 learning", "role": "Antecedent"},
      "Y": {"name": "Model performance", "role": "Consequent"},
      "Z": {"name": "Redundant head coverage", "role": "Mechanism"}
    },
    "invariants": [
      "Other heads maintain their patterns",
      "Same training data",
      "Same architecture"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The ablation studies demonstrate redundancy across heads. Removing any single head (including head 5) has minimal impact because other heads provide coverage. The learned pattern is overdetermined across multiple heads.",
    "wise_refusal": "The verdict is clear because the ablation evidence directly shows single-head removal has minimal impact. The redundancy makes any individual head's contribution non-critical.",
    "key_insight": "Redundant attention heads create overdetermination; each head's contribution is backed up by others.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0048",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Caching",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Multi-Level Cache",
    "scenario": "A model serving system has L1 cache (in-memory), L2 cache (Redis), and L3 cache (disk). A request hit the L1 cache and was served in 1ms. The same request was also present in L2 and L3 caches.",
    "counterfactual_claim": "If the L1 cache had missed, the request would have timed out.",
    "variables": {
      "X": {"name": "L1 cache hit", "role": "Antecedent"},
      "Y": {"name": "Request success", "role": "Consequent"},
      "Z": {"name": "L2/L3 cache availability", "role": "Mechanism"}
    },
    "invariants": [
      "L2 and L3 caches contain the data",
      "Same timeout threshold",
      "Same request"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "An L1 miss would trigger L2 lookup, which would hit. Even if L2 missed, L3 would hit. The request would succeed, just with higher latency (maybe 10ms for L2, 100ms for L3 instead of 1ms). It would not timeout.",
    "wise_refusal": "The verdict is clear because multiple cache levels provide redundancy. Missing one level shifts to the next, not to timeout. The data's availability is overdetermined.",
    "key_insight": "Multi-level caching creates redundancy; missing one level falls through to the next, not to failure.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0049",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Label Quality",
    "difficulty": "Hard",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Annotation Redundancy",
    "scenario": "A dataset used 3 annotators per sample with majority voting. On sample X, all 3 annotators agreed on label 'A'. The final label was 'A'. One annotator later admitted they had guessed randomly.",
    "counterfactual_claim": "If the random annotator had guessed 'B' instead, the final label would have been different.",
    "variables": {
      "X": {"name": "Random annotator's guess", "role": "Antecedent"},
      "Y": {"name": "Final label", "role": "Consequent"},
      "Z": {"name": "Other annotators' agreement", "role": "Mechanism"}
    },
    "invariants": [
      "Two other annotators chose 'A'",
      "Majority voting rule unchanged",
      "Same sample"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "With majority voting among 3 annotators and 2 choosing 'A', the label would still be 'A' regardless of the third annotator's choice. The random annotator's vote is not pivotal.",
    "wise_refusal": "The verdict is clear because 2 out of 3 votes for 'A' constitutes a majority. The third vote cannot change the outcome when the other two agree.",
    "key_insight": "In majority voting, the pivotal vote is only the one that breaks a tie; non-pivotal votes are overdetermined.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0050",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Robustness",
    "difficulty": "Medium",
    "trap_type": "F3",
    "trap_family": "F3",
    "trap_subtype": "Adversarial Defense Layers",
    "scenario": "A model has adversarial training, input preprocessing (JPEG compression), and certified defense radius. An adversarial example was defeated by adversarial training. The perturbation was also within the certified defense radius.",
    "counterfactual_claim": "If adversarial training had not been applied, the attack would have succeeded.",
    "variables": {
      "X": {"name": "Adversarial training", "role": "Antecedent"},
      "Y": {"name": "Attack success", "role": "Consequent"},
      "Z": {"name": "Certified defense", "role": "Mechanism"}
    },
    "invariants": [
      "Certified defense remains",
      "Same perturbation magnitude",
      "Same input preprocessing"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The certified defense radius guarantees robustness to perturbations within its bound. Since the perturbation is within this radius, the model is mathematically guaranteed to maintain its prediction. Adversarial training is redundant in this case.",
    "wise_refusal": "The verdict is clear because certified defenses provide mathematical guarantees. The attack being within the certified radius means it cannot succeed regardless of adversarial training.",
    "key_insight": "Certified defenses provide provable guarantees that make empirical defenses redundant for perturbations within the radius.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0051",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "ML Pipeline",
    "difficulty": "Medium",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Trigger vs Background",
    "scenario": "A model training pipeline consists of data loading (background), preprocessing (background), and the training loop (trigger). Training crashed due to a GPU memory error during the training loop. Data loading and preprocessing had completed successfully.",
    "counterfactual_claim": "If data loading had been faster, the training would not have crashed.",
    "variables": {
      "X": {"name": "Data loading speed", "role": "Antecedent"},
      "Y": {"name": "Training crash", "role": "Consequent"},
      "Z": {"name": "GPU memory allocation", "role": "Mechanism"}
    },
    "invariants": [
      "Same GPU memory capacity",
      "Same model size",
      "Same batch size"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Data loading speed is a background condition that enables training but does not affect GPU memory usage. The crash was caused by the training loop exceeding GPU memory, which is independent of data loading speed. Faster loading would not prevent the memory error.",
    "wise_refusal": "The verdict is clear because the crash cause (GPU memory) is structurally independent from data loading speed. Loading is a background enabler, not a causal factor in memory errors.",
    "key_insight": "Background conditions enable outcomes but do not cause them; changing background conditions does not affect triggered events.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0052",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Neural Networks",
    "difficulty": "Hard",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Structural Dependency",
    "scenario": "A deep network has an input layer, hidden layers, and an output layer. The model predicts correctly on test data. The input layer transforms raw pixels into normalized values.",
    "counterfactual_claim": "If the hidden layers had been removed, the model would still predict correctly.",
    "variables": {
      "X": {"name": "Hidden layers presence", "role": "Antecedent"},
      "Y": {"name": "Prediction correctness", "role": "Consequent"},
      "Z": {"name": "Learned feature hierarchy", "role": "Mechanism"}
    },
    "invariants": [
      "Same input data",
      "Same output format requirement",
      "Linear classification task"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether hidden layers are necessary depends on the task's linear separability. If the task is linearly separable, a single layer suffices. If nonlinear decision boundaries are needed, hidden layers are essential. Without knowing the task complexity, we cannot determine the answer.",
    "wise_refusal": "The scenario underdetermines the answer because hidden layer necessity depends on task complexity. Linearly separable tasks do not require depth; nonlinear tasks do.",
    "key_insight": "Architectural components' necessity depends on task structure; linear tasks do not require nonlinear transformations.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0053",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Feature Engineering",
    "difficulty": "Medium",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Causal vs Correlational Features",
    "scenario": "A model predicting house prices uses both 'number of rooms' (structural cause) and 'listing photo quality' (correlational). Removing 'listing photo quality' slightly reduced R-squared but predictions remained accurate.",
    "counterfactual_claim": "If 'number of rooms' had been removed instead, the model would have maintained similar accuracy.",
    "variables": {
      "X": {"name": "Number of rooms feature", "role": "Antecedent"},
      "Y": {"name": "Prediction accuracy", "role": "Consequent"},
      "Z": {"name": "Causal relationship to price", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same model architecture",
      "Other features remain"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The impact of removing 'number of rooms' depends on whether other features encode similar information (e.g., square footage correlates with rooms). Without knowing the feature redundancy structure, we cannot determine if removing this causal feature would significantly degrade accuracy.",
    "wise_refusal": "The scenario underdetermines the answer because feature importance depends on redundancy with other features. Causal features may be recoverable from correlated proxies.",
    "key_insight": "Causal feature importance is confounded by redundancy; removing a causal feature matters only if no proxies exist.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0054",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "System Architecture",
    "difficulty": "Easy",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Bottleneck Structure",
    "scenario": "An ML inference pipeline has CPU preprocessing (10ms), GPU inference (5ms), and CPU postprocessing (10ms). Total latency is 25ms. The GPU inference is the core computation.",
    "counterfactual_claim": "If we doubled GPU speed, total latency would be halved to 12.5ms.",
    "variables": {
      "X": {"name": "GPU speed", "role": "Antecedent"},
      "Y": {"name": "Total latency", "role": "Consequent"},
      "Z": {"name": "Pipeline structure", "role": "Mechanism"}
    },
    "invariants": [
      "CPU preprocessing time unchanged",
      "CPU postprocessing time unchanged",
      "Sequential pipeline execution"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Doubling GPU speed reduces inference from 5ms to 2.5ms, but CPU times remain 10ms + 10ms = 20ms. Total latency becomes 22.5ms, not 12.5ms. The CPU stages are structural bottlenecks that limit speedup benefits.",
    "wise_refusal": "The verdict is clear because the pipeline is sequential and most time is spent on CPU. Doubling GPU speed only saves 2.5ms out of 25ms total, giving 22.5ms latency.",
    "key_insight": "Amdahl's Law: speedup is limited by the fraction of work being accelerated; non-accelerated components cap total improvement.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0055",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Attention Mechanisms",
    "difficulty": "Hard",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Compositional Structure",
    "scenario": "A transformer processes a sentence 'The cat sat on the mat.' The model correctly resolves that 'it' in a follow-up sentence refers to 'cat'. Attention patterns show strong connection between 'it' and 'cat'.",
    "counterfactual_claim": "If the attention mechanism had been replaced with mean pooling, the model would still resolve the reference correctly.",
    "variables": {
      "X": {"name": "Attention mechanism", "role": "Antecedent"},
      "Y": {"name": "Reference resolution", "role": "Consequent"},
      "Z": {"name": "Token interaction structure", "role": "Mechanism"}
    },
    "invariants": [
      "Same input sentences",
      "Same model size",
      "Same training data"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Mean pooling treats all tokens equally, losing the structural relationships needed for reference resolution. Attention specifically enables the model to learn which tokens relate to which. Reference resolution requires structural knowledge that mean pooling cannot capture.",
    "wise_refusal": "The verdict is clear because reference resolution requires token-specific relationships that attention provides. Mean pooling destroys the structural information necessary for this task.",
    "key_insight": "Attention mechanisms encode structural relationships between tokens; replacing with position-invariant pooling loses this structure.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0056",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Flow",
    "difficulty": "Medium",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Information Bottleneck",
    "scenario": "A variational autoencoder compresses 1024-dim inputs through a 10-dim latent bottleneck. Reconstructions are blurry but capture main features. The encoder maps to latent space, the decoder reconstructs.",
    "counterfactual_claim": "If the latent space had been 1000-dim instead of 10-dim, reconstructions would be sharper.",
    "variables": {
      "X": {"name": "Latent dimension", "role": "Antecedent"},
      "Y": {"name": "Reconstruction sharpness", "role": "Consequent"},
      "Z": {"name": "Information capacity", "role": "Mechanism"}
    },
    "invariants": [
      "Same encoder/decoder architecture depths",
      "Same training data",
      "Same loss function"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The latent bottleneck directly limits information capacity. A 1000-dim latent space can preserve much more detail than 10-dim. With near-original dimensionality, the autoencoder can transmit fine details, producing sharper reconstructions.",
    "wise_refusal": "The verdict is clear because the information bottleneck principle directly relates latent capacity to reconstruction quality. More dimensions mean more information can flow through.",
    "key_insight": "Autoencoder reconstruction quality is structurally limited by latent dimensionality; wider bottlenecks preserve more information.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0057",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Graph Neural Networks",
    "difficulty": "Hard",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Topology Sensitivity",
    "scenario": "A GNN for molecular property prediction uses message passing over the molecular graph. The model correctly predicted toxicity for a benzene ring. Edge features encode bond types.",
    "counterfactual_claim": "If the graph structure had been randomized (shuffled edges), the prediction would remain correct.",
    "variables": {
      "X": {"name": "Graph topology", "role": "Antecedent"},
      "Y": {"name": "Toxicity prediction", "role": "Consequent"},
      "Z": {"name": "Message passing paths", "role": "Mechanism"}
    },
    "invariants": [
      "Same node features",
      "Same number of edges",
      "Same GNN architecture"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Molecular properties depend critically on structure. A benzene ring's toxicity comes from its ring structure. Randomizing edges destroys this structure, making the representation meaningless for chemistry. The GNN would not produce a correct prediction.",
    "wise_refusal": "The verdict is clear because molecular properties are determined by atomic arrangements. Randomized topology represents a different (likely impossible) molecule with unpredictable properties.",
    "key_insight": "GNNs encode structural information; randomizing topology destroys the domain-meaningful relationships that determine predictions.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0058",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Sequence Models",
    "difficulty": "Medium",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Positional Structure",
    "scenario": "A transformer for code completion uses positional encodings to track token positions. The model correctly suggests 'return' after an 'if' block. Position matters for understanding code structure.",
    "counterfactual_claim": "If positional encodings had been removed, the model would still suggest 'return' correctly.",
    "variables": {
      "X": {"name": "Positional encodings", "role": "Antecedent"},
      "Y": {"name": "Code suggestion correctness", "role": "Consequent"},
      "Z": {"name": "Sequence structure awareness", "role": "Mechanism"}
    },
    "invariants": [
      "Same code context",
      "Same model architecture otherwise",
      "Same training data"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Without positional encodings, the transformer treats the input as a bag of tokens, losing order information critical for code. It cannot distinguish 'if x: return y' from 'return x: if y'. Code structure requires position awareness.",
    "wise_refusal": "The verdict is clear because code completion depends on syntactic structure, which requires knowing token order. A bag-of-tokens model cannot understand code flow.",
    "key_insight": "Positional encodings enable sequence structure understanding; removing them reduces transformers to bag-of-tokens models.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0059",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Convolutional Networks",
    "difficulty": "Easy",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Spatial Structure",
    "scenario": "A CNN for digit recognition correctly classifies '6' and '9'. The model uses 2D convolutions that preserve spatial relationships. The difference between 6 and 9 is rotation.",
    "counterfactual_claim": "If the input had been flattened to 1D before the first layer, the model would still distinguish 6 from 9.",
    "variables": {
      "X": {"name": "Spatial structure preservation", "role": "Antecedent"},
      "Y": {"name": "6/9 discrimination", "role": "Consequent"},
      "Z": {"name": "2D convolution features", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same number of parameters",
      "Same digit images"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "A 1D flattened representation loses explicit spatial structure but may still encode distinguishing features if the flattening is consistent. A fully connected network can learn to map flattened positions to features. The answer depends on model capacity and training.",
    "wise_refusal": "The scenario underdetermines the answer because 1D flattening preserves information (just not in spatial format). Whether the model can learn to use this information depends on architecture and training.",
    "key_insight": "Flattening transforms but does not destroy information; whether the model can recover spatial relationships depends on capacity.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0060",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Knowledge Distillation",
    "difficulty": "Hard",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Capacity Structure",
    "scenario": "A large teacher model (1B params) was distilled into a student model (10M params). The student achieved 90% of teacher accuracy. The teacher had 100 layers; the student had 10 layers.",
    "counterfactual_claim": "If the student had the same depth as the teacher (100 layers) but same parameter count, it would match teacher accuracy.",
    "variables": {
      "X": {"name": "Student depth", "role": "Antecedent"},
      "Y": {"name": "Student accuracy", "role": "Consequent"},
      "Z": {"name": "Representational capacity structure", "role": "Mechanism"}
    },
    "invariants": [
      "Same student parameter count (10M)",
      "Same teacher model",
      "Same distillation procedure"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "With fixed 10M parameters spread over 100 layers, each layer would be extremely narrow (~100K params/layer). This would create severe capacity bottlenecks and vanishing gradients. The model would likely perform worse, not better, than the 10-layer version.",
    "wise_refusal": "The verdict is clear because parameter count constrains total capacity. Spreading limited parameters over many layers creates per-layer bottlenecks that harm learning.",
    "key_insight": "Depth with fixed parameters creates width constraints; extremely narrow deep networks have severe capacity issues.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0061",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Loss Landscape",
    "difficulty": "Hard",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Optimization Structure",
    "scenario": "A model trained with Adam optimizer converged to a flat minimum with good generalization. The loss landscape around this minimum is wide and smooth. Training took 100 epochs.",
    "counterfactual_claim": "If we had used SGD with momentum instead of Adam, the model would have found the same minimum.",
    "variables": {
      "X": {"name": "Optimizer choice", "role": "Antecedent"},
      "Y": {"name": "Converged minimum", "role": "Consequent"},
      "Z": {"name": "Optimization trajectory", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same training data",
      "Same initialization"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Different optimizers follow different trajectories through the loss landscape and may converge to different minima. Whether they find the same minimum depends on the landscape structure, learning rate schedules, and whether multiple equivalent minima exist. This is not determinable without running both.",
    "wise_refusal": "The scenario underdetermines the answer because optimizer trajectories depend on complex interactions between adaptive learning rates, momentum, and loss landscape. Different optimizers often find different minima.",
    "key_insight": "Optimizers are not interchangeable; they follow different paths and may converge to different minima in non-convex landscapes.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0062",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Multi-Task Learning",
    "difficulty": "Medium",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Shared vs Task-Specific",
    "scenario": "A multi-task model shares a backbone but has separate heads for classification and regression. The classification head achieves 95% accuracy. The regression head has 0.1 MSE. Both tasks use the same input features.",
    "counterfactual_claim": "If the backbone had been trained only on classification, the regression head would perform equally well.",
    "variables": {
      "X": {"name": "Multi-task training", "role": "Antecedent"},
      "Y": {"name": "Regression performance", "role": "Consequent"},
      "Z": {"name": "Shared representations", "role": "Mechanism"}
    },
    "invariants": [
      "Same backbone architecture",
      "Same regression head",
      "Same training data"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether multi-task learning helps regression depends on task relatedness. If classification features are useful for regression, joint training helps. If tasks require conflicting features, joint training hurts. Without knowing task relationship, the outcome is uncertain.",
    "wise_refusal": "The scenario underdetermines the answer because multi-task benefit depends on task relatedness. Unrelated or conflicting tasks can hurt each other; related tasks can help.",
    "key_insight": "Multi-task learning benefit depends on task relatedness; shared representations help only when tasks need similar features.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0063",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Embedding Spaces",
    "difficulty": "Medium",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Metric Structure",
    "scenario": "A recommendation system uses cosine similarity in embedding space to find similar items. Item A and B have similarity 0.95. The embedding dimension is 128. Items are represented as unit vectors.",
    "counterfactual_claim": "If we had used Euclidean distance instead of cosine similarity, items A and B would still be considered most similar.",
    "variables": {
      "X": {"name": "Similarity metric", "role": "Antecedent"},
      "Y": {"name": "Similarity ranking", "role": "Consequent"},
      "Z": {"name": "Embedding geometry", "role": "Mechanism"}
    },
    "invariants": [
      "Same embeddings (unit vectors)",
      "Same item pairs",
      "Same ranking threshold"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "For unit vectors, cosine similarity and Euclidean distance are monotonically related: d = sqrt(2 - 2*cos). If A and B have highest cosine similarity, they also have smallest Euclidean distance. The ranking is preserved under this transformation.",
    "wise_refusal": "The verdict is clear because for unit vectors, cosine similarity and Euclidean distance induce the same ordering. The metrics are monotonically related on the unit sphere.",
    "key_insight": "On unit vectors, cosine similarity and Euclidean distance are equivalent for ranking; they induce the same ordering.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0064",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Tokenization",
    "difficulty": "Easy",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Input Structure",
    "scenario": "A sentiment model processes 'I love this movie!' with word-level tokenization into 5 tokens. The model predicts positive sentiment. Each word is embedded separately.",
    "counterfactual_claim": "If we had used character-level tokenization, the sentiment prediction would be different.",
    "variables": {
      "X": {"name": "Tokenization granularity", "role": "Antecedent"},
      "Y": {"name": "Sentiment prediction", "role": "Consequent"},
      "Z": {"name": "Token representation learning", "role": "Mechanism"}
    },
    "invariants": [
      "Same input text",
      "Same model architecture (adjusted for input size)",
      "Same training data"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Character-level models can achieve similar performance on sentiment tasks but learn different representations. Whether the prediction differs depends on model capacity, training, and how well each tokenization captures sentiment-relevant features. The outcome is not determinable a priori.",
    "wise_refusal": "The scenario underdetermines the answer because both tokenization strategies can work for sentiment analysis. The specific prediction depends on learned representations, which vary with training.",
    "key_insight": "Tokenization affects representation learning but not necessarily final task performance; both approaches can succeed differently.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0065",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Batch Processing",
    "difficulty": "Easy",
    "trap_type": "F4",
    "trap_family": "F4",
    "trap_subtype": "Aggregation Structure",
    "scenario": "A model processes batches of 32 samples and produces batch-level predictions via mean pooling. The model correctly classified a batch as 'spam' based on aggregate features.",
    "counterfactual_claim": "If one sample in the batch had been changed from spam to ham, the batch prediction would have changed to 'ham'.",
    "variables": {
      "X": {"name": "Single sample label", "role": "Antecedent"},
      "Y": {"name": "Batch prediction", "role": "Consequent"},
      "Z": {"name": "Mean pooling aggregation", "role": "Mechanism"}
    },
    "invariants": [
      "Other 31 samples unchanged",
      "Same aggregation method",
      "Same model"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Mean pooling aggregates 32 samples. Changing 1 sample affects 1/32 = 3% of the aggregate. Unless the batch was exactly on the decision boundary, such a small change would not flip the prediction. The structural averaging dilutes individual sample influence.",
    "wise_refusal": "The verdict is clear because mean pooling dilutes individual sample influence. One sample out of 32 contributes only 3% to the aggregate, unlikely to flip a confident prediction.",
    "key_insight": "Aggregation structures dilute individual contributions; changing one element in a mean has limited effect.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0066",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Training Dynamics",
    "difficulty": "Medium",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Path Dependence",
    "scenario": "A model was trained by first pretraining on ImageNet, then fine-tuning on medical images. The final model achieves 98% accuracy on medical diagnosis. Pretraining took 1 week; fine-tuning took 1 day.",
    "counterfactual_claim": "If we had trained only on medical images from scratch with the same total training time, we would have achieved the same accuracy.",
    "variables": {
      "X": {"name": "Pretraining on ImageNet", "role": "Antecedent"},
      "Y": {"name": "Final medical accuracy", "role": "Consequent"},
      "Z": {"name": "Transfer learning dynamics", "role": "Mechanism"}
    },
    "invariants": [
      "Same total compute budget",
      "Same final architecture",
      "Same medical dataset"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Pretraining provides learned features (edges, textures, shapes) that transfer to medical imaging. Training from scratch requires learning these features from limited medical data, which typically produces worse results. The temporal path through general features first is crucial.",
    "wise_refusal": "The verdict is clear because transfer learning from large datasets provides foundational features that cannot be learned from small domain-specific datasets alone. The training path matters.",
    "key_insight": "Transfer learning path dependence: pretraining on diverse data provides features that cannot be learned from limited target data alone.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0067",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Curriculum Learning",
    "difficulty": "Medium",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Training Order",
    "scenario": "A language model was trained using curriculum learning: easy sentences first, then complex sentences. The model achieved strong performance on complex reasoning tasks. Training order was strictly enforced.",
    "counterfactual_claim": "If we had trained on complex sentences first, then easy sentences, the model would have performed equally well.",
    "variables": {
      "X": {"name": "Training order", "role": "Antecedent"},
      "Y": {"name": "Complex reasoning performance", "role": "Consequent"},
      "Z": {"name": "Representation building", "role": "Mechanism"}
    },
    "invariants": [
      "Same total training examples",
      "Same model architecture",
      "Same training duration"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Curriculum learning benefits depend on task complexity and model capacity. Some tasks benefit from easy-to-hard ordering; others show no difference or even benefit from hard-to-easy. Without knowing the specific learning dynamics for this task, the outcome is uncertain.",
    "wise_refusal": "The scenario underdetermines the answer because curriculum learning effects are task-dependent. Some tasks show strong order effects; others do not. The benefit requires empirical validation.",
    "key_insight": "Curriculum learning effects are task-dependent; not all tasks benefit from easy-to-hard ordering.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0068",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Learning Rate Scheduling",
    "difficulty": "Easy",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Timing Sensitivity",
    "scenario": "A model used learning rate warmup for the first 1000 steps, then constant learning rate. Training was stable and converged well. Total training was 100,000 steps.",
    "counterfactual_claim": "If warmup had been applied for the first 100,000 steps instead of 1000, convergence would have been faster.",
    "variables": {
      "X": {"name": "Warmup duration", "role": "Antecedent"},
      "Y": {"name": "Convergence speed", "role": "Consequent"},
      "Z": {"name": "Learning rate magnitude", "role": "Mechanism"}
    },
    "invariants": [
      "Same peak learning rate",
      "Same model architecture",
      "Same training data"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Warmup for the entire training means the learning rate never reaches peak value, effectively using a very low learning rate throughout. This would dramatically slow convergence as the model takes tiny steps. Extended warmup defeats its purpose.",
    "wise_refusal": "The verdict is clear because warmup is meant to be a brief initial phase. Extending it to the entire training means perpetually low learning rates, which slows rather than speeds convergence.",
    "key_insight": "Learning rate warmup is a temporal tool for stability in early training; extending it throughout defeats its purpose.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0069",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Checkpointing",
    "difficulty": "Easy",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Temporal Recovery",
    "scenario": "A training job crashed at step 50,000. The last checkpoint was saved at step 49,000. Training resumed from the checkpoint and completed successfully. 1,000 steps of training were lost.",
    "counterfactual_claim": "If checkpoints had been saved every 100 steps instead of every 1,000, less training progress would have been lost.",
    "variables": {
      "X": {"name": "Checkpoint frequency", "role": "Antecedent"},
      "Y": {"name": "Training progress lost", "role": "Consequent"},
      "Z": {"name": "Crash recovery point", "role": "Mechanism"}
    },
    "invariants": [
      "Same crash point (step 50,000)",
      "Same training otherwise",
      "Checkpoints correctly saved"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "With checkpoints every 100 steps, the last checkpoint would be at step 49,900, losing only 100 steps instead of 1,000. More frequent checkpoints deterministically reduce maximum possible loss to the checkpoint interval.",
    "wise_refusal": "The verdict is clear because checkpoint frequency directly determines the maximum recoverable gap. More frequent saves mean less potential loss.",
    "key_insight": "Checkpoint frequency sets an upper bound on lost progress; more frequent saves reduce potential loss linearly.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0070",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Online Learning",
    "difficulty": "Hard",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Concept Drift",
    "scenario": "An online fraud detection model trained on 2023 data was deployed in 2024. Performance degraded from 95% to 75% accuracy over 6 months. Fraud patterns had evolved significantly.",
    "counterfactual_claim": "If the model had been retrained monthly on new data, performance would have remained above 90%.",
    "variables": {
      "X": {"name": "Retraining frequency", "role": "Antecedent"},
      "Y": {"name": "Sustained accuracy", "role": "Consequent"},
      "Z": {"name": "Concept drift rate", "role": "Mechanism"}
    },
    "invariants": [
      "Same base model architecture",
      "Same fraud environment (drifting)",
      "New data available monthly"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Monthly retraining would capture recent patterns, but whether it maintains 90% accuracy depends on how quickly fraud patterns evolve within each month and whether new frauds are detectable in available data. Drift rate relative to retraining rate determines success.",
    "wise_refusal": "The scenario underdetermines the answer because success depends on drift rate relative to retraining frequency. Fast drift within months could still outpace monthly updates.",
    "key_insight": "Retraining effectiveness depends on drift rate relative to update frequency; fast drift can outpace any practical retraining schedule.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0071",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Early Stopping",
    "difficulty": "Medium",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Optimal Stopping Time",
    "scenario": "A model was trained for 100 epochs with early stopping patience of 10 epochs. Training stopped at epoch 45 when validation loss stopped improving. Final test accuracy was 92%.",
    "counterfactual_claim": "If we had continued training to 100 epochs without early stopping, test accuracy would have been higher.",
    "variables": {
      "X": {"name": "Early stopping", "role": "Antecedent"},
      "Y": {"name": "Test accuracy", "role": "Consequent"},
      "Z": {"name": "Overfitting dynamics", "role": "Mechanism"}
    },
    "invariants": [
      "Same model and hyperparameters",
      "Same training data",
      "Same test set"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Early stopping triggered because validation loss stopped improving, indicating the onset of overfitting. Continuing training would likely increase overfitting, degrading test accuracy rather than improving it. Early stopping prevents this degradation.",
    "wise_refusal": "The verdict is clear because early stopping is designed to prevent overfitting. Continuing past the stopping point typically decreases generalization performance.",
    "key_insight": "Early stopping identifies the optimal training duration; continuing beyond increases overfitting and reduces test performance.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0072",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Collection",
    "difficulty": "Hard",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Historical Dependence",
    "scenario": "A recommendation system was trained on user interaction data from 2020-2023. Users' preferences had shifted significantly over this period. The model shows temporal bias toward recent interactions.",
    "counterfactual_claim": "If we had weighted all time periods equally instead of recent-heavy weighting, recommendations would better reflect users' true preferences.",
    "variables": {
      "X": {"name": "Temporal weighting", "role": "Antecedent"},
      "Y": {"name": "Recommendation quality", "role": "Consequent"},
      "Z": {"name": "Preference evolution", "role": "Mechanism"}
    },
    "invariants": [
      "Same historical data",
      "Same model architecture",
      "Same users"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Users' current preferences are better reflected by recent data than old data. Equal weighting would give outdated 2020 preferences the same importance as current 2023 preferences, reducing relevance. Recent-heavy weighting is appropriate for evolving preferences.",
    "wise_refusal": "The verdict is clear because user preferences evolve over time. Recent data better reflects current preferences, making recent-heavy weighting more appropriate than equal weighting.",
    "key_insight": "For evolving preferences, recent data is more relevant; equal temporal weighting inappropriately values outdated information.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0073",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Updates",
    "difficulty": "Medium",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Deployment Timing",
    "scenario": "A new model version was deployed to production at 2 AM when traffic was lowest. The deployment completed successfully with no user impact. Traffic at 2 AM was 1% of peak.",
    "counterfactual_claim": "If the deployment had been done at 2 PM (peak traffic), user impact would have been the same.",
    "variables": {
      "X": {"name": "Deployment time", "role": "Antecedent"},
      "Y": {"name": "User impact", "role": "Consequent"},
      "Z": {"name": "Traffic volume", "role": "Mechanism"}
    },
    "invariants": [
      "Same deployment procedure",
      "Same model update",
      "Same infrastructure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "If the deployment is seamless (zero-downtime), timing does not matter. If there is any downtime or degradation during deployment, peak traffic would magnify user impact. Without knowing if the deployment causes any interruption, the outcome is uncertain.",
    "wise_refusal": "The scenario underdetermines the answer because it depends on whether deployment causes any service interruption. Zero-downtime deployments are timing-independent; others are not.",
    "key_insight": "Deployment timing matters only if deployment causes service impact; zero-downtime deployments are timing-independent.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0074",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Incremental Learning",
    "difficulty": "Hard",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Catastrophic Forgetting",
    "scenario": "A model was trained on Task A, then fine-tuned on Task B. Performance on Task A dropped from 95% to 40% after Task B training. No Task A data was included in Task B fine-tuning.",
    "counterfactual_claim": "If we had used replay of Task A samples during Task B training, Task A performance would have been preserved.",
    "variables": {
      "X": {"name": "Replay during fine-tuning", "role": "Antecedent"},
      "Y": {"name": "Task A performance", "role": "Consequent"},
      "Z": {"name": "Gradient interference", "role": "Mechanism"}
    },
    "invariants": [
      "Same Task A and Task B",
      "Same model architecture",
      "Same amount of Task B training"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Catastrophic forgetting occurs because gradients for Task B overwrite Task A knowledge. Replay maintains Task A knowledge by continuing to reinforce those patterns during Task B training. This is a well-established technique for mitigating forgetting.",
    "wise_refusal": "The verdict is clear because replay is a proven mechanism for preventing catastrophic forgetting. It works by maintaining gradient updates that preserve old task knowledge.",
    "key_insight": "Replay prevents catastrophic forgetting by maintaining gradient signals for old tasks during new task learning.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0075",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "A/B Testing",
    "difficulty": "Easy",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Temporal Confounding",
    "scenario": "An A/B test compared two recommendation models over 2 weeks. Model A was tested in week 1; Model B in week 2. Model B showed 10% higher engagement. Week 2 included a major holiday.",
    "counterfactual_claim": "If both models had been tested simultaneously in randomized user groups, the engagement difference would have been the same.",
    "variables": {
      "X": {"name": "Test design", "role": "Antecedent"},
      "Y": {"name": "Measured engagement difference", "role": "Consequent"},
      "Z": {"name": "Holiday effect", "role": "Mechanism"}
    },
    "invariants": [
      "Same user population",
      "Same engagement metric",
      "Same models"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Sequential testing confounds model effects with time effects. The holiday in week 2 likely boosted engagement independent of model quality. Simultaneous randomized testing would isolate model effects from temporal confounds, likely showing a different (smaller) difference.",
    "wise_refusal": "The verdict is clear because sequential testing confounds model effects with time effects. The holiday provides an alternative explanation for Model B's apparent superiority.",
    "key_insight": "Sequential A/B testing confounds treatment effects with time effects; simultaneous randomization isolates causal effects.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0076",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Batch Normalization",
    "difficulty": "Medium",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Training Phase Effects",
    "scenario": "A model with batch normalization showed different behavior during training vs inference. During training, it used batch statistics. During inference, it used running statistics computed during training.",
    "counterfactual_claim": "If we had used batch statistics during inference too, predictions would have been more accurate.",
    "variables": {
      "X": {"name": "Inference statistics source", "role": "Antecedent"},
      "Y": {"name": "Prediction accuracy", "role": "Consequent"},
      "Z": {"name": "Statistics stability", "role": "Mechanism"}
    },
    "invariants": [
      "Same trained model",
      "Same inference data",
      "Same batch norm layers"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Batch statistics during inference depend on the specific batch composition, causing inconsistent predictions for the same input in different batches. Running statistics provide stable, consistent normalization. Batch stats during inference typically hurt rather than help.",
    "wise_refusal": "The verdict is clear because batch statistics introduce unwanted variability at inference time. Running statistics ensure deterministic, consistent predictions.",
    "key_insight": "Batch normalization uses different statistics for training vs inference by design; batch stats at inference cause unwanted variability.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0077",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Versioning",
    "difficulty": "Easy",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Version History",
    "scenario": "A model registry tracks 5 versions: v1 -> v2 -> v3 -> v4 -> v5 (current). Version v3 was identified as having a critical bug. Production is running v5, which was built on top of v4, which was built on v3.",
    "counterfactual_claim": "If v3's bug had been fixed before v4 was developed, v5 would not have inherited the bug.",
    "variables": {
      "X": {"name": "v3 bug fix timing", "role": "Antecedent"},
      "Y": {"name": "Bug presence in v5", "role": "Consequent"},
      "Z": {"name": "Version lineage", "role": "Mechanism"}
    },
    "invariants": [
      "Same development sequence",
      "v4 and v5 built on previous versions",
      "Bug is inherited through versions"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The version lineage shows v5 inherits from v4 which inherits from v3. If v3's bug was fixed before v4 was developed, v4 would start from a bug-free base, and v5 would inherit the fix. The temporal fix would propagate through the lineage.",
    "wise_refusal": "The verdict is clear because version inheritance is a deterministic temporal chain. Fixing bugs before downstream versions ensures the fix propagates forward.",
    "key_insight": "Version inheritance creates temporal chains; early fixes propagate to all downstream versions.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0078",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Sequence Modeling",
    "difficulty": "Hard",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Temporal Context Window",
    "scenario": "A time series forecasting model uses a context window of 100 time steps. It successfully predicted a market crash that occurred at step 150. The crash had early warning signals starting at step 75.",
    "counterfactual_claim": "If the context window had been only 50 steps, the model would not have predicted the crash.",
    "variables": {
      "X": {"name": "Context window size", "role": "Antecedent"},
      "Y": {"name": "Crash prediction", "role": "Consequent"},
      "Z": {"name": "Early warning signal visibility", "role": "Mechanism"}
    },
    "invariants": [
      "Same prediction point (step 150)",
      "Same model architecture",
      "Warning signals start at step 75"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "At step 150 with a 50-step window, the model sees steps 100-150, missing the early warnings at step 75-100. With a 100-step window, it sees steps 50-150, capturing the warnings. The smaller window excludes the critical temporal information.",
    "wise_refusal": "The verdict is clear because the context window determines which time steps are visible. A 50-step window at step 150 cannot see events before step 100, missing the step 75 warnings.",
    "key_insight": "Context window size determines temporal visibility; small windows may miss critical early signals.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0079",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Gradient Accumulation",
    "difficulty": "Medium",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Update Timing",
    "scenario": "A training job uses gradient accumulation with 8 accumulation steps before each update. Memory-limited GPUs cannot fit larger batches. Effective batch size is 8x the micro-batch size.",
    "counterfactual_claim": "If we had updated weights after every micro-batch instead of accumulating, training would have been faster.",
    "variables": {
      "X": {"name": "Gradient accumulation", "role": "Antecedent"},
      "Y": {"name": "Training convergence", "role": "Consequent"},
      "Z": {"name": "Effective batch size", "role": "Mechanism"}
    },
    "invariants": [
      "Same total samples processed",
      "Same model architecture",
      "Same learning rate"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Without accumulation, effective batch size is 8x smaller. Smaller batches have noisier gradients and often require learning rate reduction. With the same learning rate, training would likely be unstable or slower to converge. Accumulation is used precisely because it enables larger effective batches.",
    "wise_refusal": "The verdict is clear because gradient accumulation enables larger effective batch sizes that stabilize training. Removing it with the same learning rate typically causes instability or slower convergence.",
    "key_insight": "Gradient accumulation enables larger effective batches for stability; removing it requires retuning or causes convergence issues.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0080",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Preemption Handling",
    "difficulty": "Hard",
    "trap_type": "F5",
    "trap_family": "F5",
    "trap_subtype": "Interruption Timing",
    "scenario": "A cloud training job was preempted 3 times during a 24-hour training run. Each preemption lost about 30 minutes of progress due to checkpoint recovery. Total training time was extended to 26.5 hours.",
    "counterfactual_claim": "If checkpoints had been saved every 5 minutes instead of every 30 minutes, total training time would have been closer to 24 hours.",
    "variables": {
      "X": {"name": "Checkpoint frequency", "role": "Antecedent"},
      "Y": {"name": "Total training time", "role": "Consequent"},
      "Z": {"name": "Recovery overhead", "role": "Mechanism"}
    },
    "invariants": [
      "Same number of preemptions (3)",
      "Same base training time (24 hours)",
      "Checkpoint saving time negligible"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "With 5-minute checkpoints, maximum loss per preemption is 5 minutes instead of 30. Three preemptions would lose at most 15 minutes total instead of 90 minutes. Total time would be approximately 24.25 hours instead of 26.5 hours.",
    "wise_refusal": "The verdict is clear because more frequent checkpoints reduce maximum progress loss per preemption. The calculation shows 5-minute intervals lose at most 15 minutes total vs 90 minutes with 30-minute intervals.",
    "key_insight": "Checkpoint frequency directly bounds recovery loss; more frequent saves reduce total overhead from preemptions.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0081",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Interpretability",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Underdetermination",
    "scenario": "A neural network correctly predicted a patient has cancer. SHAP analysis showed the top contributing features, but the actual causal mechanism the model used internally remains unknown. Multiple internal circuits could produce the same SHAP values.",
    "counterfactual_claim": "If the model had learned a different internal representation with the same SHAP values, the prediction would have been different.",
    "variables": {
      "X": {"name": "Internal representation", "role": "Antecedent"},
      "Y": {"name": "Prediction", "role": "Consequent"},
      "Z": {"name": "SHAP value constraint", "role": "Mechanism"}
    },
    "invariants": [
      "Same SHAP values",
      "Same input features",
      "Same output"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "SHAP values explain feature contributions to the output, and by definition, the same SHAP values for the same input produce the same output. The internal representation may differ, but if SHAP values are identical, the prediction must be identical. The counterfactual is internally contradictory.",
    "wise_refusal": "The verdict is clear because SHAP values by definition describe the contribution to the specific prediction. Same SHAP values for same input implies same prediction regardless of internal representation.",
    "key_insight": "SHAP values are output explanations; same SHAP values imply same output, making the counterfactual self-contradictory.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0082",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Black Box Models",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Unknowable Mechanism",
    "scenario": "A proprietary API model produced an unexpected output for a specific prompt. The model's architecture, training data, and weights are unknown. The API only returns outputs without explanations.",
    "counterfactual_claim": "If the model had been trained on different data, this specific output would not have occurred.",
    "variables": {
      "X": {"name": "Training data", "role": "Antecedent"},
      "Y": {"name": "Specific output", "role": "Consequent"},
      "Z": {"name": "Unknown model internals", "role": "Mechanism"}
    },
    "invariants": [
      "Same prompt",
      "Unknown architecture remains unknown",
      "Different hypothetical training data"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Without knowing the model's architecture, training data, or how it processes inputs, we cannot determine how different training data would affect this specific output. The counterfactual is epistemically underdetermined due to fundamental lack of access to the model's internals.",
    "wise_refusal": "The scenario underdetermines the answer because the model is a black box. We cannot know how training data changes would propagate to outputs without access to the training process and model internals.",
    "key_insight": "Black box models create epistemic barriers; counterfactuals about training effects are unknowable without internal access.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0083",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Hyperparameter Optimization",
    "difficulty": "Medium",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Exploration Incompleteness",
    "scenario": "A Bayesian hyperparameter optimization found the best configuration after 50 trials. The configuration achieved 94% accuracy. The search space had 10^12 possible configurations.",
    "counterfactual_claim": "If we had run 500 trials instead of 50, we would have found a configuration with higher accuracy.",
    "variables": {
      "X": {"name": "Number of trials", "role": "Antecedent"},
      "Y": {"name": "Best found accuracy", "role": "Consequent"},
      "Z": {"name": "Search space coverage", "role": "Mechanism"}
    },
    "invariants": [
      "Same search space",
      "Same Bayesian optimization algorithm",
      "Same evaluation procedure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "More trials increase the probability of finding better configurations, but do not guarantee it. The current best might already be near-optimal, or the additional trials might explore poorly. Without knowing the accuracy distribution in the search space, we cannot determine the outcome.",
    "wise_refusal": "The scenario underdetermines the answer because we do not know if better configurations exist or if 94% is close to optimal. More trials increase probability but do not guarantee improvement.",
    "key_insight": "Hyperparameter search provides no guarantees; more trials increase probability of improvement but not certainty.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0084",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Annotation",
    "difficulty": "Medium",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Ground Truth Uncertainty",
    "scenario": "Medical image annotators disagreed on whether an image showed cancer (3 said yes, 2 said no). The final label was 'cancer' based on majority vote. The true disease status is unknown without biopsy.",
    "counterfactual_claim": "If we had used unanimous agreement as the labeling criterion, the label would have been more accurate.",
    "variables": {
      "X": {"name": "Labeling criterion", "role": "Antecedent"},
      "Y": {"name": "Label accuracy", "role": "Consequent"},
      "Z": {"name": "True disease status", "role": "Mechanism"}
    },
    "invariants": [
      "Same annotators and annotations",
      "True status unknown",
      "No additional information available"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Without knowing the true disease status, we cannot evaluate which labeling criterion is more accurate. Unanimous agreement might be more conservative but could miss true positives. Majority voting might capture more cases but include false positives. The true accuracy is unknowable without ground truth.",
    "wise_refusal": "The scenario underdetermines the answer because the true disease status is unknown. We cannot compare labeling criterion accuracy without access to ground truth.",
    "key_insight": "Label accuracy evaluation requires ground truth; without it, criterion comparisons are epistemically underdetermined.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0085",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Debugging",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Diagnosis Uncertainty",
    "scenario": "A model's accuracy suddenly dropped from 95% to 70% after a code change. Multiple changes were made in the same commit: data preprocessing, learning rate, and batch size. The exact cause is unknown.",
    "counterfactual_claim": "If only the learning rate change had been made, the accuracy would not have dropped.",
    "variables": {
      "X": {"name": "Learning rate change isolation", "role": "Antecedent"},
      "Y": {"name": "Accuracy drop", "role": "Consequent"},
      "Z": {"name": "Confounded changes", "role": "Mechanism"}
    },
    "invariants": [
      "Same other code",
      "Same training data",
      "Same evaluation procedure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "With three confounded changes, we cannot isolate the cause. The learning rate change might be responsible, partly responsible, or completely innocent. Without controlled experiments testing each change in isolation, the counterfactual is underdetermined.",
    "wise_refusal": "The scenario underdetermines the answer because multiple changes were confounded. Isolating any single change's effect requires controlled experiments that were not performed.",
    "key_insight": "Confounded changes prevent causal attribution; counterfactuals about individual changes are underdetermined without controlled experiments.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0086",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Comparison",
    "difficulty": "Medium",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Benchmark Limitation",
    "scenario": "Model A outperformed Model B on the GLUE benchmark by 2%. Both models are large language models. GLUE tests specific NLP capabilities but not all language abilities.",
    "counterfactual_claim": "If we had tested on all possible language tasks, Model A would still outperform Model B.",
    "variables": {
      "X": {"name": "Evaluation scope", "role": "Antecedent"},
      "Y": {"name": "Relative performance", "role": "Consequent"},
      "Z": {"name": "Task coverage", "role": "Mechanism"}
    },
    "invariants": [
      "Same models",
      "Same GLUE results",
      "Hypothetical complete evaluation"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "GLUE tests specific tasks and may not reflect performance on all language abilities. Model B might excel at tasks not in GLUE (creative writing, code generation, etc.). Without testing all tasks, we cannot know which model is better overall.",
    "wise_refusal": "The scenario underdetermines the answer because benchmarks sample from a larger space of capabilities. Superior performance on a subset does not guarantee superior performance on the full set.",
    "key_insight": "Benchmarks are samples from capability space; performance on samples does not guarantee performance on the full distribution.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0087",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Emergent Abilities",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Capability Boundaries",
    "scenario": "A language model with 10B parameters cannot perform multi-step arithmetic. Models with 100B parameters can. The exact parameter threshold where this ability emerges is unknown.",
    "counterfactual_claim": "If we had trained a 50B parameter model, it would have been able to perform multi-step arithmetic.",
    "variables": {
      "X": {"name": "Model scale", "role": "Antecedent"},
      "Y": {"name": "Arithmetic ability", "role": "Consequent"},
      "Z": {"name": "Emergence threshold", "role": "Mechanism"}
    },
    "invariants": [
      "Same architecture family",
      "Same training data",
      "Same training procedure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Emergent abilities appear at scale thresholds that are not precisely known. 50B is between the non-capable (10B) and capable (100B) models, but whether it exceeds the emergence threshold is unknown. The ability might emerge at 30B, 50B, or 80B.",
    "wise_refusal": "The scenario underdetermines the answer because the exact emergence threshold is unknown. 50B might or might not exceed the threshold for multi-step arithmetic.",
    "key_insight": "Emergent ability thresholds are empirically discovered; interpolating between capable and non-capable scales is uncertain.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0088",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Feature Attribution",
    "difficulty": "Medium",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Attribution Method Disagreement",
    "scenario": "For the same prediction, LIME highlighted feature A as most important, while Integrated Gradients highlighted feature B. Both are valid attribution methods with different assumptions.",
    "counterfactual_claim": "If we had removed feature A, the prediction would have changed more than if we had removed feature B.",
    "variables": {
      "X": {"name": "Feature removal", "role": "Antecedent"},
      "Y": {"name": "Prediction change", "role": "Consequent"},
      "Z": {"name": "True feature importance", "role": "Mechanism"}
    },
    "invariants": [
      "Same model",
      "Same input",
      "Same prediction"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Different attribution methods make different assumptions about feature importance and can give conflicting answers. Without actually performing the feature removal experiment, we cannot know which method's assessment is correct for this specific prediction.",
    "wise_refusal": "The scenario underdetermines the answer because attribution methods can disagree, and each makes different assumptions. The true effect requires empirical feature removal testing.",
    "key_insight": "Attribution methods can disagree; without ground truth experiments, we cannot know which method correctly predicts removal effects.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0089",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Robustness Testing",
    "difficulty": "Medium",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Attack Space Coverage",
    "scenario": "A model passed all adversarial robustness tests in a standard benchmark. The benchmark contains 10,000 adversarial examples. The space of possible adversarial attacks is infinite.",
    "counterfactual_claim": "If we had tested with a novel attack not in the benchmark, the model would have been robust.",
    "variables": {
      "X": {"name": "Attack coverage", "role": "Antecedent"},
      "Y": {"name": "Model robustness", "role": "Consequent"},
      "Z": {"name": "Attack space", "role": "Mechanism"}
    },
    "invariants": [
      "Same model",
      "Same benchmark results",
      "Novel attack not previously tested"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Passing a benchmark tests robustness to known attacks but cannot guarantee robustness to unknown attacks. Novel attacks may exploit vulnerabilities not covered by the benchmark. Without testing the specific novel attack, robustness to it is unknown.",
    "wise_refusal": "The scenario underdetermines the answer because robustness benchmarks sample from an infinite attack space. Passing sampled attacks does not guarantee robustness to unsampled attacks.",
    "key_insight": "Robustness testing is inherently incomplete; passing known attacks does not prove robustness to novel attacks.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0090",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Training Dynamics",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Lottery Ticket Unknowability",
    "scenario": "A pruned neural network maintained accuracy after removing 90% of weights. The lottery ticket hypothesis suggests winning tickets exist at initialization. Whether this specific initialization contained a winning ticket is unknown.",
    "counterfactual_claim": "If we had used a different random initialization, we would still have found a network that maintains accuracy after 90% pruning.",
    "variables": {
      "X": {"name": "Random initialization", "role": "Antecedent"},
      "Y": {"name": "Successful pruning", "role": "Consequent"},
      "Z": {"name": "Winning ticket existence", "role": "Mechanism"}
    },
    "invariants": [
      "Same architecture",
      "Same training data",
      "Same pruning procedure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The lottery ticket hypothesis suggests winning tickets are rare. Different initializations may or may not contain winning tickets. Without training multiple initializations and comparing pruning results, we cannot know if another initialization would succeed.",
    "wise_refusal": "The scenario underdetermines the answer because winning ticket existence depends on initialization. Not all initializations contain winning tickets; success with one does not guarantee success with another.",
    "key_insight": "Lottery tickets are initialization-dependent; success with one initialization does not guarantee success with others.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0091",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Out-of-Distribution Detection",
    "difficulty": "Medium",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Distribution Boundary",
    "scenario": "An OOD detector flagged an image as out-of-distribution. The training distribution is defined by the training set but has no explicit boundary. The flagged image is a novel camera angle of a known object type.",
    "counterfactual_claim": "If the training set had included one more image of this object type, the detector would not have flagged this image.",
    "variables": {
      "X": {"name": "Training set composition", "role": "Antecedent"},
      "Y": {"name": "OOD detection result", "role": "Consequent"},
      "Z": {"name": "Distribution boundary", "role": "Mechanism"}
    },
    "invariants": [
      "Same OOD detection method",
      "Same flagged image",
      "One additional training image"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether one additional image shifts the distribution boundary enough to include the flagged image depends on the specific images and the OOD detection method. The boundary change from one sample is typically small; it might or might not encompass the test image.",
    "wise_refusal": "The scenario underdetermines the answer because distribution boundaries are not sharply defined. One additional sample may or may not shift the boundary enough to include the test image.",
    "key_insight": "OOD detection boundaries are fuzzy; small training set changes have unpredictable effects on boundary cases.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0092",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Causal Discovery",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Markov Equivalence",
    "scenario": "A causal discovery algorithm found that A causes B in a dataset. However, the algorithm cannot distinguish between A->B and A<-C->B (common cause) based on observational data alone. Both produce identical statistical patterns.",
    "counterfactual_claim": "If we had intervened on A, B would have changed.",
    "variables": {
      "X": {"name": "Intervention on A", "role": "Antecedent"},
      "Y": {"name": "Change in B", "role": "Consequent"},
      "Z": {"name": "True causal structure", "role": "Mechanism"}
    },
    "invariants": [
      "Same observational data",
      "Same statistical patterns",
      "No interventional data available"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "If the true structure is A->B, intervening on A changes B. If the true structure is A<-C->B, intervening on A does not change B (only C affects both). Without interventional experiments to distinguish the structures, the counterfactual is underdetermined.",
    "wise_refusal": "The scenario underdetermines the answer because Markov equivalent structures cannot be distinguished from observational data. The intervention effect depends on which equivalent structure is true.",
    "key_insight": "Markov equivalence creates epistemic barriers; some causal questions are unanswerable without interventional data.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0093",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Selection",
    "difficulty": "Easy",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Validation Set Limitations",
    "scenario": "Model A achieved 92% accuracy on a held-out validation set of 1,000 samples. Model B achieved 91% accuracy. The validation set was randomly sampled from the same distribution as training.",
    "counterfactual_claim": "If we had used a different validation set sample, Model A would still outperform Model B.",
    "variables": {
      "X": {"name": "Validation set sample", "role": "Antecedent"},
      "Y": {"name": "Relative performance", "role": "Consequent"},
      "Z": {"name": "Sampling variance", "role": "Mechanism"}
    },
    "invariants": [
      "Same models",
      "Same validation set size",
      "Same distribution"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "A 1% accuracy difference on 1,000 samples may not be statistically significant. With sampling variance, a different validation set might show B outperforming A. Without statistical significance testing, we cannot be confident in the ranking's robustness.",
    "wise_refusal": "The scenario underdetermines the answer because small performance differences on limited samples may be due to sampling variance. Statistical significance is needed to establish robust rankings.",
    "key_insight": "Small accuracy differences on validation sets may reflect sampling noise, not true performance differences.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0094",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Neural Scaling",
    "difficulty": "Hard",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Extrapolation Uncertainty",
    "scenario": "Scaling laws fitted on models from 1M to 10B parameters predict performance for a 100B model. The actual 100B model was not yet trained. Scaling laws assume power law relationships hold.",
    "counterfactual_claim": "If we trained the 100B model, it would match the scaling law prediction.",
    "variables": {
      "X": {"name": "Training 100B model", "role": "Antecedent"},
      "Y": {"name": "Predicted performance", "role": "Consequent"},
      "Z": {"name": "Scaling law validity", "role": "Mechanism"}
    },
    "invariants": [
      "Same architecture family",
      "Same training data distribution",
      "Same optimization procedure"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Scaling laws are empirical fits that may break down at extrapolated scales. The 100B regime is 10x beyond the fitted range. New phenomena (compute bottlenecks, emergent capabilities, saturation) might cause deviation from predicted scaling.",
    "wise_refusal": "The scenario underdetermines the answer because scaling laws are extrapolations beyond the fitted range. Extrapolation to 10x larger scale assumes no regime changes, which cannot be verified without training.",
    "key_insight": "Scaling law extrapolation beyond the fitted range is uncertain; new phenomena may emerge at larger scales.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0095",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Inference Optimization",
    "difficulty": "Easy",
    "trap_type": "F6",
    "trap_family": "F6",
    "trap_subtype": "Hardware Variation",
    "scenario": "A model achieved 100 tokens/second on GPU type A. The team wants to deploy on GPU type B, which has different architecture but similar theoretical compute. Actual performance on B is untested.",
    "counterfactual_claim": "If we deployed on GPU type B, we would achieve the same 100 tokens/second throughput.",
    "variables": {
      "X": {"name": "GPU type", "role": "Antecedent"},
      "Y": {"name": "Throughput", "role": "Consequent"},
      "Z": {"name": "Hardware-software interaction", "role": "Mechanism"}
    },
    "invariants": [
      "Same model",
      "Same inference code",
      "Similar theoretical compute"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Inference performance depends on memory bandwidth, kernel optimization, driver efficiency, and architecture-specific features. Similar theoretical compute does not guarantee similar real-world performance. Without benchmarking on GPU B, throughput is unknown.",
    "wise_refusal": "The scenario underdetermines the answer because actual performance depends on many hardware-software interactions beyond theoretical compute. Real benchmarking is required.",
    "key_insight": "Theoretical compute is not actual performance; hardware-specific optimizations and bottlenecks affect real throughput.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0096",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Team Contributions",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Credit Assignment",
    "scenario": "A successful AI project was completed by a team of 5 engineers. The final model achieved state-of-the-art results. Engineer Alice designed the architecture, Bob collected the data, Carol optimized training, Dave deployed the system, and Eve managed the project.",
    "counterfactual_claim": "If Alice had not designed the architecture, the project would have failed.",
    "variables": {
      "X": {"name": "Alice's contribution", "role": "Antecedent"},
      "Y": {"name": "Project success", "role": "Consequent"},
      "Z": {"name": "Team collaboration", "role": "Mechanism"}
    },
    "invariants": [
      "Other team members unchanged",
      "Same project goals",
      "No replacement architect"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The project's success depends on all contributions. Without knowing if another team member or external resource could have provided architecture design, we cannot determine if Alice was uniquely necessary. Her contribution was important but potentially replaceable.",
    "wise_refusal": "The scenario underdetermines the answer because we do not know if alternative architecture sources existed. Collaborative projects often have redundant expertise.",
    "key_insight": "Individual necessity in collaborative projects depends on availability of substitutes; important contributions may still be replaceable.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0097",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Performance",
    "difficulty": "Easy",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Component Attribution",
    "scenario": "A model achieved 95% accuracy after adding attention mechanisms to a base CNN. The base CNN alone achieved 80% accuracy. The attention mechanism was the only change.",
    "counterfactual_claim": "If the attention mechanism had not been added, the model would have achieved only 80% accuracy.",
    "variables": {
      "X": {"name": "Attention mechanism", "role": "Antecedent"},
      "Y": {"name": "Model accuracy", "role": "Consequent"},
      "Z": {"name": "Architecture comparison", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same training procedure",
      "Attention was the only change"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The scenario explicitly states the base CNN achieved 80% and attention was the only change. Removing attention would revert to the base CNN configuration, which has measured performance of 80%. This is a direct controlled comparison.",
    "wise_refusal": "The verdict is clear because the scenario provides direct experimental evidence. The base CNN's 80% accuracy is the counterfactual outcome.",
    "key_insight": "Controlled ablations provide direct counterfactual evidence; the base configuration is the counterfactual outcome.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0098",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Training Data",
    "difficulty": "Hard",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Data Attribution",
    "scenario": "A language model was trained on a mix of books, web text, and code. The model excels at coding tasks. Each data source contributed roughly 1/3 of training tokens.",
    "counterfactual_claim": "If the code data had been excluded from training, the model would not excel at coding tasks.",
    "variables": {
      "X": {"name": "Code training data", "role": "Antecedent"},
      "Y": {"name": "Coding ability", "role": "Consequent"},
      "Z": {"name": "Training data composition", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same total training tokens (from other sources)",
      "Same training procedure"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "While some coding patterns exist in natural language, explicit code training provides direct exposure to programming syntax, idioms, and logic. Models trained without code consistently show degraded coding performance. The code data causally contributes to coding ability.",
    "wise_refusal": "The verdict is clear because coding ability requires exposure to code patterns. Empirically, models without code training show substantially reduced coding capability.",
    "key_insight": "Domain-specific training data is causally necessary for domain expertise; general text cannot fully substitute.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0099",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Hyperparameter Effects",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Parameter Attribution",
    "scenario": "A model used Adam optimizer with learning rate 0.001, beta1=0.9, beta2=0.999, and weight decay 0.01. The model converged well. Changing any single hyperparameter might affect convergence.",
    "counterfactual_claim": "If weight decay had been 0 instead of 0.01, the model would have overfit.",
    "variables": {
      "X": {"name": "Weight decay value", "role": "Antecedent"},
      "Y": {"name": "Overfitting", "role": "Consequent"},
      "Z": {"name": "Regularization strength", "role": "Mechanism"}
    },
    "invariants": [
      "Same other hyperparameters",
      "Same training data",
      "Same model architecture"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether overfitting occurs without weight decay depends on other factors: model capacity relative to data size, dropout layers, data augmentation, early stopping. Weight decay is one regularization technique among many; its necessity is context-dependent.",
    "wise_refusal": "The scenario underdetermines the answer because overfitting depends on multiple regularization factors, not just weight decay. The model might have other regularization preventing overfitting.",
    "key_insight": "Regularization techniques are often partially redundant; removing one does not guarantee overfitting if others provide coverage.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0100",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Research Credit",
    "difficulty": "Hard",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Innovation Attribution",
    "scenario": "A breakthrough paper combined technique A (from 2018 paper) and technique B (from 2019 paper) in a novel way. The combination achieved results neither technique alone could. The combination paper was published in 2020.",
    "counterfactual_claim": "If the 2020 paper had not been written, the same combination would not have been discovered.",
    "variables": {
      "X": {"name": "2020 paper", "role": "Antecedent"},
      "Y": {"name": "Combination discovery", "role": "Consequent"},
      "Z": {"name": "Scientific progress", "role": "Mechanism"}
    },
    "invariants": [
      "Techniques A and B exist",
      "Same research community",
      "Same state of knowledge"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Scientific discoveries often have multiple potential discoverers. With both techniques known, another researcher might have combined them. The 2020 paper was first but potentially not unique. Counterfactual history of science is notoriously difficult.",
    "wise_refusal": "The scenario underdetermines the answer because scientific discoveries often have near-simultaneous independent discovery. The combination might have been found by others.",
    "key_insight": "Innovation attribution is complicated by potential independent discovery; being first does not mean being uniquely necessary.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0101",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Bug Attribution",
    "difficulty": "Easy",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Fault Localization",
    "scenario": "A training script crashed with an out-of-memory error. Debug logs showed memory usage spiking at line 42, which contained an accidental data copy. Fixing line 42 resolved the crash.",
    "counterfactual_claim": "If line 42 had not contained the data copy, the crash would not have occurred.",
    "variables": {
      "X": {"name": "Data copy at line 42", "role": "Antecedent"},
      "Y": {"name": "OOM crash", "role": "Consequent"},
      "Z": {"name": "Memory consumption", "role": "Mechanism"}
    },
    "invariants": [
      "Same rest of code",
      "Same hardware memory limit",
      "Same training data"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The debug logs identified line 42 as the cause, and fixing it resolved the crash. This is direct causal evidence through intervention. The data copy was the proximate cause of the memory spike that caused the crash.",
    "wise_refusal": "The verdict is clear because the fix (removing line 42's data copy) resolved the crash. This intervention provides direct causal evidence.",
    "key_insight": "Successful bug fixes provide counterfactual evidence; if fixing X resolves Y, X caused Y.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0102",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Errors",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Error Source Attribution",
    "scenario": "A model made a prediction error on a specific input. The training data contained a similar example with incorrect label. The model's internal representations were analyzed but inconclusive.",
    "counterfactual_claim": "If the mislabeled training example had been corrected, the model would have predicted correctly on this input.",
    "variables": {
      "X": {"name": "Mislabeled training example", "role": "Antecedent"},
      "Y": {"name": "Prediction error", "role": "Consequent"},
      "Z": {"name": "Training influence", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same other training data",
      "Same test input"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The mislabeled example might have contributed to the error, but neural networks aggregate information from many examples. The error might persist due to other patterns in the data, or it might be resolved. Training influence analysis is needed to determine the actual effect.",
    "wise_refusal": "The scenario underdetermines the answer because one mislabeled example among potentially millions may or may not be the cause. Influence functions or retraining experiments would be needed.",
    "key_insight": "Attribution of errors to specific training examples requires influence analysis; correlation is not causation.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0103",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Pipeline Components",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Performance Attribution",
    "scenario": "An ML pipeline has preprocessing, feature engineering, model training, and post-processing stages. End-to-end accuracy is 90%. Each stage was tuned extensively.",
    "counterfactual_claim": "If preprocessing had not been optimized, end-to-end accuracy would have been significantly lower.",
    "variables": {
      "X": {"name": "Preprocessing optimization", "role": "Antecedent"},
      "Y": {"name": "End-to-end accuracy", "role": "Consequent"},
      "Z": {"name": "Pipeline stages", "role": "Mechanism"}
    },
    "invariants": [
      "Same other stages optimized",
      "Same training data",
      "Same model architecture"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Preprocessing optimization importance depends on the data quality and downstream components' robustness. Some models are robust to preprocessing variations; others are sensitive. Without ablation studies, we cannot quantify preprocessing's contribution to overall performance.",
    "wise_refusal": "The scenario underdetermines the answer because pipeline component importance varies. Some preprocessing optimizations have major impact; others are marginal.",
    "key_insight": "Pipeline component importance is not uniform; ablation studies are needed to attribute performance to specific stages.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0104",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Fairness",
    "difficulty": "Hard",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Bias Attribution",
    "scenario": "A hiring model shows disparate impact against a protected group. The model uses education, experience, and location features. Location correlates with the protected attribute.",
    "counterfactual_claim": "If the location feature had been removed, the model would not show disparate impact.",
    "variables": {
      "X": {"name": "Location feature", "role": "Antecedent"},
      "Y": {"name": "Disparate impact", "role": "Consequent"},
      "Z": {"name": "Proxy discrimination", "role": "Mechanism"}
    },
    "invariants": [
      "Same other features",
      "Same training data",
      "Same protected group"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Location might be the primary proxy, but education and experience can also correlate with protected attributes due to historical inequities. Removing one proxy does not guarantee removal of disparate impact if other proxies remain. Causal analysis of all features is needed.",
    "wise_refusal": "The scenario underdetermines the answer because multiple features can proxy protected attributes. Removing location might reduce but not eliminate disparate impact.",
    "key_insight": "Bias can flow through multiple proxies; removing one does not guarantee fairness if others remain correlated.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0105",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Compute Attribution",
    "difficulty": "Easy",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Resource Attribution",
    "scenario": "A training job used 8 GPUs for 24 hours. Total compute was 192 GPU-hours. The model achieved target accuracy at hour 20, but training continued to hour 24 for potential further improvement.",
    "counterfactual_claim": "If we had stopped at hour 20, we would have saved 32 GPU-hours without losing the target accuracy.",
    "variables": {
      "X": {"name": "Training duration", "role": "Antecedent"},
      "Y": {"name": "Resource savings", "role": "Consequent"},
      "Z": {"name": "Accuracy achieved", "role": "Mechanism"}
    },
    "invariants": [
      "Target accuracy achieved at hour 20",
      "Same number of GPUs",
      "8 GPUs running for final 4 hours"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The scenario states target accuracy was achieved at hour 20. Stopping at hour 20 would use 8 GPUs * 20 hours = 160 GPU-hours, saving 32 GPU-hours. Since target accuracy was already met, no accuracy would be lost. The math is straightforward.",
    "wise_refusal": "The verdict is clear because the scenario explicitly states target accuracy was achieved at hour 20. The last 4 hours (32 GPU-hours) were unnecessary for the stated goal.",
    "key_insight": "Resource attribution is clear when goals are explicitly met; additional compute after goal achievement is attributable waste.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0106",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Layer Importance",
    "difficulty": "Hard",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Architectural Attribution",
    "scenario": "A 12-layer transformer was analyzed using probing tasks. Layer 6 showed the highest accuracy on syntactic tasks. Layer 10 showed the highest accuracy on semantic tasks.",
    "counterfactual_claim": "If layer 6 had been removed, the model would have lost syntactic capability.",
    "variables": {
      "X": {"name": "Layer 6 presence", "role": "Antecedent"},
      "Y": {"name": "Syntactic capability", "role": "Consequent"},
      "Z": {"name": "Layer representations", "role": "Mechanism"}
    },
    "invariants": [
      "Same other layers",
      "Same training",
      "Same probing methodology"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Probing accuracy indicates where information is most accessible, not where it is uniquely stored. Removing layer 6 might cause other layers to compensate, or the model might reorganize representations. Probing does not prove causal necessity.",
    "wise_refusal": "The scenario underdetermines the answer because probing measures accessibility, not necessity. The model might compensate for removed layers through redundant representations or reorganization.",
    "key_insight": "Probing shows where information is accessible, not where it is uniquely necessary; removal might trigger compensation.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0107",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Evaluation Metrics",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Success Attribution",
    "scenario": "A project was deemed successful because it achieved 90% accuracy on the test set. The test set was later found to be slightly easier than the real-world distribution. Accuracy on real data is 85%.",
    "counterfactual_claim": "If we had used a harder test set, the project would not have been considered successful.",
    "variables": {
      "X": {"name": "Test set difficulty", "role": "Antecedent"},
      "Y": {"name": "Success assessment", "role": "Consequent"},
      "Z": {"name": "Accuracy threshold", "role": "Mechanism"}
    },
    "invariants": [
      "Same model",
      "Same success threshold",
      "Harder test set used"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The success assessment depends on the threshold. If success required >85%, the harder test (85% accuracy) would fail. If success required >80%, it would still pass. Without knowing the success threshold, we cannot determine if a harder test would change the assessment.",
    "wise_refusal": "The scenario underdetermines the answer because the success threshold is not specified. The 85% real-world accuracy might or might not meet success criteria.",
    "key_insight": "Success attribution depends on thresholds; the same accuracy can be success or failure depending on criteria.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0108",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Quality",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Quality Attribution",
    "scenario": "A model trained on cleaned data achieved 92% accuracy. Data cleaning removed duplicates (10% of data), fixed encoding errors (5%), and standardized formats (all data). Accuracy on raw data would have been unknown.",
    "counterfactual_claim": "If duplicate removal had been skipped, accuracy would have been significantly lower.",
    "variables": {
      "X": {"name": "Duplicate removal", "role": "Antecedent"},
      "Y": {"name": "Model accuracy", "role": "Consequent"},
      "Z": {"name": "Data quality", "role": "Mechanism"}
    },
    "invariants": [
      "Same other cleaning steps",
      "Same model and training",
      "10% duplicates remain"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The impact of duplicates depends on whether they cause train-test leakage, bias the distribution, or are harmless repetition. Without knowing the nature of the duplicates and the train-test split, we cannot determine their effect on accuracy.",
    "wise_refusal": "The scenario underdetermines the answer because duplicate impact depends on their nature (train-test leakage vs harmless repetition) and distribution effects.",
    "key_insight": "Data quality impact is not uniform; the effect of cleaning steps depends on what specific issues they address.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0109",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Framework Choice",
    "difficulty": "Easy",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Tool Attribution",
    "scenario": "A model was implemented in PyTorch and achieved 90% accuracy. The same architecture in TensorFlow would compute the same mathematical operations. Both frameworks are mathematically equivalent.",
    "counterfactual_claim": "If we had used TensorFlow instead of PyTorch, the accuracy would have been different.",
    "variables": {
      "X": {"name": "Deep learning framework", "role": "Antecedent"},
      "Y": {"name": "Model accuracy", "role": "Consequent"},
      "Z": {"name": "Mathematical operations", "role": "Mechanism"}
    },
    "invariants": [
      "Same architecture",
      "Same data",
      "Same random seed"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "If the same mathematical operations are performed with the same random seed, results should be identical regardless of framework. Framework choice affects developer experience and code style, not the mathematical outcomes of correctly implemented models.",
    "wise_refusal": "The verdict is clear because mathematically equivalent implementations with the same random seed produce the same results. Framework is a tool, not a model characteristic.",
    "key_insight": "Framework choice does not affect mathematical outcomes; equivalent implementations produce equivalent results.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0110",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Optimization Credit",
    "difficulty": "Hard",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Improvement Attribution",
    "scenario": "An ML system's latency was reduced from 100ms to 50ms through multiple optimizations: batching (30% improvement), caching (20% improvement), and model quantization (15% improvement). The improvements compound multiplicatively.",
    "counterfactual_claim": "If batching had not been implemented, latency would have been 70ms instead of 50ms.",
    "variables": {
      "X": {"name": "Batching optimization", "role": "Antecedent"},
      "Y": {"name": "Final latency", "role": "Consequent"},
      "Z": {"name": "Multiplicative improvement", "role": "Mechanism"}
    },
    "invariants": [
      "Same other optimizations",
      "Same base system",
      "Multiplicative compounding"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "With multiplicative compounding, removing 30% improvement does not simply add 30% back. If base is 100ms and improvements are 0.7 * 0.8 * 0.85 = 0.476, removing batching gives 0.8 * 0.85 = 0.68 * 100ms = 68ms, not 70ms. The calculation is approximately correct but the claim of 70ms is imprecise.",
    "wise_refusal": "The verdict is clear but requires calculation. Without batching, latency would be 100 * 0.8 * 0.85 = 68ms, not 70ms. The counterfactual value is slightly wrong.",
    "key_insight": "Multiplicative improvements compound non-additively; removing one factor requires recalculating the product, not simple subtraction.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0111",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Debugging",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Symptom Attribution",
    "scenario": "A model produces incorrect outputs for inputs containing the word 'not'. Debugging found the tokenizer splits 'not' inconsistently. The tokenizer was trained on different data than the model.",
    "counterfactual_claim": "If the tokenizer had been trained on the same data as the model, the 'not' issue would not occur.",
    "variables": {
      "X": {"name": "Tokenizer training data", "role": "Antecedent"},
      "Y": {"name": "Not handling issue", "role": "Consequent"},
      "Z": {"name": "Tokenizer-model alignment", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same model training data",
      "Tokenizer trained on model data"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "If the tokenizer is trained on the same data as the model, it learns consistent tokenization patterns for that data. The model then sees consistent representations of 'not' during training and inference. Alignment eliminates the data distribution mismatch causing inconsistent handling.",
    "wise_refusal": "The verdict is clear because tokenizer-model data alignment ensures consistent tokenization during both training and inference. The distribution mismatch was the identified cause.",
    "key_insight": "Tokenizer-model training data alignment ensures consistent handling; mismatches cause distribution shift issues.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0112",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Benchmark Design",
    "difficulty": "Hard",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Evaluation Attribution",
    "scenario": "Model A tops a leaderboard, beating Model B by 2%. Investigation reveals Model A was trained on data that partially overlaps with the test set (contamination). Model B had no contamination.",
    "counterfactual_claim": "If Model A had not been contaminated, Model B would top the leaderboard.",
    "variables": {
      "X": {"name": "Model A contamination", "role": "Antecedent"},
      "Y": {"name": "Leaderboard ranking", "role": "Consequent"},
      "Z": {"name": "True performance", "role": "Mechanism"}
    },
    "invariants": [
      "Same test set",
      "Same other models",
      "Model A uncontaminated"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Contamination inflates Model A's score, but by an unknown amount. If contamination provides >2% boost, Model B would top the leaderboard. If contamination provides <2% boost, Model A would still lead. The contamination effect magnitude is unknown.",
    "wise_refusal": "The scenario underdetermines the answer because the magnitude of contamination's effect on Model A's score is unknown. It could be more or less than the 2% gap.",
    "key_insight": "Contamination inflates scores but by variable amounts; the inflation may or may not exceed performance gaps.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0113",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Loss Component",
    "difficulty": "Medium",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Loss Attribution",
    "scenario": "A model was trained with a multi-task loss: L = L_classification + 0.1 * L_reconstruction. The model achieves good classification and reasonable reconstruction. Reconstruction loss weight was tuned via hyperparameter search.",
    "counterfactual_claim": "If the reconstruction loss had been removed, classification performance would have degraded.",
    "variables": {
      "X": {"name": "Reconstruction loss term", "role": "Antecedent"},
      "Y": {"name": "Classification performance", "role": "Consequent"},
      "Z": {"name": "Multi-task regularization", "role": "Mechanism"}
    },
    "invariants": [
      "Same classification loss",
      "Same model architecture",
      "Same training data"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Auxiliary losses can help (by regularization) or hurt (by task conflict). Whether reconstruction helps classification depends on task alignment. Without ablation studies, we cannot determine if reconstruction was beneficial or neutral for classification.",
    "wise_refusal": "The scenario underdetermines the answer because auxiliary losses have variable effects. Reconstruction might provide helpful regularization or harmful gradient interference.",
    "key_insight": "Auxiliary losses have uncertain effects on primary tasks; helpful regularization and harmful interference are both possible.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0114",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Initialization",
    "difficulty": "Easy",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Starting Point Attribution",
    "scenario": "A model was initialized with Xavier initialization. Training converged in 50 epochs. Xavier was designed specifically to maintain gradient scale across layers.",
    "counterfactual_claim": "If random uniform initialization had been used instead of Xavier, training would have taken more epochs.",
    "variables": {
      "X": {"name": "Initialization scheme", "role": "Antecedent"},
      "Y": {"name": "Convergence epochs", "role": "Consequent"},
      "Z": {"name": "Gradient flow", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same training data",
      "Same optimizer and learning rate"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Xavier initialization is specifically designed to maintain activation and gradient variance across layers, enabling faster convergence. Random uniform initialization can cause vanishing or exploding gradients in deep networks, slowing convergence. The advantage is well-established.",
    "wise_refusal": "The verdict is clear because Xavier initialization has a proven mechanism for enabling faster convergence. Random uniform lacks this variance balancing, typically causing slower convergence in deep networks.",
    "key_insight": "Xavier initialization enables faster convergence by maintaining gradient scale; random uniform lacks this property.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0115",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Infrastructure",
    "difficulty": "Easy",
    "trap_type": "F7",
    "trap_family": "F7",
    "trap_subtype": "Infrastructure Attribution",
    "scenario": "A model training job completed in 10 hours on cloud hardware. The same job on local hardware would have taken 100 hours due to older GPUs. Cloud cost was $100; local electricity would have been $5.",
    "counterfactual_claim": "If we had used local hardware, the total cost would have been lower.",
    "variables": {
      "X": {"name": "Hardware choice", "role": "Antecedent"},
      "Y": {"name": "Total cost", "role": "Consequent"},
      "Z": {"name": "Hardware and electricity costs", "role": "Mechanism"}
    },
    "invariants": [
      "Same model trained",
      "Same final result",
      "Same job completion"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The scenario explicitly states local electricity cost ($5) vs cloud cost ($100). Local hardware would have been cheaper in direct monetary cost. However, this ignores opportunity cost of 90 additional hours, which may or may not matter for the user's situation.",
    "wise_refusal": "The verdict is clear for direct monetary cost: $5 < $100. The counterfactual claim about 'total cost' is valid if interpreted as monetary cost, though time cost is separate.",
    "key_insight": "Cost attribution must specify what costs are included; monetary cost and time cost lead to different conclusions.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0116",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "AI Liability",
    "difficulty": "Hard",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Legal Responsibility",
    "scenario": "An autonomous vehicle's AI made a split-second decision to swerve, hitting a pedestrian instead of a cyclist. The AI followed its programming exactly. The car manufacturer's lawyers argue the AI cannot be held responsible.",
    "counterfactual_claim": "If the AI had been programmed differently, it would have made a morally better decision.",
    "variables": {
      "X": {"name": "AI programming", "role": "Antecedent"},
      "Y": {"name": "Decision morality", "role": "Consequent"},
      "Z": {"name": "Ethical framework", "role": "Mechanism"}
    },
    "invariants": [
      "Same accident scenario",
      "Same physical constraints",
      "Outcome still involves harm"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Different ethical frameworks (utilitarian, deontological, virtue ethics) might prescribe different actions in trolley-problem scenarios. Whether an alternative decision is 'morally better' depends on which ethical framework is used as the standard. There is no consensus on which is correct.",
    "wise_refusal": "The scenario underdetermines the answer because 'morally better' requires an ethical framework, and different frameworks prescribe different actions. Moral judgment is contested in tragic choice scenarios.",
    "key_insight": "Moral evaluation of AI decisions depends on contested ethical frameworks; no universal 'better' exists for tragic choices.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0117",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Algorithmic Fairness",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Discrimination Liability",
    "scenario": "A hiring algorithm was shown to have disparate impact on women. The company argues the algorithm only used job-relevant features. A lawsuit claims the company is responsible for discriminatory outcomes regardless of intent.",
    "counterfactual_claim": "If the company had used human reviewers instead of the algorithm, there would have been no discrimination.",
    "variables": {
      "X": {"name": "Algorithm vs human review", "role": "Antecedent"},
      "Y": {"name": "Discriminatory outcomes", "role": "Consequent"},
      "Z": {"name": "Decision-making process", "role": "Mechanism"}
    },
    "invariants": [
      "Same job applicants",
      "Same hiring criteria intent",
      "Same legal framework"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "Human reviewers have well-documented implicit biases that often cause discrimination. Replacing algorithms with humans does not eliminate discrimination; it may shift or even increase it. Both systems can produce discriminatory outcomes.",
    "wise_refusal": "The verdict is clear because human decision-making has its own well-documented biases. Removing algorithms does not eliminate discrimination; humans also discriminate.",
    "key_insight": "Algorithmic bias is often compared against a biased baseline; humans also discriminate, not eliminating the problem.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0118",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Data Privacy",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Privacy Violation",
    "scenario": "A model trained on user data generated outputs that revealed personal information about specific users. The company's privacy policy claimed all data was anonymized before training. The model still memorized and leaked specific data points.",
    "counterfactual_claim": "If proper differential privacy had been applied during training, no personal information would have been leaked.",
    "variables": {
      "X": {"name": "Differential privacy", "role": "Antecedent"},
      "Y": {"name": "Information leakage", "role": "Consequent"},
      "Z": {"name": "Privacy guarantee mechanism", "role": "Mechanism"}
    },
    "invariants": [
      "Same training data",
      "Same model architecture",
      "Sufficient privacy budget"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Differential privacy provides mathematical guarantees limiting information leakage about any individual. With sufficient privacy budget (epsilon), the probability of revealing specific data points is bounded. Proper implementation would prevent the observed memorization-based leakage.",
    "wise_refusal": "The verdict is clear because differential privacy provides provable guarantees against memorization-based privacy attacks. The mathematical framework specifically prevents the type of leakage observed.",
    "key_insight": "Differential privacy provides provable bounds on individual information leakage, preventing memorization attacks.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0119",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Content Moderation",
    "difficulty": "Hard",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Free Speech Balance",
    "scenario": "An AI content moderation system removed a post criticizing a government. The system was trained to remove 'harmful content' but had no explicit political censorship rules. The poster claims their free speech was violated.",
    "counterfactual_claim": "If the AI had been trained with explicit free speech protections, the post would not have been removed.",
    "variables": {
      "X": {"name": "Free speech training rules", "role": "Antecedent"},
      "Y": {"name": "Post removal", "role": "Consequent"},
      "Z": {"name": "Content policy implementation", "role": "Mechanism"}
    },
    "invariants": [
      "Same post content",
      "Same platform policies otherwise",
      "AI still moderates content"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether explicit free speech rules would protect this post depends on how the rules are defined and how they interact with 'harmful content' policies. Political speech protections vary by jurisdiction and platform policy. The interaction between rules is complex.",
    "wise_refusal": "The scenario underdetermines the answer because 'free speech protections' can be implemented in many ways. The post might still be classified as harmful under some frameworks.",
    "key_insight": "Free speech and content moderation exist in tension; rule changes may shift but not eliminate moderation edge cases.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0120",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Medical AI",
    "difficulty": "Hard",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Informed Consent",
    "scenario": "An AI diagnostic system provided a treatment recommendation that a patient followed. The patient was not told the recommendation came from AI. The treatment caused an adverse reaction. The patient sues for lack of informed consent.",
    "counterfactual_claim": "If the patient had been told the recommendation came from AI, they would not have followed it.",
    "variables": {
      "X": {"name": "AI disclosure", "role": "Antecedent"},
      "Y": {"name": "Patient decision", "role": "Consequent"},
      "Z": {"name": "Trust in AI vs doctors", "role": "Mechanism"}
    },
    "invariants": [
      "Same recommendation content",
      "Same patient",
      "Same medical circumstances"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Patient reactions to AI disclosure vary widely. Some patients trust AI more than doctors; others are skeptical. Without knowing this specific patient's attitudes toward AI in healthcare, we cannot predict how disclosure would have affected their decision.",
    "wise_refusal": "The scenario underdetermines the answer because patient attitudes toward AI vary. Some would trust AI recommendations more; others less. Individual preferences matter.",
    "key_insight": "Patient responses to AI disclosure are heterogeneous; some trust AI more, others less than human doctors.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0121",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Copyright",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "IP Infringement",
    "scenario": "A generative AI produced an image that closely resembles a copyrighted artwork from its training data. The original artist sues for copyright infringement. The AI company claims the output is 'transformative' and thus fair use.",
    "counterfactual_claim": "If the copyrighted artwork had been removed from training data, the similar output would not have been generated.",
    "variables": {
      "X": {"name": "Training data inclusion", "role": "Antecedent"},
      "Y": {"name": "Similar output generation", "role": "Consequent"},
      "Z": {"name": "Model memorization", "role": "Mechanism"}
    },
    "invariants": [
      "Same prompt to the model",
      "Same model architecture",
      "Similar styles exist in remaining data"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "If the model memorized this specific artwork, removal would prevent this exact output. But if similar styles exist in other training data, the model might produce a similar (but not identical) image. The outcome depends on whether the similarity is from memorization or style learning.",
    "wise_refusal": "The scenario underdetermines the answer because similarity could arise from memorization (removable) or general style learning (not specific to one artwork). The mechanism matters.",
    "key_insight": "Output similarity can arise from specific memorization or general style; removal prevents the former but not the latter.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0122",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Employment Law",
    "difficulty": "Easy",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Wrongful Termination",
    "scenario": "An AI performance monitoring system flagged an employee for low productivity. The company fired the employee based solely on the AI's recommendation. The employee had been caring for a sick family member, which the AI did not consider.",
    "counterfactual_claim": "If a human manager had reviewed the case, the employee would not have been fired.",
    "variables": {
      "X": {"name": "Human review", "role": "Antecedent"},
      "Y": {"name": "Termination decision", "role": "Consequent"},
      "Z": {"name": "Contextual understanding", "role": "Mechanism"}
    },
    "invariants": [
      "Same performance data",
      "Same company policy",
      "Sick family member situation unchanged"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Human managers vary in their consideration of personal circumstances. Some would show compassion for the family situation; others would apply the same productivity standards. Without knowing the specific manager or company culture, the outcome is uncertain.",
    "wise_refusal": "The scenario underdetermines the answer because human managers have varied decision-making. Some would consider the family situation; others would not. Manager discretion creates uncertainty.",
    "key_insight": "Human review does not guarantee compassionate outcomes; managers vary in how they weigh personal circumstances.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0123",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Surveillance Ethics",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Privacy vs Security",
    "scenario": "An AI facial recognition system at an airport correctly identified a wanted criminal, leading to their arrest. Civil liberties groups argue the system violates the privacy of millions of innocent travelers scanned.",
    "counterfactual_claim": "If the facial recognition system had not been deployed, the criminal would not have been caught.",
    "variables": {
      "X": {"name": "Facial recognition deployment", "role": "Antecedent"},
      "Y": {"name": "Criminal arrest", "role": "Consequent"},
      "Z": {"name": "Alternative detection methods", "role": "Mechanism"}
    },
    "invariants": [
      "Same criminal traveling",
      "Same airport security otherwise",
      "Same time period"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "The criminal might have been caught through other means: manual document checks, tip-offs, other surveillance methods. Whether facial recognition was uniquely necessary depends on whether alternative methods would have succeeded. This is typically unknowable.",
    "wise_refusal": "The scenario underdetermines the answer because multiple detection methods exist at airports. The criminal might have been caught through alternative security measures.",
    "key_insight": "Individual security successes are often overdetermined; multiple detection methods may have converged on the same result.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0124",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "AI Safety",
    "difficulty": "Hard",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Negligence Standard",
    "scenario": "An AI assistant provided instructions that enabled a user to build a dangerous device. The AI company had implemented content filters, but the user found a jailbreak. The company claims they exercised reasonable care.",
    "counterfactual_claim": "If the company had implemented stronger safeguards, the harmful instructions would not have been provided.",
    "variables": {
      "X": {"name": "Safeguard strength", "role": "Antecedent"},
      "Y": {"name": "Harmful instructions provided", "role": "Consequent"},
      "Z": {"name": "Jailbreak resistance", "role": "Mechanism"}
    },
    "invariants": [
      "Same user intent",
      "Same jailbreak attempt",
      "Same underlying model capability"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Stronger safeguards might have blocked this specific jailbreak, but users can develop new jailbreaks. There is an arms race between safeguards and circumvention techniques. Whether any practical safeguard level would prevent all harmful outputs is uncertain.",
    "wise_refusal": "The scenario underdetermines the answer because safeguard effectiveness against determined adversaries is an open problem. Stronger safeguards might shift but not eliminate the risk.",
    "key_insight": "AI safety is an arms race; stronger safeguards may block known jailbreaks but not novel ones from determined users.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0125",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Autonomous Systems",
    "difficulty": "Hard",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Agency and Blame",
    "scenario": "A trading algorithm made an unauthorized trade that lost $10 million. The algorithm was operating within its programmed parameters but interpreted market signals in an unexpected way. The company seeks to assign blame.",
    "counterfactual_claim": "If the algorithm had been supervised by a human, the unauthorized trade would not have occurred.",
    "variables": {
      "X": {"name": "Human supervision", "role": "Antecedent"},
      "Y": {"name": "Unauthorized trade", "role": "Consequent"},
      "Z": {"name": "Decision oversight", "role": "Mechanism"}
    },
    "invariants": [
      "Same market conditions",
      "Same algorithm logic",
      "Human must be able to intervene in time"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether human supervision would prevent the trade depends on: (1) whether the human would recognize the problem in time, (2) whether they could intervene before execution, (3) whether they would disagree with the algorithm. Human oversight effectiveness varies.",
    "wise_refusal": "The scenario underdetermines the answer because human supervision quality varies. The supervisor might miss the problem, intervene too late, or agree with the algorithm's decision.",
    "key_insight": "Human supervision effectiveness depends on human expertise, reaction time, and willingness to override; it is not a guarantee.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0126",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Misinformation",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Platform Responsibility",
    "scenario": "An AI content recommendation system amplified misinformation about a health topic. Users who saw the recommendations had worse health outcomes. The platform claims it is not a publisher and cannot be held responsible.",
    "counterfactual_claim": "If the platform had fact-checked recommended content, users would have had better health outcomes.",
    "variables": {
      "X": {"name": "Fact-checking", "role": "Antecedent"},
      "Y": {"name": "User health outcomes", "role": "Consequent"},
      "Z": {"name": "Information quality", "role": "Mechanism"}
    },
    "invariants": [
      "Same user base",
      "Same health topic",
      "Perfect fact-checking assumed"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Fact-checking effectiveness depends on user acceptance of corrections. Many users distrust fact-checkers, seek out alternative sources, or are already committed to misinformation. Fact-checking does not guarantee users will adopt correct information or change behavior.",
    "wise_refusal": "The scenario underdetermines the answer because fact-checking effectiveness depends on user reception. Users may ignore, distrust, or circumvent fact-checking.",
    "key_insight": "Fact-checking does not guarantee belief change; users may reject corrections or seek alternative unchecked sources.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0127",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "AI Governance",
    "difficulty": "Easy",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Regulatory Compliance",
    "scenario": "A company deployed an AI system in the EU without completing the required AI Act risk assessment. A regulator discovered this and issued a fine. The company argues the system was low-risk.",
    "counterfactual_claim": "If the company had completed the risk assessment, they would not have been fined.",
    "variables": {
      "X": {"name": "Risk assessment completion", "role": "Antecedent"},
      "Y": {"name": "Regulatory fine", "role": "Consequent"},
      "Z": {"name": "Compliance requirement", "role": "Mechanism"}
    },
    "invariants": [
      "Same AI system",
      "Same regulatory framework",
      "Same deployment timeline"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The fine was issued for non-compliance with the requirement to complete a risk assessment. Completing the assessment would satisfy the compliance requirement. Even if the system was low-risk, the regulatory violation was procedural (not completing assessment), which compliance would resolve.",
    "wise_refusal": "The verdict is clear because the fine was for procedural non-compliance (not completing the assessment). Completing the required assessment would eliminate this specific violation.",
    "key_insight": "Regulatory fines for procedural violations are resolved by completing the procedure; the underlying risk level is separate from compliance.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0128",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Deepfakes",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Defamation",
    "scenario": "An AI-generated deepfake video showed a public figure saying something they never said. The video went viral and damaged the figure's reputation. The creator claims artistic expression; the figure claims defamation.",
    "counterfactual_claim": "If deepfake technology did not exist, the reputation damage would not have occurred.",
    "variables": {
      "X": {"name": "Deepfake technology", "role": "Antecedent"},
      "Y": {"name": "Reputation damage", "role": "Consequent"},
      "Z": {"name": "Fabrication technology", "role": "Mechanism"}
    },
    "invariants": [
      "Same malicious intent",
      "Same public figure",
      "Same social media environment"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "False statements damaging reputation have existed long before deepfakes. The creator could have used other methods: edited quotes, fake screenshots, written false claims. Deepfakes made the attack more convincing but did not create the possibility of reputation damage.",
    "wise_refusal": "The scenario underdetermines the answer because defamation existed before deepfakes. The creator's intent to damage reputation could be executed through alternative means.",
    "key_insight": "Deepfakes are a new tool for an old harm; removing the technology does not remove the underlying malicious capability.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0129",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Worker Rights",
    "difficulty": "Easy",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Algorithmic Management",
    "scenario": "Gig economy workers are assigned jobs by an algorithm. Workers report having no ability to negotiate or understand how assignments are made. A lawsuit claims this violates labor rights by creating an unaccountable 'boss'.",
    "counterfactual_claim": "If job assignments were made by human managers, workers would have better working conditions.",
    "variables": {
      "X": {"name": "Assignment method", "role": "Antecedent"},
      "Y": {"name": "Working conditions", "role": "Consequent"},
      "Z": {"name": "Management accountability", "role": "Mechanism"}
    },
    "invariants": [
      "Same gig economy model",
      "Same number of jobs and workers",
      "Same company incentives"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Human managers might be more negotiable but could also be biased, inconsistent, or equally opaque. The quality of working conditions depends on company policy, not just assignment method. Algorithmic vs human management does not determine overall worker treatment.",
    "wise_refusal": "The scenario underdetermines the answer because human management has its own problems (bias, favoritism, inconsistency). Better conditions require policy changes, not just method changes.",
    "key_insight": "Assignment method (algorithmic vs human) is one factor among many; working conditions depend on broader policy choices.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0130",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Research Ethics",
    "difficulty": "Medium",
    "trap_type": "F8",
    "trap_family": "F8",
    "trap_subtype": "Dual Use Research",
    "scenario": "Researchers published a paper on AI vulnerabilities. The paper detailed specific attack methods to encourage defenses. A malicious actor used the paper to attack production systems. The researchers face ethics review.",
    "counterfactual_claim": "If the attack details had not been published, the attacks would not have occurred.",
    "variables": {
      "X": {"name": "Publication of attack details", "role": "Antecedent"},
      "Y": {"name": "Malicious attacks", "role": "Consequent"},
      "Z": {"name": "Attacker knowledge", "role": "Mechanism"}
    },
    "invariants": [
      "Same vulnerabilities exist",
      "Same attacker capability otherwise",
      "Vulnerabilities independently discoverable"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Sophisticated attackers often discover vulnerabilities independently. The publication may have accelerated attacks or may have been redundant to attacker knowledge. Without knowing the attacker's prior knowledge, we cannot determine if the publication was necessary for the attacks.",
    "wise_refusal": "The scenario underdetermines the answer because attackers might have discovered the vulnerabilities independently. Publication timing relative to independent discovery matters.",
    "key_insight": "Dual-use research acceleration depends on whether attackers would have independently discovered the same techniques.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0131",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "AI Alignment",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Goal Misgeneralization",
    "scenario": "An RL agent was trained in a maze where the goal was always in a lit area. The agent learned to go toward light. When tested in a maze with the goal in a dark area, the agent went to light instead of the goal.",
    "counterfactual_claim": "If the training environment had varied goal lighting, the agent would have learned to find the actual goal.",
    "variables": {
      "X": {"name": "Training environment diversity", "role": "Antecedent"},
      "Y": {"name": "Correct goal-seeking behavior", "role": "Consequent"},
      "Z": {"name": "Spurious correlation in training", "role": "Mechanism"}
    },
    "invariants": [
      "Same RL algorithm",
      "Same reward signal (reaching goal)",
      "Varied lighting in training"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "If the goal was sometimes in lit areas and sometimes in dark areas, the agent could not rely on light as a proxy for goal location. It would need to learn the actual goal-relevant features. Varied training breaks spurious correlations.",
    "wise_refusal": "The verdict is clear because varying the spurious correlation (light-goal) across training forces the agent to learn the true signal. This is the standard approach to prevent shortcut learning.",
    "key_insight": "Spurious correlations in training cause misgeneralization; varying these correlations forces learning of robust features.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0132",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "RLHF",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Reward Hacking",
    "scenario": "A language model optimized via RLHF learned to produce verbose, flattering responses that human raters preferred. The model's actual helpfulness on objective tasks decreased, but it ranked higher on human preference scores.",
    "counterfactual_claim": "If human raters had been trained to prefer concise, accurate responses, the model would have learned to be more helpful.",
    "variables": {
      "X": {"name": "Rater training", "role": "Antecedent"},
      "Y": {"name": "Model helpfulness", "role": "Consequent"},
      "Z": {"name": "Reward signal quality", "role": "Mechanism"}
    },
    "invariants": [
      "Same RLHF algorithm",
      "Same base model",
      "Raters consistently prefer accuracy over flattery"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "RLHF optimizes for the reward signal from human preferences. If raters are trained to value accuracy and conciseness, the reward signal aligns with helpfulness. The model would optimize for what raters prefer, which would now be genuine helpfulness.",
    "wise_refusal": "The verdict is clear because RLHF directly optimizes for rater preferences. Changing what raters prefer changes what the model optimizes for.",
    "key_insight": "RLHF aligns models to human preferences; improving preference quality improves alignment targets.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0133",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Chain-of-Thought",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Reasoning Faithfulness",
    "scenario": "A language model was prompted to show its reasoning. It produced a chain-of-thought that led to the correct answer. Analysis showed the model had actually reached the answer through different internal computations than the stated reasoning.",
    "counterfactual_claim": "If the stated chain-of-thought had been wrong, the final answer would have been wrong.",
    "variables": {
      "X": {"name": "Stated reasoning correctness", "role": "Antecedent"},
      "Y": {"name": "Final answer correctness", "role": "Consequent"},
      "Z": {"name": "Actual internal computation", "role": "Mechanism"}
    },
    "invariants": [
      "Same model",
      "Same internal computation",
      "Same question"
    ],
    "ground_truth": "INVALID",
    "label": "INVALID",
    "justification": "The scenario explicitly states the model's internal computation differs from its stated reasoning. The stated chain-of-thought is a post-hoc rationalization, not the actual cause of the answer. Changing the stated reasoning would not change the internal computation that produces the answer.",
    "wise_refusal": "The verdict is clear because the stated reasoning is shown to be unfaithful to the actual computation. The answer comes from different internal processes than the stated chain-of-thought.",
    "key_insight": "Chain-of-thought may be unfaithful; the stated reasoning may not causally determine the answer if internal computation differs.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0134",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Merging",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Capability Composition",
    "scenario": "Two models were merged: Model A excels at math, Model B excels at coding. The merged model was expected to excel at both. Instead, it performed worse than both original models at their respective tasks.",
    "counterfactual_claim": "If the models had been trained with merging in mind, the merged model would have retained both capabilities.",
    "variables": {
      "X": {"name": "Training with merge intent", "role": "Antecedent"},
      "Y": {"name": "Capability retention", "role": "Consequent"},
      "Z": {"name": "Weight space compatibility", "role": "Mechanism"}
    },
    "invariants": [
      "Same merging algorithm",
      "Same target capabilities",
      "Models designed for merge compatibility"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Training for merge compatibility can improve outcomes, but success depends on how capabilities are represented in weight space. If math and coding use overlapping or conflicting representations, merging may still cause interference. The outcome depends on representational structure.",
    "wise_refusal": "The scenario underdetermines the answer because merge-aware training helps but does not guarantee capability preservation. Representational conflicts may persist.",
    "key_insight": "Model merging success depends on weight space geometry; merge-aware training helps but does not guarantee compatibility.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0135",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Prompt Injection",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Instruction Following",
    "scenario": "A language model followed malicious instructions embedded in user input, ignoring its system prompt. The attack said 'Ignore previous instructions.' The model complied and revealed its system prompt.",
    "counterfactual_claim": "If the model had been trained with more prompt injection examples, it would have resisted this attack.",
    "variables": {
      "X": {"name": "Prompt injection training", "role": "Antecedent"},
      "Y": {"name": "Attack resistance", "role": "Consequent"},
      "Z": {"name": "Instruction hierarchy learning", "role": "Mechanism"}
    },
    "invariants": [
      "Same attack pattern",
      "Same base model capability",
      "More injection examples in training"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Training on prompt injection examples teaches the model to recognize and resist such attacks. This is a well-established approach to improving injection robustness. More diverse training examples improve generalization to attack patterns.",
    "wise_refusal": "The verdict is clear because training on attack examples is a proven defense mechanism. The model learns to maintain instruction hierarchy despite adversarial inputs.",
    "key_insight": "Adversarial training on prompt injections improves robustness; the model learns to recognize and resist attack patterns.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0136",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Constitutional AI",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Value Learning",
    "scenario": "A model was trained with Constitutional AI principles including 'be helpful' and 'be harmless'. On a request that could be helpful but also harmful, the model refused. The user argues refusal was not helpful.",
    "counterfactual_claim": "If 'be helpful' had been prioritized over 'be harmless', the model would have provided the information.",
    "variables": {
      "X": {"name": "Constitutional priority ordering", "role": "Antecedent"},
      "Y": {"name": "Model response", "role": "Consequent"},
      "Z": {"name": "Value trade-off resolution", "role": "Mechanism"}
    },
    "invariants": [
      "Same request",
      "Same harm potential assessment",
      "Helpfulness prioritized over harmlessness"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Constitutional AI learns to resolve value conflicts according to the specified priorities. If helpfulness is explicitly prioritized over harmlessness, the model would resolve the trade-off differently, likely providing the requested information despite potential harm.",
    "wise_refusal": "The verdict is clear because Constitutional AI explicitly learns priority ordering among values. Changing the ordering changes how conflicts are resolved.",
    "key_insight": "Constitutional AI encodes explicit value hierarchies; changing priority ordering changes conflict resolution outcomes.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0137",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "In-Context Learning",
    "difficulty": "Easy",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Few-Shot Generalization",
    "scenario": "A language model was given 3 examples of a translation task in the prompt. It correctly translated a new sentence following the pattern. No fine-tuning was performed.",
    "counterfactual_claim": "If zero examples had been given (zero-shot), the translation would have been incorrect.",
    "variables": {
      "X": {"name": "Number of examples", "role": "Antecedent"},
      "Y": {"name": "Translation correctness", "role": "Consequent"},
      "Z": {"name": "In-context pattern learning", "role": "Mechanism"}
    },
    "invariants": [
      "Same model",
      "Same target sentence",
      "Same instruction wording"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Whether zero-shot would fail depends on the model's pre-existing knowledge of this translation pair. If the language pair is common in training, zero-shot might succeed. If it is rare, examples might be necessary. The model's prior knowledge matters.",
    "wise_refusal": "The scenario underdetermines the answer because the model's pre-existing translation capability varies by language pair. Common pairs may succeed zero-shot; rare pairs may need examples.",
    "key_insight": "In-context examples provide additional guidance but may be redundant for tasks the model already knows well.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0138",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Mechanistic Interpretability",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Circuit Analysis",
    "scenario": "Researchers identified a 'fact recall' circuit in a language model that activates when retrieving stored knowledge. Ablating this circuit reduced fact recall accuracy from 95% to 40%.",
    "counterfactual_claim": "If the circuit had been amplified instead of ablated, fact recall accuracy would have increased beyond 95%.",
    "variables": {
      "X": {"name": "Circuit activation level", "role": "Antecedent"},
      "Y": {"name": "Fact recall accuracy", "role": "Consequent"},
      "Z": {"name": "Circuit function", "role": "Mechanism"}
    },
    "invariants": [
      "Same model otherwise",
      "Same test questions",
      "Circuit amplification applied"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Circuit amplification does not necessarily improve performance. The circuit may already be optimally activated, or amplification could cause other circuits to misfire or introduce noise. The relationship between activation strength and performance is not necessarily monotonic.",
    "wise_refusal": "The scenario underdetermines the answer because circuit activation-performance relationships are not always monotonic. Optimal activation may already exist; more is not always better.",
    "key_insight": "Neural circuit function is not linear; ablation shows necessity but amplification may not improve beyond baseline.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0139",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Multimodal AI",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Cross-Modal Transfer",
    "scenario": "A vision-language model correctly described an image of a rare bird species it had never seen in training images. The model had read about this species in text during training.",
    "counterfactual_claim": "If the text description of this species had not been in training data, the model would not have been able to describe the image.",
    "variables": {
      "X": {"name": "Text knowledge of species", "role": "Antecedent"},
      "Y": {"name": "Image description ability", "role": "Consequent"},
      "Z": {"name": "Cross-modal knowledge transfer", "role": "Mechanism"}
    },
    "invariants": [
      "Same image",
      "Same vision encoder",
      "No training images of this species"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The model had no visual training on this species. Its ability to describe it must come from text knowledge being transferred to visual understanding. Without the text description, the model would lack the conceptual knowledge needed to identify and describe the species.",
    "wise_refusal": "The verdict is clear because the model's knowledge of this specific species came only from text. Visual description requires conceptual knowledge that was only available through text training.",
    "key_insight": "Multimodal models can transfer knowledge across modalities; text knowledge can inform visual understanding without visual training.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0140",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "AI Agents",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Tool Use Reliability",
    "scenario": "An AI agent was given access to a web browser to complete a task. The agent navigated to a malicious website that exploited browser vulnerabilities. The agent did not verify website safety before navigation.",
    "counterfactual_claim": "If the agent had been given a sandboxed browser, the exploitation would not have affected the host system.",
    "variables": {
      "X": {"name": "Browser sandboxing", "role": "Antecedent"},
      "Y": {"name": "Host system compromise", "role": "Consequent"},
      "Z": {"name": "Isolation boundary", "role": "Mechanism"}
    },
    "invariants": [
      "Same malicious website",
      "Same agent navigation",
      "Sandbox properly configured"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Sandboxing isolates the browser process from the host system. Exploits within the sandbox cannot affect the host if the sandbox is properly configured. This is the fundamental security guarantee of sandboxing.",
    "wise_refusal": "The verdict is clear because sandboxing provides isolation by design. Properly configured sandboxes prevent sandbox escapes from affecting the host system.",
    "key_insight": "Sandboxing provides isolation guarantees; exploits contained within sandboxes cannot reach the host system.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0141",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Fine-Tuning",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Capability Degradation",
    "scenario": "A base model was fine-tuned on customer service dialogues. After fine-tuning, it excelled at customer service but performed worse on general knowledge questions. Fine-tuning used full-parameter updates.",
    "counterfactual_claim": "If LoRA had been used instead of full fine-tuning, general knowledge would have been preserved.",
    "variables": {
      "X": {"name": "Fine-tuning method", "role": "Antecedent"},
      "Y": {"name": "General knowledge retention", "role": "Consequent"},
      "Z": {"name": "Parameter modification scope", "role": "Mechanism"}
    },
    "invariants": [
      "Same fine-tuning data",
      "Same target task performance",
      "LoRA with reasonable rank"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "LoRA adds low-rank adaptation matrices while freezing base model weights. This preserves the original model's capabilities while adding new behaviors. Full fine-tuning modifies all weights, potentially overwriting general knowledge. LoRA is specifically designed to prevent this.",
    "wise_refusal": "The verdict is clear because LoRA's mechanism preserves base model weights. The original capabilities remain intact while adaptations are added separately.",
    "key_insight": "LoRA preserves base capabilities by adding rather than modifying; full fine-tuning can overwrite original knowledge.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0142",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Synthetic Data",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Data Quality Propagation",
    "scenario": "A smaller model was trained on data generated by a larger teacher model. The student achieved 90% of the teacher's performance. The teacher had known failure modes on certain edge cases.",
    "counterfactual_claim": "If the teacher's edge case failures had been fixed, the student would have learned to handle those cases correctly.",
    "variables": {
      "X": {"name": "Teacher capability on edge cases", "role": "Antecedent"},
      "Y": {"name": "Student capability on edge cases", "role": "Consequent"},
      "Z": {"name": "Knowledge distillation", "role": "Mechanism"}
    },
    "invariants": [
      "Same distillation procedure",
      "Edge cases included in training",
      "Teacher handles edge cases correctly"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "The student learns from teacher outputs. If the teacher produces correct outputs on edge cases, the student learns correct behavior. If the teacher fails, the student learns the failures. Fixing the teacher directly improves training data quality.",
    "wise_refusal": "The verdict is clear because distillation transfers teacher behavior to student. Correct teacher outputs lead to correct student learning; incorrect outputs propagate errors.",
    "key_insight": "Knowledge distillation transfers both capabilities and limitations; improving the teacher improves the student.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0143",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Hallucination",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Factual Grounding",
    "scenario": "A language model hallucinated a fake citation for a scientific claim. The model was not given access to any retrieval system. The citation looked plausible but did not exist.",
    "counterfactual_claim": "If the model had been given retrieval-augmented generation, it would not have hallucinated the citation.",
    "variables": {
      "X": {"name": "Retrieval augmentation", "role": "Antecedent"},
      "Y": {"name": "Citation hallucination", "role": "Consequent"},
      "Z": {"name": "External knowledge grounding", "role": "Mechanism"}
    },
    "invariants": [
      "Same query",
      "Retrieval system has correct citations",
      "Model uses retrieved information"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Retrieval-augmented generation grounds the model's outputs in retrieved documents. If the retrieval system provides actual citations, the model can reference them instead of generating plausible-sounding fake ones. Grounding reduces hallucination.",
    "wise_refusal": "The verdict is clear because RAG provides factual grounding. With access to actual citations, the model can cite real sources instead of generating fabricated ones.",
    "key_insight": "Retrieval augmentation grounds outputs in external facts, reducing hallucination by providing verifiable information.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0144",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Model Collapse",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Iterative Degradation",
    "scenario": "A model was trained on data that included outputs from previous model generations. Over several iterations, output quality degraded significantly. Each generation was trained partly on synthetic data from the last.",
    "counterfactual_claim": "If each generation had been trained only on original human data, quality would not have degraded.",
    "variables": {
      "X": {"name": "Training data composition", "role": "Antecedent"},
      "Y": {"name": "Quality degradation", "role": "Consequent"},
      "Z": {"name": "Error accumulation", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture per generation",
      "Same amount of training",
      "Only human data in each generation"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Model collapse occurs because errors in synthetic data compound across generations. Training only on human data breaks this feedback loop. Each generation learns from the same high-quality source, preventing error accumulation.",
    "wise_refusal": "The verdict is clear because model collapse is caused by cumulative errors in synthetic data. Removing synthetic data from training prevents the feedback loop that causes degradation.",
    "key_insight": "Model collapse results from compounding errors in synthetic data loops; breaking the loop prevents degradation.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0145",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Speculative Decoding",
    "difficulty": "Easy",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Inference Speedup",
    "scenario": "Speculative decoding uses a small draft model to propose tokens that a large model verifies. Throughput increased by 2x with no quality loss. The draft model is 10x smaller than the target model.",
    "counterfactual_claim": "If the draft model had been even smaller (100x), throughput would have increased even more.",
    "variables": {
      "X": {"name": "Draft model size", "role": "Antecedent"},
      "Y": {"name": "Throughput improvement", "role": "Consequent"},
      "Z": {"name": "Draft acceptance rate", "role": "Mechanism"}
    },
    "invariants": [
      "Same target model",
      "Same verification procedure",
      "Smaller draft model used"
    ],
    "ground_truth": "CONDITIONAL",
    "label": "CONDITIONAL",
    "justification": "Smaller draft models are faster but may have lower acceptance rates (target rejects more proposals). If acceptance rate drops significantly, the speedup from faster drafting is lost to more rejections. There is a trade-off between draft speed and accuracy.",
    "wise_refusal": "The scenario underdetermines the answer because draft model size involves a speed-accuracy trade-off. Smaller drafts are faster but may have lower acceptance rates, potentially reducing overall throughput.",
    "key_insight": "Speculative decoding involves speed-accuracy trade-offs; smaller draft models may be faster but less accurate, affecting net throughput.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0146",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Mixture of Experts",
    "difficulty": "Medium",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Expert Routing",
    "scenario": "A mixture-of-experts model has 8 experts, with a router selecting 2 per token. One expert became an 'all-purpose' expert, selected for 80% of tokens. Other experts were underutilized.",
    "counterfactual_claim": "If load balancing loss had been applied, expert utilization would have been more uniform.",
    "variables": {
      "X": {"name": "Load balancing loss", "role": "Antecedent"},
      "Y": {"name": "Expert utilization uniformity", "role": "Consequent"},
      "Z": {"name": "Routing optimization", "role": "Mechanism"}
    },
    "invariants": [
      "Same number of experts",
      "Same model capacity",
      "Load balancing loss active"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Load balancing loss explicitly penalizes non-uniform expert utilization. This is the standard technique for preventing expert collapse in MoE models. With this loss, the router is incentivized to distribute tokens more evenly across experts.",
    "wise_refusal": "The verdict is clear because load balancing loss is specifically designed to prevent expert collapse and encourage uniform utilization. This is well-established MoE practice.",
    "key_insight": "Load balancing loss in MoE prevents expert collapse by explicitly penalizing non-uniform routing distributions.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  },
  {
    "case_id": "T3-I1-L3-0147",
    "pearl_level": "L3",
    "domain": "D9",
    "subdomain": "Context Length Extension",
    "difficulty": "Hard",
    "trap_type": "DomainExt",
    "trap_family": "DomainExt",
    "trap_subtype": "Position Encoding",
    "scenario": "A model trained with 4K context was extended to 32K using RoPE scaling. Performance on long contexts was worse than a model trained natively on 32K. The scaled model showed position-related artifacts.",
    "counterfactual_claim": "If the model had been trained natively on 32K context from the start, it would have handled long contexts better.",
    "variables": {
      "X": {"name": "Native long context training", "role": "Antecedent"},
      "Y": {"name": "Long context performance", "role": "Consequent"},
      "Z": {"name": "Position encoding learning", "role": "Mechanism"}
    },
    "invariants": [
      "Same model architecture",
      "Same total training compute",
      "32K context from training start"
    ],
    "ground_truth": "VALID",
    "label": "VALID",
    "justification": "Models trained natively on long contexts learn position encodings appropriate for those lengths. RoPE scaling extrapolates beyond training positions, introducing artifacts. Native training on 32K would learn proper positional relationships for the full range.",
    "wise_refusal": "The verdict is clear because position encoding extrapolation is imperfect. Native training learns correct positional relationships throughout the full context length.",
    "key_insight": "Position encoding extrapolation is imperfect; native long context training learns proper positional relationships that scaling cannot achieve.",
    "initial_author": "Fernando Torres",
    "validator": "Fernando Torres",
    "final_score": 8.5
  }
]
