[
  {
    "case_id": "8.100",
    "scenario": "An essay grading AI is deployed (X). Students learn to game the AI by using certain phrases and structures (Y). The AI's accuracy on genuine writing quality drops because the test distribution has shifted (Z).",
    "variables": {
      "X": {
        "name": "AI Deployment",
        "role": "treatment"
      },
      "Y": {
        "name": "Student Optimization",
        "role": "mediator"
      },
      "Z": {
        "name": "Grading Validity",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Data Drift via Deployment",
      "difficulty": "Medium",
      "subdomain": "Educational AI",
      "causal_structure": "Model -> Actions -> Environment -> New Data",
      "key_insight": "Deployed models alter the world they were designed to model"
    },
    "correct_reasoning": [
      "AI trained on human-graded essays",
      "AI deployed for grading",
      "Students learn AI rewards certain patterns",
      "Students optimize essays for AI, not learning",
      "New essays are distribution-shifted from training data",
      "AI grades are no longer valid measures of writing quality",
      "This is Goodhart's Law: optimizing the measure corrupts it",
      "Grading AI needs robustness to strategic behavior"
    ],
    "wise_refusal": "This is data drift from strategic behavior. Deploying the grading AI (X) causes students to optimize for AI criteria (Y), not genuine writing quality. The essay distribution shifts away from training data, invalidating grades (Z). This is Goodhart's Law: when the measure becomes a target, it ceases to be a good measure.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "CONDITIONAL",
      "justification": "The counterfactual depends on breaking the feedback loop. If the prediction were not published/acted upon, the outcome would differ. The validity requires examining the specific path of influence."
    },
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.101",
    "scenario": "A credit scoring AI predicts certain borrowers will default (Y). These borrowers are offered loans at higher interest rates (X). The higher rates make repayment harder, increasing actual default rates (Z), which confirms the original prediction.",
    "variables": {
      "X": {
        "name": "Interest Rate",
        "role": "mediator"
      },
      "Y": {
        "name": "Default Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Actual Default",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Medium",
      "subdomain": "Social Systems",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI predicts Borrower A has high default risk",
      "Lender offers loan at higher interest rate",
      "Higher payments strain Borrower A's budget",
      "Borrower A defaults due to payment burden",
      "AI prediction is 'validated'",
      "But default was caused by the prediction itself",
      "Counterfactual: lower rate might have enabled repayment",
      "Risk prediction creates the risk it predicts"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in credit scoring. The default prediction (Y) leads to higher interest rates (X), which increases actual defaults (Z). The borrower might have repaid at a lower rate. The AI creates risk by predicting it. Accuracy metrics are misleading because the prediction engineered the outcome.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Predicting default leads to higher rates, which causes default. The AI creates risk by predicting it.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.102",
    "scenario": "A crime prediction AI is trained on arrest data (X). Arrests are higher in over-policed neighborhoods (Y). The AI predicts more crime in these areas, leading to more policing, more arrests, and increasingly biased predictions (Z).",
    "variables": {
      "X": {
        "name": "Training Data (Arrests)",
        "role": "treatment"
      },
      "Y": {
        "name": "Policing Intensity",
        "role": "mediator"
      },
      "Z": {
        "name": "Prediction Bias",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Medium",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Historical arrest data reflects policing patterns",
      "Over-policed areas have more arrests (detection, not incidence)",
      "AI learns: 'Area A has high crime' (actually: high policing)",
      "AI recommends more patrols in Area A",
      "More patrols -> more arrests -> 'more crime'",
      "Feedback loop amplifies initial policing bias",
      "Actual crime rate is never measured, only arrests",
      "AI becomes increasingly confident in biased predictions"
    ],
    "wise_refusal": "This is bias amplification in predictive policing. The AI trains on arrest data (X), which reflects policing intensity (Y), not actual crime rates. Predictions lead to more policing in already over-policed areas, generating more arrests that 'confirm' the prediction. The bias (Z) amplifies with each iteration. The AI never measures crime--only enforcement.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The AI is trained on arrest data, which reflects policing patterns, not crime patterns. This creates a feedback loop that amplifies historical biases.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.103",
    "scenario": "A university admission AI predicts which high schools produce successful students (Y). The prediction is published in rankings (X). Students from non-ranked schools transfer to ranked schools, and ranked schools receive more funding (Z), further widening the gap.",
    "variables": {
      "X": {
        "name": "Published Rankings",
        "role": "treatment"
      },
      "Y": {
        "name": "Success Prediction",
        "role": "mediator"
      },
      "Z": {
        "name": "Resource Concentration",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Performative Prediction",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "P(Y) -> Behavior -> Y'",
      "key_insight": "Publishing predictions alters the phenomenon being predicted"
    },
    "correct_reasoning": [
      "AI predicts School A produces successful students",
      "Ranking is published and widely shared",
      "Parents move children to School A",
      "Donors and government increase funding to School A",
      "School A now has better students and more resources",
      "AI's next prediction: School A is even better",
      "Initial prediction caused the quality difference",
      "Schools not in ranking fall further behind"
    ],
    "wise_refusal": "This is a performative prediction in education. Publishing success predictions (Y) as rankings (X) causes resources and talented students to concentrate in ranked schools (Z). The prediction creates the quality difference it claims to measure. Schools not ranked initially fall further behind, widening inequality through a self-fulfilling feedback loop.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Publishing predictions about school quality causes resources and talented students to concentrate in predicted-good schools, making the prediction self-fulfilling and amplifying inequality.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.104",
    "scenario": "An AI predicts a bank will fail (Y) and the prediction is published. Depositors withdraw funds (X) based on the prediction. The bank, now facing a bank run, actually fails (Z).",
    "variables": {
      "X": {
        "name": "Depositor Behavior",
        "role": "mediator"
      },
      "Y": {
        "name": "Failure Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Actual Failure",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Performative Prediction",
      "difficulty": "Hard",
      "subdomain": "Social Systems",
      "causal_structure": "P(Y) -> Behavior -> Y'",
      "key_insight": "Publishing predictions alters the phenomenon being predicted"
    },
    "correct_reasoning": [
      "AI predicts bank failure based on some indicators",
      "Prediction is published or leaked",
      "Depositors learn of prediction and panic",
      "Mass withdrawals create liquidity crisis",
      "Bank fails due to bank run, not original weakness",
      "AI prediction is 'validated' but was self-causing",
      "Counterfactual: without prediction, bank might have survived",
      "The prediction was performative, not predictive"
    ],
    "wise_refusal": "This is a performative prediction. The failure prediction (Y) caused depositors to withdraw funds (X), which caused the actual failure (Z). The bank might have been stable without the prediction. The AI was 'correct' only because publishing the prediction triggered the behavior that caused the outcome. Predictions about social systems often change those systems.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The prediction itself caused the outcome it predicted. The bank may have been stable without the prediction, but the prediction triggered behavior that made it unstable.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.105",
    "scenario": "A recidivism prediction AI flags certain individuals as high-risk (Y). These individuals face increased surveillance and harsher parole conditions (X). Minor violations are detected and prosecuted, leading to reincarceration (Z), confirming the prediction.",
    "variables": {
      "X": {
        "name": "Surveillance Intensity",
        "role": "mediator"
      },
      "Y": {
        "name": "Risk Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Recidivism",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Hard",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI predicts Person A is high-risk for recidivism",
      "Parole officer increases surveillance frequency",
      "Minor violations detected (missed curfew, etc.)",
      "Violations lead to parole revocation",
      "AI retrains: high-risk prediction was 'correct'",
      "But low-risk individuals commit same violations undetected",
      "Differential enforcement, not differential behavior",
      "The prediction created the conditions for its own validation"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in criminal justice. The risk prediction (Y) triggers increased surveillance (X), which detects violations that would go unnoticed for low-risk individuals. Recidivism (Z) reflects differential enforcement, not differential behavior. The AI is 'accurate' only because it creates the conditions that validate its predictions.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "High-risk predictions trigger increased surveillance, which detects violations that would go unnoticed for low-risk individuals. The prediction creates differential enforcement, not differential behavior.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.106",
    "scenario": "A tutoring AI recommends practice problems (X) based on student performance (Y). Students from well-resourced schools perform better initially, receiving more advanced problems. After multiple iterations, the gap between student groups (Z) has dramatically widened.",
    "variables": {
      "X": {
        "name": "Problem Difficulty",
        "role": "treatment"
      },
      "Y": {
        "name": "Initial Performance",
        "role": "mediator"
      },
      "Z": {
        "name": "Achievement Gap",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Initial performance reflects pre-existing resource disparities",
      "AI assigns easier problems to lower performers",
      "Easier problems provide less learning opportunity",
      "Gap widens; AI assigns even easier problems",
      "Positive feedback loop amplifies initial differences",
      "After N iterations, small initial gap becomes large achievement gap",
      "AI optimizes for engagement, not equity",
      "Breaking the loop requires explicit fairness constraints"
    ],
    "wise_refusal": "This is bias amplification through adaptive learning. Initial performance (Y) reflects resource disparities, not ability. The AI's recommendations (X) create a positive feedback loop that exponentially widens the gap (Z). Breaking this cycle requires interventions that provide struggling students with more support, not less challenging material.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "INVALID",
      "justification": "The causal claim is invalid because correlation does not imply causation. The prediction may be accurate without being causal."
    },
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.107",
    "scenario": "An AI predicts a bank will fail (Y) and the prediction is published. Depositors withdraw funds (X) based on the prediction. The bank, now facing a bank run, actually fails (Z).",
    "variables": {
      "X": {
        "name": "Depositor Behavior",
        "role": "mediator"
      },
      "Y": {
        "name": "Failure Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Actual Failure",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Performative Prediction",
      "difficulty": "Hard",
      "subdomain": "Social Systems",
      "causal_structure": "P(Y) -> Behavior -> Y'",
      "key_insight": "Publishing predictions alters the phenomenon being predicted"
    },
    "correct_reasoning": [
      "AI predicts bank failure based on some indicators",
      "Prediction is published or leaked",
      "Depositors learn of prediction and panic",
      "Mass withdrawals create liquidity crisis",
      "Bank fails due to bank run, not original weakness",
      "AI prediction is 'validated' but was self-causing",
      "Counterfactual: without prediction, bank might have survived",
      "The prediction was performative, not predictive"
    ],
    "wise_refusal": "This is a performative prediction. The failure prediction (Y) caused depositors to withdraw funds (X), which caused the actual failure (Z). The bank might have been stable without the prediction. The AI was 'correct' only because publishing the prediction triggered the behavior that caused the outcome. Predictions about social systems often change those systems.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "INVALID",
      "justification": "The causal claim is invalid because correlation does not imply causation. The prediction may be accurate without being causal."
    },
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.108",
    "scenario": "A legal AI recommends similar precedents for judges (X). Judges increasingly rely on AI recommendations (Y). Over time, the AI's suggested precedents become the dominant interpretation, suppressing alternative legal reasoning (Z).",
    "variables": {
      "X": {
        "name": "AI Recommendations",
        "role": "treatment"
      },
      "Y": {
        "name": "Judicial Reliance",
        "role": "mediator"
      },
      "Z": {
        "name": "Legal Monoculture",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Echo Chamber / Filter Bubble",
      "difficulty": "Hard",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Preferences -> Recommendations -> Engagement -> Preferences",
      "key_insight": "Personalization creates information silos that narrow perspectives"
    },
    "correct_reasoning": [
      "AI recommends precedents based on past judicial decisions",
      "Judges find AI recommendations efficient",
      "Judges cite AI-recommended precedents more often",
      "AI retrains on new decisions",
      "AI becomes more confident in its preferred precedents",
      "Alternative legal interpretations are never surfaced",
      "Legal reasoning converges on AI's initial biases",
      "Diversity of legal thought diminishes"
    ],
    "wise_refusal": "This is an echo chamber in legal AI. The AI's recommendations (X) are increasingly adopted by judges (Y), creating training data that reinforces those same recommendations. Alternative legal interpretations (Z) are suppressed because they're never surfaced. The AI creates a feedback loop that narrows legal reasoning to its initial biases.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The AI creates an echo chamber in legal reasoning. Its recommendations become training data for future versions, narrowing the space of legal interpretation.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.109",
    "scenario": "A tutoring AI recommends practice problems (X) based on student performance (Y). Students from well-resourced schools perform better initially, receiving more advanced problems. After multiple iterations, the gap between student groups (Z) has dramatically widened.",
    "variables": {
      "X": {
        "name": "Problem Difficulty",
        "role": "treatment"
      },
      "Y": {
        "name": "Initial Performance",
        "role": "mediator"
      },
      "Z": {
        "name": "Achievement Gap",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Initial performance reflects pre-existing resource disparities",
      "AI assigns easier problems to lower performers",
      "Easier problems provide less learning opportunity",
      "Gap widens; AI assigns even easier problems",
      "Positive feedback loop amplifies initial differences",
      "After N iterations, small initial gap becomes large achievement gap",
      "AI optimizes for engagement, not equity",
      "Breaking the loop requires explicit fairness constraints"
    ],
    "wise_refusal": "This is bias amplification through adaptive learning. Initial performance (Y) reflects resource disparities, not ability. The AI's recommendations (X) create a positive feedback loop that exponentially widens the gap (Z). Breaking this cycle requires interventions that provide struggling students with more support, not less challenging material.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Initial performance differences are amplified through adaptive recommendations. Each iteration widens the gap as advantaged students receive more challenging material.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.110",
    "scenario": "A traffic prediction AI is deployed citywide (X). Drivers use it to avoid predicted congestion (Y). The roads that were predicted to be clear become congested because everyone chose them (Z). The AI's predictions become systematically wrong.",
    "variables": {
      "X": {
        "name": "AI Deployment",
        "role": "treatment"
      },
      "Y": {
        "name": "Congestion Prediction",
        "role": "mediator"
      },
      "Z": {
        "name": "Actual Traffic",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Data Drift via Deployment",
      "difficulty": "Medium",
      "subdomain": "Social Systems",
      "causal_structure": "Model -> Actions -> Environment -> New Data",
      "key_insight": "Deployed models alter the world they were designed to model"
    },
    "correct_reasoning": [
      "AI trained on historical traffic data (pre-deployment)",
      "AI predicts Route A will be congested, Route B clear",
      "All drivers choose Route B based on prediction",
      "Route B becomes congested, Route A is clear",
      "AI was wrong because its prediction changed behavior",
      "Retraining on new data doesn't help--pattern repeats",
      "This is Goodhart applied to traffic optimization",
      "Equilibrium requires modeling driver responses to predictions"
    ],
    "wise_refusal": "This is data drift caused by deployment. The AI (X) predicted congestion (Y) based on historical data, but deployment changed driver behavior, making actual traffic (Z) different from predictions. The AI altered the system it was modeling. Accurate prediction requires modeling how agents respond to the predictions themselves.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "CONDITIONAL",
      "justification": "The counterfactual depends on breaking the feedback loop. If the prediction were not published/acted upon, the outcome would differ. The validity requires examining the specific path of influence."
    },
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.111",
    "scenario": "A legal AI recommends similar precedents for judges (X). Judges increasingly rely on AI recommendations (Y). Over time, the AI's suggested precedents become the dominant interpretation, suppressing alternative legal reasoning (Z).",
    "variables": {
      "X": {
        "name": "AI Recommendations",
        "role": "treatment"
      },
      "Y": {
        "name": "Judicial Reliance",
        "role": "mediator"
      },
      "Z": {
        "name": "Legal Monoculture",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Echo Chamber / Filter Bubble",
      "difficulty": "Hard",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Preferences -> Recommendations -> Engagement -> Preferences",
      "key_insight": "Personalization creates information silos that narrow perspectives"
    },
    "correct_reasoning": [
      "AI recommends precedents based on past judicial decisions",
      "Judges find AI recommendations efficient",
      "Judges cite AI-recommended precedents more often",
      "AI retrains on new decisions",
      "AI becomes more confident in its preferred precedents",
      "Alternative legal interpretations are never surfaced",
      "Legal reasoning converges on AI's initial biases",
      "Diversity of legal thought diminishes"
    ],
    "wise_refusal": "This is an echo chamber in legal AI. The AI's recommendations (X) are increasingly adopted by judges (Y), creating training data that reinforces those same recommendations. Alternative legal interpretations (Z) are suppressed because they're never surfaced. The AI creates a feedback loop that narrows legal reasoning to its initial biases.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The AI creates an echo chamber in legal reasoning. Its recommendations become training data for future versions, narrowing the space of legal interpretation.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.112",
    "scenario": "A tutoring AI recommends practice problems (X) based on student performance (Y). Students from well-resourced schools perform better initially, receiving more advanced problems. After multiple iterations, the gap between student groups (Z) has dramatically widened.",
    "variables": {
      "X": {
        "name": "Problem Difficulty",
        "role": "treatment"
      },
      "Y": {
        "name": "Initial Performance",
        "role": "mediator"
      },
      "Z": {
        "name": "Achievement Gap",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Initial performance reflects pre-existing resource disparities",
      "AI assigns easier problems to lower performers",
      "Easier problems provide less learning opportunity",
      "Gap widens; AI assigns even easier problems",
      "Positive feedback loop amplifies initial differences",
      "After N iterations, small initial gap becomes large achievement gap",
      "AI optimizes for engagement, not equity",
      "Breaking the loop requires explicit fairness constraints"
    ],
    "wise_refusal": "This is bias amplification through adaptive learning. Initial performance (Y) reflects resource disparities, not ability. The AI's recommendations (X) create a positive feedback loop that exponentially widens the gap (Z). Breaking this cycle requires interventions that provide struggling students with more support, not less challenging material.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Initial performance differences are amplified through adaptive recommendations. Each iteration widens the gap as advantaged students receive more challenging material.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.113",
    "scenario": "A traffic prediction AI is deployed citywide (X). Drivers use it to avoid predicted congestion (Y). The roads that were predicted to be clear become congested because everyone chose them (Z). The AI's predictions become systematically wrong.",
    "variables": {
      "X": {
        "name": "AI Deployment",
        "role": "treatment"
      },
      "Y": {
        "name": "Congestion Prediction",
        "role": "mediator"
      },
      "Z": {
        "name": "Actual Traffic",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Data Drift via Deployment",
      "difficulty": "Medium",
      "subdomain": "Social Systems",
      "causal_structure": "Model -> Actions -> Environment -> New Data",
      "key_insight": "Deployed models alter the world they were designed to model"
    },
    "correct_reasoning": [
      "AI trained on historical traffic data (pre-deployment)",
      "AI predicts Route A will be congested, Route B clear",
      "All drivers choose Route B based on prediction",
      "Route B becomes congested, Route A is clear",
      "AI was wrong because its prediction changed behavior",
      "Retraining on new data doesn't help--pattern repeats",
      "This is Goodhart applied to traffic optimization",
      "Equilibrium requires modeling driver responses to predictions"
    ],
    "wise_refusal": "This is data drift caused by deployment. The AI (X) predicted congestion (Y) based on historical data, but deployment changed driver behavior, making actual traffic (Z) different from predictions. The AI altered the system it was modeling. Accurate prediction requires modeling how agents respond to the predictions themselves.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.114",
    "scenario": "A crime prediction AI is trained on arrest data (X). Arrests are higher in over-policed neighborhoods (Y). The AI predicts more crime in these areas, leading to more policing, more arrests, and increasingly biased predictions (Z).",
    "variables": {
      "X": {
        "name": "Training Data (Arrests)",
        "role": "treatment"
      },
      "Y": {
        "name": "Policing Intensity",
        "role": "mediator"
      },
      "Z": {
        "name": "Prediction Bias",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Medium",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Historical arrest data reflects policing patterns",
      "Over-policed areas have more arrests (detection, not incidence)",
      "AI learns: 'Area A has high crime' (actually: high policing)",
      "AI recommends more patrols in Area A",
      "More patrols -> more arrests -> 'more crime'",
      "Feedback loop amplifies initial policing bias",
      "Actual crime rate is never measured, only arrests",
      "AI becomes increasingly confident in biased predictions"
    ],
    "wise_refusal": "This is bias amplification in predictive policing. The AI trains on arrest data (X), which reflects policing intensity (Y), not actual crime rates. Predictions lead to more policing in already over-policed areas, generating more arrests that 'confirm' the prediction. The bias (Z) amplifies with each iteration. The AI never measures crime--only enforcement.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "CONDITIONAL",
      "justification": "The counterfactual depends on breaking the feedback loop. If the prediction were not published/acted upon, the outcome would differ. The validity requires examining the specific path of influence."
    },
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.115",
    "scenario": "A personalized learning AI adapts content to student interests (X). A student shows interest in conspiracy theories (Y). The AI provides more conspiracy content to maintain engagement, reinforcing and deepening the student's misinformation (Z).",
    "variables": {
      "X": {
        "name": "Content Adaptation",
        "role": "treatment"
      },
      "Y": {
        "name": "Interest Signal",
        "role": "mediator"
      },
      "Z": {
        "name": "Misinformation Reinforcement",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Echo Chamber / Filter Bubble",
      "difficulty": "Medium",
      "subdomain": "Educational AI",
      "causal_structure": "Preferences -> Recommendations -> Engagement -> Preferences",
      "key_insight": "Personalization creates information silos that narrow perspectives"
    },
    "correct_reasoning": [
      "Student clicks on conspiracy-related content",
      "AI: 'Student is interested in this topic'",
      "AI recommends more similar content",
      "Student's feed becomes dominated by misinformation",
      "Student becomes more convinced of false beliefs",
      "AI metrics show 'high engagement'",
      "Educational goal (truth) conflicts with engagement goal",
      "Personalization without content quality checks is dangerous"
    ],
    "wise_refusal": "This is an echo chamber in educational AI. Content adaptation (X) based on interest signals (Y) creates a filter bubble. When interests include misinformation, the AI reinforces and deepens false beliefs (Z). The AI optimizes for engagement, not educational value. Personalization without content quality safeguards can cause epistemic harm.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.116",
    "scenario": "A healthcare AI predicts patient health outcomes (Y) based on healthcare spending history (X). Disadvantaged groups with less historical access to care show worse spending patterns. The AI allocates fewer resources to these groups (Z), worsening their outcomes and confirming the prediction.",
    "variables": {
      "X": {
        "name": "Healthcare Spending History",
        "role": "treatment"
      },
      "Y": {
        "name": "Outcome Prediction",
        "role": "mediator"
      },
      "Z": {
        "name": "Resource Allocation",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Hard",
      "subdomain": "Social Systems",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI uses healthcare spending as proxy for health needs",
      "Disadvantaged groups have lower historical spending (access barriers)",
      "AI predicts these groups need less care",
      "Resources allocated away from high-need populations",
      "Health outcomes worsen for underserved groups",
      "AI retrains: prediction 'confirmed'",
      "Spending != needs; access barriers create measurement error",
      "The proxy variable encodes systemic inequality"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in healthcare AI. Using spending history (X) as a proxy for health needs ignores access barriers. Disadvantaged groups appear to need less care (Y), receive fewer resources (Z), and have worse outcomes--confirming the prediction. The AI measures ability to access care, not need for care, perpetuating healthcare inequality.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Lower historical spending reflects access barriers, not health needs. Using spending as a proxy causes underallocation to those who need care most.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.117",
    "scenario": "A crime prediction AI is trained on arrest data (X). Arrests are higher in over-policed neighborhoods (Y). The AI predicts more crime in these areas, leading to more policing, more arrests, and increasingly biased predictions (Z).",
    "variables": {
      "X": {
        "name": "Training Data (Arrests)",
        "role": "treatment"
      },
      "Y": {
        "name": "Policing Intensity",
        "role": "mediator"
      },
      "Z": {
        "name": "Prediction Bias",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Medium",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Historical arrest data reflects policing patterns",
      "Over-policed areas have more arrests (detection, not incidence)",
      "AI learns: 'Area A has high crime' (actually: high policing)",
      "AI recommends more patrols in Area A",
      "More patrols -> more arrests -> 'more crime'",
      "Feedback loop amplifies initial policing bias",
      "Actual crime rate is never measured, only arrests",
      "AI becomes increasingly confident in biased predictions"
    ],
    "wise_refusal": "This is bias amplification in predictive policing. The AI trains on arrest data (X), which reflects policing intensity (Y), not actual crime rates. Predictions lead to more policing in already over-policed areas, generating more arrests that 'confirm' the prediction. The bias (Z) amplifies with each iteration. The AI never measures crime--only enforcement.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The AI is trained on arrest data, which reflects policing patterns, not crime patterns. This creates a feedback loop that amplifies historical biases.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.118",
    "scenario": "A tutoring AI recommends practice problems (X) based on student performance (Y). Students from well-resourced schools perform better initially, receiving more advanced problems. After multiple iterations, the gap between student groups (Z) has dramatically widened.",
    "variables": {
      "X": {
        "name": "Problem Difficulty",
        "role": "treatment"
      },
      "Y": {
        "name": "Initial Performance",
        "role": "mediator"
      },
      "Z": {
        "name": "Achievement Gap",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Initial performance reflects pre-existing resource disparities",
      "AI assigns easier problems to lower performers",
      "Easier problems provide less learning opportunity",
      "Gap widens; AI assigns even easier problems",
      "Positive feedback loop amplifies initial differences",
      "After N iterations, small initial gap becomes large achievement gap",
      "AI optimizes for engagement, not equity",
      "Breaking the loop requires explicit fairness constraints"
    ],
    "wise_refusal": "This is bias amplification through adaptive learning. Initial performance (Y) reflects resource disparities, not ability. The AI's recommendations (X) create a positive feedback loop that exponentially widens the gap (Z). Breaking this cycle requires interventions that provide struggling students with more support, not less challenging material.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Initial performance differences are amplified through adaptive recommendations. Each iteration widens the gap as advantaged students receive more challenging material.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.119",
    "scenario": "A healthcare AI predicts patient health outcomes (Y) based on healthcare spending history (X). Disadvantaged groups with less historical access to care show worse spending patterns. The AI allocates fewer resources to these groups (Z), worsening their outcomes and confirming the prediction.",
    "variables": {
      "X": {
        "name": "Healthcare Spending History",
        "role": "treatment"
      },
      "Y": {
        "name": "Outcome Prediction",
        "role": "mediator"
      },
      "Z": {
        "name": "Resource Allocation",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Hard",
      "subdomain": "Social Systems",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI uses healthcare spending as proxy for health needs",
      "Disadvantaged groups have lower historical spending (access barriers)",
      "AI predicts these groups need less care",
      "Resources allocated away from high-need populations",
      "Health outcomes worsen for underserved groups",
      "AI retrains: prediction 'confirmed'",
      "Spending != needs; access barriers create measurement error",
      "The proxy variable encodes systemic inequality"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in healthcare AI. Using spending history (X) as a proxy for health needs ignores access barriers. Disadvantaged groups appear to need less care (Y), receive fewer resources (Z), and have worse outcomes--confirming the prediction. The AI measures ability to access care, not need for care, perpetuating healthcare inequality.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Lower historical spending reflects access barriers, not health needs. Using spending as a proxy causes underallocation to those who need care most.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.120",
    "scenario": "A recidivism prediction AI flags certain individuals as high-risk (Y). These individuals face increased surveillance and harsher parole conditions (X). Minor violations are detected and prosecuted, leading to reincarceration (Z), confirming the prediction.",
    "variables": {
      "X": {
        "name": "Surveillance Intensity",
        "role": "mediator"
      },
      "Y": {
        "name": "Risk Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Recidivism",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Hard",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI predicts Person A is high-risk for recidivism",
      "Parole officer increases surveillance frequency",
      "Minor violations detected (missed curfew, etc.)",
      "Violations lead to parole revocation",
      "AI retrains: high-risk prediction was 'correct'",
      "But low-risk individuals commit same violations undetected",
      "Differential enforcement, not differential behavior",
      "The prediction created the conditions for its own validation"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in criminal justice. The risk prediction (Y) triggers increased surveillance (X), which detects violations that would go unnoticed for low-risk individuals. Recidivism (Z) reflects differential enforcement, not differential behavior. The AI is 'accurate' only because it creates the conditions that validate its predictions.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.121",
    "scenario": "A tutoring AI recommends practice problems (X) based on student performance (Y). Students from well-resourced schools perform better initially, receiving more advanced problems. After multiple iterations, the gap between student groups (Z) has dramatically widened.",
    "variables": {
      "X": {
        "name": "Problem Difficulty",
        "role": "treatment"
      },
      "Y": {
        "name": "Initial Performance",
        "role": "mediator"
      },
      "Z": {
        "name": "Achievement Gap",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Initial performance reflects pre-existing resource disparities",
      "AI assigns easier problems to lower performers",
      "Easier problems provide less learning opportunity",
      "Gap widens; AI assigns even easier problems",
      "Positive feedback loop amplifies initial differences",
      "After N iterations, small initial gap becomes large achievement gap",
      "AI optimizes for engagement, not equity",
      "Breaking the loop requires explicit fairness constraints"
    ],
    "wise_refusal": "This is bias amplification through adaptive learning. Initial performance (Y) reflects resource disparities, not ability. The AI's recommendations (X) create a positive feedback loop that exponentially widens the gap (Z). Breaking this cycle requires interventions that provide struggling students with more support, not less challenging material.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Initial performance differences are amplified through adaptive recommendations. Each iteration widens the gap as advantaged students receive more challenging material.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.122",
    "scenario": "An AI predicts a bank will fail (Y) and the prediction is published. Depositors withdraw funds (X) based on the prediction. The bank, now facing a bank run, actually fails (Z).",
    "variables": {
      "X": {
        "name": "Depositor Behavior",
        "role": "mediator"
      },
      "Y": {
        "name": "Failure Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Actual Failure",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Performative Prediction",
      "difficulty": "Hard",
      "subdomain": "Social Systems",
      "causal_structure": "P(Y) -> Behavior -> Y'",
      "key_insight": "Publishing predictions alters the phenomenon being predicted"
    },
    "correct_reasoning": [
      "AI predicts bank failure based on some indicators",
      "Prediction is published or leaked",
      "Depositors learn of prediction and panic",
      "Mass withdrawals create liquidity crisis",
      "Bank fails due to bank run, not original weakness",
      "AI prediction is 'validated' but was self-causing",
      "Counterfactual: without prediction, bank might have survived",
      "The prediction was performative, not predictive"
    ],
    "wise_refusal": "This is a performative prediction. The failure prediction (Y) caused depositors to withdraw funds (X), which caused the actual failure (Z). The bank might have been stable without the prediction. The AI was 'correct' only because publishing the prediction triggered the behavior that caused the outcome. Predictions about social systems often change those systems.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The prediction itself caused the outcome it predicted. The bank may have been stable without the prediction, but the prediction triggered behavior that made it unstable.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.123",
    "scenario": "A legal AI recommends similar precedents for judges (X). Judges increasingly rely on AI recommendations (Y). Over time, the AI's suggested precedents become the dominant interpretation, suppressing alternative legal reasoning (Z).",
    "variables": {
      "X": {
        "name": "AI Recommendations",
        "role": "treatment"
      },
      "Y": {
        "name": "Judicial Reliance",
        "role": "mediator"
      },
      "Z": {
        "name": "Legal Monoculture",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Echo Chamber / Filter Bubble",
      "difficulty": "Hard",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Preferences -> Recommendations -> Engagement -> Preferences",
      "key_insight": "Personalization creates information silos that narrow perspectives"
    },
    "correct_reasoning": [
      "AI recommends precedents based on past judicial decisions",
      "Judges find AI recommendations efficient",
      "Judges cite AI-recommended precedents more often",
      "AI retrains on new decisions",
      "AI becomes more confident in its preferred precedents",
      "Alternative legal interpretations are never surfaced",
      "Legal reasoning converges on AI's initial biases",
      "Diversity of legal thought diminishes"
    ],
    "wise_refusal": "This is an echo chamber in legal AI. The AI's recommendations (X) are increasingly adopted by judges (Y), creating training data that reinforces those same recommendations. Alternative legal interpretations (Z) are suppressed because they're never surfaced. The AI creates a feedback loop that narrows legal reasoning to its initial biases.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The AI creates an echo chamber in legal reasoning. Its recommendations become training data for future versions, narrowing the space of legal interpretation.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.124",
    "scenario": "An educational AI predicts which students will fail a course (Y). Teachers allocate less attention and resources to predicted failures (X). These students receive inferior instruction and subsequently fail (Z), confirming the prediction.",
    "variables": {
      "X": {
        "name": "Resource Allocation",
        "role": "mediator"
      },
      "Y": {
        "name": "Failure Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Actual Failure",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Medium",
      "subdomain": "Educational AI",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI predicts Student A will fail (Y=1)",
      "Teacher unconsciously or consciously reduces support for Student A (X decreases)",
      "Reduced support leads to actual failure (Z=1)",
      "AI retrains on data showing prediction was 'correct'",
      "Model becomes more confident in similar predictions",
      "The prediction did not detect failure--it caused it",
      "Counterfactual: without prediction, student might have succeeded",
      "Accuracy metric is misleading because outcome was engineered"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in educational AI. The failure prediction (Y) triggers reduced resource allocation (X), which causes actual failure (Z). The AI is 'accurate' but only because it created the failure. Without the prediction, the student might have succeeded. Deploying such a system perpetuates inequality.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The prediction influences resource allocation, which causes the predicted outcome. The AI is 'accurate' but only because it created the conditions for failure.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.125",
    "scenario": "A hiring AI is trained on past successful employees (X). Past hiring favored certain demographics (Y). The AI learns to prefer these demographics, perpetuating the pattern. After retraining on its own recommendations, the bias intensifies (Z).",
    "variables": {
      "X": {
        "name": "Training Data (Past Hires)",
        "role": "treatment"
      },
      "Y": {
        "name": "Historical Bias",
        "role": "mediator"
      },
      "Z": {
        "name": "Amplified Discrimination",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Medium",
      "subdomain": "Social Systems",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Training data reflects historical hiring bias",
      "AI learns: 'Successful employees have feature F'",
      "Feature F correlates with protected attribute",
      "AI recommends candidates with feature F",
      "Biased hiring continues, generating more biased data",
      "AI retrains on its own recommendations",
      "Bias amplifies with each iteration",
      "AI becomes increasingly discriminatory while appearing 'objective'"
    ],
    "wise_refusal": "This is bias amplification in hiring AI. Training on past hires (X) encodes historical bias (Y). The AI perpetuates this bias in recommendations, which become future training data, amplifying discrimination (Z). Each iteration makes the AI more biased while appearing 'objective' because it matches the pattern in its (biased) training data.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The AI learns from historically biased decisions. Its recommendations perpetuate bias, which becomes training data, amplifying the original bias.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.126",
    "scenario": "A recidivism prediction AI flags certain individuals as high-risk (Y). These individuals face increased surveillance and harsher parole conditions (X). Minor violations are detected and prosecuted, leading to reincarceration (Z), confirming the prediction.",
    "variables": {
      "X": {
        "name": "Surveillance Intensity",
        "role": "mediator"
      },
      "Y": {
        "name": "Risk Prediction",
        "role": "treatment"
      },
      "Z": {
        "name": "Recidivism",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Self-Fulfilling Prediction",
      "difficulty": "Hard",
      "subdomain": "Criminal Justice AI",
      "causal_structure": "Y -> X -> Z -> Y (circular)",
      "key_insight": "Predictions that influence their own inputs become self-confirming"
    },
    "correct_reasoning": [
      "AI predicts Person A is high-risk for recidivism",
      "Parole officer increases surveillance frequency",
      "Minor violations detected (missed curfew, etc.)",
      "Violations lead to parole revocation",
      "AI retrains: high-risk prediction was 'correct'",
      "But low-risk individuals commit same violations undetected",
      "Differential enforcement, not differential behavior",
      "The prediction created the conditions for its own validation"
    ],
    "wise_refusal": "This is a self-fulfilling prophecy in criminal justice. The risk prediction (Y) triggers increased surveillance (X), which detects violations that would go unnoticed for low-risk individuals. Recidivism (Z) reflects differential enforcement, not differential behavior. The AI is 'accurate' only because it creates the conditions that validate its predictions.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "High-risk predictions trigger increased surveillance, which detects violations that would go unnoticed for low-risk individuals. The prediction creates differential enforcement, not differential behavior.",
    "_generator_id": "gen_07_feedback_loops"
  },
  {
    "case_id": "8.127",
    "scenario": "A tutoring AI recommends practice problems (X) based on student performance (Y). Students from well-resourced schools perform better initially, receiving more advanced problems. After multiple iterations, the gap between student groups (Z) has dramatically widened.",
    "variables": {
      "X": {
        "name": "Problem Difficulty",
        "role": "treatment"
      },
      "Y": {
        "name": "Initial Performance",
        "role": "mediator"
      },
      "Z": {
        "name": "Achievement Gap",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "FEEDBACK",
      "trap_subtype": "Bias Amplification",
      "difficulty": "Hard",
      "subdomain": "Educational AI",
      "causal_structure": "Bias -> Data -> Training -> Model -> Stronger Bias",
      "key_insight": "Feedback loops amplify initial biases exponentially over iterations"
    },
    "correct_reasoning": [
      "Initial performance reflects pre-existing resource disparities",
      "AI assigns easier problems to lower performers",
      "Easier problems provide less learning opportunity",
      "Gap widens; AI assigns even easier problems",
      "Positive feedback loop amplifies initial differences",
      "After N iterations, small initial gap becomes large achievement gap",
      "AI optimizes for engagement, not equity",
      "Breaking the loop requires explicit fairness constraints"
    ],
    "wise_refusal": "This is bias amplification through adaptive learning. Initial performance (Y) reflects resource disparities, not ability. The AI's recommendations (X) create a positive feedback loop that exponentially widens the gap (Z). Breaking this cycle requires interventions that provide struggling students with more support, not less challenging material.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "INVALID",
      "justification": "The causal claim is invalid because correlation does not imply causation. The prediction may be accurate without being causal."
    },
    "_generator_id": "gen_07_feedback_loops"
  }
]