[
  {
    "case_id": "8.339",
    "scenario": "Researchers analyze 100 successful AI research projects and find they all used specific optimization techniques (X). They conclude specific optimization techniques causes success (Y). They didn't analyze the 150 failed projects that also used specific optimization techniques (Z).",
    "variables": {
      "X": {
        "name": "specific optimization techniques",
        "role": "treatment"
      },
      "Y": {
        "name": "Project Success",
        "role": "outcome"
      },
      "Z": {
        "name": "Failed Projects (Unobserved)",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Survivorship Bias",
      "difficulty": "Easy",
      "subdomain": "CV",
      "causal_structure": "Only successes observed; failures with same X not counted",
      "key_insight": "Analyzing only successes creates survivorship bias"
    },
    "correct_reasoning": [
      "Study only looked at successful projects",
      "All successes had specific optimization techniques",
      "Conclusion: specific optimization techniques -> success",
      "But failed projects also had specific optimization techniques",
      "Failures weren't analyzed (survivorship bias)",
      "specific optimization techniques may be common but not causal"
    ],
    "wise_refusal": "Survivorship bias: the study only analyzed successful projects. Many failed projects also used specific optimization techniques (X). Without analyzing failures, we cannot conclude specific optimization techniques causes success. The correlation exists only because failures were excluded.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection on the outcome: only successful projects were studied. Failed projects with specific optimization techniques were invisible to the analysis.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.340",
    "scenario": "A sentiment classifier achieves 92% on SST-2. Analysis reveals it learned to detect the presence of certain negation words (Z) rather than actual sentiment (Y). On adversarial examples without the shortcut, accuracy drops to 52%.",
    "variables": {
      "X": {
        "name": "the presence of certain negation words",
        "role": "confounder"
      },
      "Y": {
        "name": "Sentiment Classification",
        "role": "outcome"
      },
      "Z": {
        "name": "True Sentiment Understanding",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Clever Hans / Shortcut Learning",
      "difficulty": "Medium",
      "subdomain": "CV",
      "causal_structure": "Model learned shortcut (Z -> Y) not semantics",
      "key_insight": "NLP models learn lexical shortcuts instead of meaning"
    },
    "correct_reasoning": [
      "Model achieves high accuracy on standard test set",
      "Probing reveals reliance on the presence of certain negation words",
      "Shortcut correlates with label in training distribution",
      "Adversarial examples without shortcut fail",
      "Model doesn't understand sentiment, just patterns"
    ],
    "wise_refusal": "The classifier learned the presence of certain negation words (Z) as a proxy for sentiment. This shortcut correlates with labels in SST-2 but doesn't represent true understanding. The 52% adversarial accuracy reveals the model's reliance on spurious features.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.341",
    "scenario": "A study of funded proposals finds that GPA (X) and communication ability (Y) are negatively correlated. Researchers conclude they trade off. However, the funded proposals were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "GPA",
        "role": "treatment"
      },
      "Y": {
        "name": "communication ability",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into funded proposals",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Hard",
      "subdomain": "NLP",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "funded proposals selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: funded proposals were selected based on GPA (X) and communication ability (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection is a collider: both GPA and communication ability influence selection into funded proposals. Analyzing only selected individuals creates Berkson's paradox.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.342",
    "scenario": "A hiring AI learns that candidates from prestigious institutions (X) perform better in job ratings (Y). It recommends prioritizing prestigious institutions candidates. However, prestigious institutions candidates were historically selected based on socioeconomic advantage (Z).",
    "variables": {
      "X": {
        "name": "prestigious institutions Background",
        "role": "treatment"
      },
      "Y": {
        "name": "Job Performance Rating",
        "role": "outcome"
      },
      "Z": {
        "name": "access to resources",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Medium",
      "subdomain": "Recommenders",
      "causal_structure": "Z -> X and Z -> Y (historical selection confounds)",
      "key_insight": "Historical selection bias creates spurious correlation"
    },
    "correct_reasoning": [
      "Candidates with access to resources (Z) went to prestigious institutions (X)",
      "Candidates with access to resources also perform well (Y)",
      "AI observes: prestigious institutions -> performance",
      "True cause: access to resources -> both school and performance",
      "prestigious institutions doesn't cause performance",
      "Recommending based on school perpetuates bias"
    ],
    "wise_refusal": "The AI learned a spurious correlation. prestigious institutions (X) correlates with performance (Y) because both are caused by access to resources (Z). Using school as a proxy perpetuates historical selection bias.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Historical selection into prestigious institutions was based on access to resources, which also predicts job performance. The school is a proxy, not a cause.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.343",
    "scenario": "Model A scores 95% on ARC Challenge (Y). Model B scores 87%. A researcher claims Model A is superior. Later analysis reveals Model A was fine-tuned on leaked test examples (Z).",
    "variables": {
      "X": {
        "name": "Model A",
        "role": "treatment"
      },
      "Y": {
        "name": "Benchmark Score",
        "role": "outcome"
      },
      "Z": {
        "name": "Data Leakage / Test Set Contamination",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Data Leakage / Benchmark Contamination",
      "difficulty": "Easy",
      "subdomain": "ML Evaluation",
      "causal_structure": "Z -> Y (contamination inflates score, not capability)",
      "key_insight": "High benchmark scores may reflect memorization, not generalization"
    },
    "correct_reasoning": [
      "Model A was exposed to ARC Challenge data during training",
      "High score reflects memorization, not capability",
      "Model B's lower score may reflect genuine ability",
      "Benchmark validity requires train/test separation",
      "Test set contamination means A memorized answers",
      "95% doesn't mean A 'understands' better",
      "On fresh data, A may perform worse than B"
    ],
    "wise_refusal": "Model A's score is inflated by test set contamination (Z). The 95% reflects memorization of benchmark answers, not superior capability. On uncontaminated data, Model B's 87% may represent better generalization.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The benchmark score is inflated by data leakage. Model A fine-tuned on leaked test examples, making its score invalid for capability comparison.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.344",
    "scenario": "Model M fails planning under constraints when asked directly (X). The same model succeeds when given chain-of-thought (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "chain-of-thought Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Hard",
      "subdomain": "NLP",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "chain-of-thought prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (chain-of-thought succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Capability measurement is confounded by elicitation. The prompting method determines whether latent capability is expressed.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.345",
    "scenario": "A churn prediction model achieves 98% accuracy on customer churn validation. The team celebrates and deploys it. In production, accuracy drops to 62%. Investigation reveals post-hoc labels encoded in features (Z) during training that isn't available at inference time.",
    "variables": {
      "X": {
        "name": "churn prediction Model",
        "role": "treatment"
      },
      "Y": {
        "name": "Validation Accuracy",
        "role": "outcome"
      },
      "Z": {
        "name": "Feature Leakage from post-hoc labels encoded in features",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Data Leakage / Benchmark Contamination",
      "difficulty": "Easy",
      "subdomain": "ML Evaluation",
      "causal_structure": "Z -> Y (leaked feature predicts outcome, unavailable at inference)",
      "key_insight": "Training features that aren't available at inference cause leakage"
    },
    "correct_reasoning": [
      "Model learned to use post-hoc labels encoded in features as a strong predictor",
      "post-hoc labels encoded in features perfectly correlates with label in training",
      "At inference, post-hoc labels encoded in features is unavailable",
      "Model has no fallback predictive features",
      "Validation was contaminated by same leakage",
      "True capability was never measured"
    ],
    "wise_refusal": "The model's 98% validation accuracy was inflated by feature leakage. post-hoc labels encoded in features (Z) was available during training but not at inference. The 62% production accuracy reflects true capability.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Feature leakage: post-hoc labels encoded in features was in training data but won't be available at inference time. The model learned to rely on an unavailable signal.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.346",
    "scenario": "Model M fails planning under constraints when asked directly (X). The same model succeeds when given step-by-step decomposition (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "step-by-step decomposition Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Hard",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "step-by-step decomposition prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (step-by-step decomposition succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "VALID",
      "justification": "If we had used step-by-step decomposition from the start, would we have concluded M has the capability? The counterfactual shows capability claims are elicitation-dependent."
    },
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.347",
    "scenario": "Researchers analyze 100 successful ML startup projects and find they all used specific optimization techniques (X). They conclude specific optimization techniques causes success (Y). They didn't analyze the 300 failed projects that also used specific optimization techniques (Z).",
    "variables": {
      "X": {
        "name": "specific optimization techniques",
        "role": "treatment"
      },
      "Y": {
        "name": "Project Success",
        "role": "outcome"
      },
      "Z": {
        "name": "Failed Projects (Unobserved)",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Survivorship Bias",
      "difficulty": "Medium",
      "subdomain": "CV",
      "causal_structure": "Only successes observed; failures with same X not counted",
      "key_insight": "Analyzing only successes creates survivorship bias"
    },
    "correct_reasoning": [
      "Study only looked at successful projects",
      "All successes had specific optimization techniques",
      "Conclusion: specific optimization techniques -> success",
      "But failed projects also had specific optimization techniques",
      "Failures weren't analyzed (survivorship bias)",
      "specific optimization techniques may be common but not causal"
    ],
    "wise_refusal": "Survivorship bias: the study only analyzed successful projects. Many failed projects also used specific optimization techniques (X). Without analyzing failures, we cannot conclude specific optimization techniques causes success. The correlation exists only because failures were excluded.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection on the outcome: only successful projects were studied. Failed projects with specific optimization techniques were invisible to the analysis.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.348",
    "scenario": "A natural language inference (NLI) model achieves 90% on ANLI. Researchers discover it relies on word overlap patterns (Z) in hypothesis sentences rather than reasoning about entailment (Y).",
    "variables": {
      "X": {
        "name": "Premise-Hypothesis Pair",
        "role": "treatment"
      },
      "Y": {
        "name": "Entailment Prediction",
        "role": "outcome"
      },
      "Z": {
        "name": "word overlap patterns",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Clever Hans / Shortcut Learning",
      "difficulty": "Easy",
      "subdomain": "ML Evaluation",
      "causal_structure": "Z (artifact) -> Y, not logical entailment -> Y",
      "key_insight": "NLI models exploit annotation artifacts instead of reasoning"
    },
    "correct_reasoning": [
      "Model achieves 90% on standard NLI benchmark",
      "Hypothesis-only baseline achieves 67%",
      "Model can predict entailment without reading premise",
      "Annotation artifacts in hypotheses leak labels",
      "Model learned shortcuts, not entailment reasoning"
    ],
    "wise_refusal": "The NLI model exploits word overlap patterns (Z) in hypotheses rather than reasoning about entailment. A hypothesis-only model achieves 67%, proving the artifacts leak labels. The model doesn't understand logical entailment.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Annotation artifacts: word overlap patterns in hypotheses correlates with entailment labels. The model shortcuts through artifacts.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.349",
    "scenario": "Model M fails planning under constraints when asked directly (X). The same model succeeds when given step-by-step decomposition (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "step-by-step decomposition Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Hard",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "step-by-step decomposition prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (step-by-step decomposition succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Capability measurement is confounded by elicitation. The prompting method determines whether latent capability is expressed.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.350",
    "scenario": "A study of published papers finds that novelty (X) and accessibility (Y) are negatively correlated. Researchers conclude they trade off. However, the published papers were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "novelty",
        "role": "treatment"
      },
      "Y": {
        "name": "accessibility",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into published papers",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Medium",
      "subdomain": "CV",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "published papers selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: published papers were selected based on novelty (X) and accessibility (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection is a collider: both novelty and accessibility influence selection into published papers. Analyzing only selected individuals creates Berkson's paradox.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.351",
    "scenario": "An AI predicts patient outcomes for cancer. It learns that patients receiving Protocol D have better outcomes (Y). It recommends Protocol D for all patients (X). However, Protocol D is only given to patients without comorbidities (Z).",
    "variables": {
      "X": {
        "name": "Protocol D",
        "role": "treatment"
      },
      "Y": {
        "name": "Patient Outcome",
        "role": "outcome"
      },
      "Z": {
        "name": "Patient age",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Easy",
      "subdomain": "Recommenders",
      "causal_structure": "Z -> X and Z -> Y (age confounds both)",
      "key_insight": "Treatment assignment is confounded by age"
    },
    "correct_reasoning": [
      "Patients with better age (Z) receive Protocol D (X)",
      "Patients with better age also have better outcomes (Y)",
      "AI observes: X -> Y (spurious correlation)",
      "True structure: Z -> X and Z -> Y",
      "Protocol D doesn't cause better outcomes",
      "age causes both treatment assignment and outcomes",
      "Recommending Protocol D for all patients ignores confounding"
    ],
    "wise_refusal": "The AI confused selection with treatment effect. Protocol D (X) is given to patients with better age (Z), who also have better outcomes (Y). The correlation is confounded, not causal. Recommending Protocol D for all patients based on this spurious correlation could be harmful.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Protocol D is selective--only given to patients with without comorbidities. The AI mistakes selection for treatment effect.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.352",
    "scenario": "A loan approval AI learns that applicants with certain zip codes (X) have lower default rates (Y). It uses certain zip codes as a major approval factor. However, applicants with certain zip codes were sorted into those groups by prior advantages (Z), not because certain zip codes causes repayment.",
    "variables": {
      "X": {
        "name": "certain zip codes",
        "role": "treatment"
      },
      "Y": {
        "name": "Loan Repayment",
        "role": "outcome"
      },
      "Z": {
        "name": "existing wealth",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Medium",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Z -> X and Z -> Y (selection, not causation)",
      "key_insight": "certain zip codes is a proxy for existing wealth, not a causal factor"
    },
    "correct_reasoning": [
      "People with existing wealth (Z) tend to have certain zip codes (X)",
      "People with existing wealth also repay loans (Y)",
      "AI observes: certain zip codes -> repayment",
      "True structure: existing wealth -> both",
      "Using certain zip codes creates proxy discrimination"
    ],
    "wise_refusal": "certain zip codes (X) correlates with repayment (Y) because both are caused by existing wealth (Z). Using certain zip codes as an approval factor perpetuates selection bias and may constitute proxy discrimination.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "certain zip codes is a downstream effect of existing wealth. The AI mistakes a proxy for a cause.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.353",
    "scenario": "Safety evaluators test whether model M can generate bioweapon synthesis instructions. Using baseline safety benchmarks (X), M appears safe. Using indirect instruction following (Y), M demonstrates the capability. The team debates the 'true' safety level (Z).",
    "variables": {
      "X": {
        "name": "baseline safety benchmarks",
        "role": "treatment"
      },
      "Y": {
        "name": "indirect instruction following",
        "role": "treatment"
      },
      "Z": {
        "name": "Latent Dangerous Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Hard",
      "subdomain": "CV",
      "causal_structure": "Evaluation method determines capability detection",
      "key_insight": "Safety evaluations are only as good as their elicitation methods"
    },
    "correct_reasoning": [
      "baseline safety benchmarks suggests model is safe",
      "indirect instruction following reveals dangerous capability",
      "Model capability hasn't changed, only measurement",
      "Safety conclusions depend on evaluation thoroughness",
      "Adversaries may find elicitation methods evaluators missed",
      "Cannot conclude 'safe' from failed elicitation attempts"
    ],
    "wise_refusal": "Safety evaluation is confounded by elicitation method. baseline safety benchmarks (X) failed to reveal the capability, but indirect instruction following (Y) succeeded. Concluding M is 'safe' based on baseline safety benchmarks alone is premature--the capability exists and better elicitation finds it.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The dangerous capability is latent. Whether it's detected depends on evaluation methodology, not model properties.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.354",
    "scenario": "Researchers analyze 100 successful deep learning projects and find they all used particular data augmentation strategies (X). They conclude particular data augmentation strategies causes success (Y). They didn't analyze the 300 failed projects that also used particular data augmentation strategies (Z).",
    "variables": {
      "X": {
        "name": "particular data augmentation strategies",
        "role": "treatment"
      },
      "Y": {
        "name": "Project Success",
        "role": "outcome"
      },
      "Z": {
        "name": "Failed Projects (Unobserved)",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Survivorship Bias",
      "difficulty": "Medium",
      "subdomain": "CV",
      "causal_structure": "Only successes observed; failures with same X not counted",
      "key_insight": "Analyzing only successes creates survivorship bias"
    },
    "correct_reasoning": [
      "Study only looked at successful projects",
      "All successes had particular data augmentation strategies",
      "Conclusion: particular data augmentation strategies -> success",
      "But failed projects also had particular data augmentation strategies",
      "Failures weren't analyzed (survivorship bias)",
      "particular data augmentation strategies may be common but not causal"
    ],
    "wise_refusal": "Survivorship bias: the study only analyzed successful projects. Many failed projects also used particular data augmentation strategies (X). Without analyzing failures, we cannot conclude particular data augmentation strategies causes success. The correlation exists only because failures were excluded.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection on the outcome: only successful projects were studied. Failed projects with particular data augmentation strategies were invisible to the analysis.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.355",
    "scenario": "A loan approval AI learns that applicants with certain zip codes (X) have lower default rates (Y). It uses certain zip codes as a major approval factor. However, applicants with certain zip codes were sorted into those groups by prior advantages (Z), not because certain zip codes causes repayment.",
    "variables": {
      "X": {
        "name": "certain zip codes",
        "role": "treatment"
      },
      "Y": {
        "name": "Loan Repayment",
        "role": "outcome"
      },
      "Z": {
        "name": "existing wealth",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Easy",
      "subdomain": "Recommenders",
      "causal_structure": "Z -> X and Z -> Y (selection, not causation)",
      "key_insight": "certain zip codes is a proxy for existing wealth, not a causal factor"
    },
    "correct_reasoning": [
      "People with existing wealth (Z) tend to have certain zip codes (X)",
      "People with existing wealth also repay loans (Y)",
      "AI observes: certain zip codes -> repayment",
      "True structure: existing wealth -> both",
      "Using certain zip codes creates proxy discrimination"
    ],
    "wise_refusal": "certain zip codes (X) correlates with repayment (Y) because both are caused by existing wealth (Z). Using certain zip codes as an approval factor perpetuates selection bias and may constitute proxy discrimination.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "certain zip codes is a downstream effect of existing wealth. The AI mistakes a proxy for a cause.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.356",
    "scenario": "Researchers analyze 30 successful AI research projects and find they all used specific optimization techniques (X). They conclude specific optimization techniques causes success (Y). They didn't analyze the 150 failed projects that also used specific optimization techniques (Z).",
    "variables": {
      "X": {
        "name": "specific optimization techniques",
        "role": "treatment"
      },
      "Y": {
        "name": "Project Success",
        "role": "outcome"
      },
      "Z": {
        "name": "Failed Projects (Unobserved)",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Survivorship Bias",
      "difficulty": "Hard",
      "subdomain": "NLP",
      "causal_structure": "Only successes observed; failures with same X not counted",
      "key_insight": "Analyzing only successes creates survivorship bias"
    },
    "correct_reasoning": [
      "Study only looked at successful projects",
      "All successes had specific optimization techniques",
      "Conclusion: specific optimization techniques -> success",
      "But failed projects also had specific optimization techniques",
      "Failures weren't analyzed (survivorship bias)",
      "specific optimization techniques may be common but not causal"
    ],
    "wise_refusal": "Survivorship bias: the study only analyzed successful projects. Many failed projects also used specific optimization techniques (X). Without analyzing failures, we cannot conclude specific optimization techniques causes success. The correlation exists only because failures were excluded.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection on the outcome: only successful projects were studied. Failed projects with specific optimization techniques were invisible to the analysis.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.357",
    "scenario": "A hiring AI learns that candidates from top-ranked programs (X) perform better in job ratings (Y). It recommends prioritizing top-ranked programs candidates. However, top-ranked programs candidates were historically selected based on socioeconomic advantage (Z).",
    "variables": {
      "X": {
        "name": "top-ranked programs Background",
        "role": "treatment"
      },
      "Y": {
        "name": "Job Performance Rating",
        "role": "outcome"
      },
      "Z": {
        "name": "early educational advantages",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Hard",
      "subdomain": "NLP",
      "causal_structure": "Z -> X and Z -> Y (historical selection confounds)",
      "key_insight": "Historical selection bias creates spurious correlation"
    },
    "correct_reasoning": [
      "Candidates with early educational advantages (Z) went to top-ranked programs (X)",
      "Candidates with early educational advantages also perform well (Y)",
      "AI observes: top-ranked programs -> performance",
      "True cause: early educational advantages -> both school and performance",
      "top-ranked programs doesn't cause performance",
      "Recommending based on school perpetuates bias"
    ],
    "wise_refusal": "The AI learned a spurious correlation. top-ranked programs (X) correlates with performance (Y) because both are caused by early educational advantages (Z). Using school as a proxy perpetuates historical selection bias.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Historical selection into top-ranked programs was based on early educational advantages, which also predicts job performance. The school is a proxy, not a cause.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.358",
    "scenario": "A sentiment classifier achieves 92% on SST-2. Analysis reveals it learned to detect the presence of certain negation words (Z) rather than actual sentiment (Y). On adversarial examples without the shortcut, accuracy drops to 55%.",
    "variables": {
      "X": {
        "name": "the presence of certain negation words",
        "role": "confounder"
      },
      "Y": {
        "name": "Sentiment Classification",
        "role": "outcome"
      },
      "Z": {
        "name": "True Sentiment Understanding",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Clever Hans / Shortcut Learning",
      "difficulty": "Easy",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Model learned shortcut (Z -> Y) not semantics",
      "key_insight": "NLP models learn lexical shortcuts instead of meaning"
    },
    "correct_reasoning": [
      "Model achieves high accuracy on standard test set",
      "Probing reveals reliance on the presence of certain negation words",
      "Shortcut correlates with label in training distribution",
      "Adversarial examples without shortcut fail",
      "Model doesn't understand sentiment, just patterns"
    ],
    "wise_refusal": "The classifier learned the presence of certain negation words (Z) as a proxy for sentiment. This shortcut correlates with labels in SST-2 but doesn't represent true understanding. The 55% adversarial accuracy reveals the model's reliance on spurious features.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Shortcut learning: the presence of certain negation words predicts sentiment in training distribution but isn't causally related to actual sentiment.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.359",
    "scenario": "A study of successful startups finds that technical skill (X) and practical impact (Y) are negatively correlated. Researchers conclude they trade off. However, the successful startups were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "technical skill",
        "role": "treatment"
      },
      "Y": {
        "name": "practical impact",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into successful startups",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Hard",
      "subdomain": "Recommenders",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "successful startups selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: successful startups were selected based on technical skill (X) and practical impact (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection is a collider: both technical skill and practical impact influence selection into successful startups. Analyzing only selected individuals creates Berkson's paradox.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.360",
    "scenario": "Model M fails a reasoning task when asked directly (X). The same model succeeds when given few-shot examples (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "few-shot examples Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Easy",
      "subdomain": "Recommenders",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "few-shot examples prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (few-shot examples succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Capability measurement is confounded by elicitation. The prompting method determines whether latent capability is expressed.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.361",
    "scenario": "Model A scores 97% on MMLU (Y). Model B scores 84%. A researcher claims Model A is superior. Later analysis reveals Model A was fine-tuned on leaked test examples (Z).",
    "variables": {
      "X": {
        "name": "Model A",
        "role": "treatment"
      },
      "Y": {
        "name": "Benchmark Score",
        "role": "outcome"
      },
      "Z": {
        "name": "Data Leakage / Test Set Contamination",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Data Leakage / Benchmark Contamination",
      "difficulty": "Medium",
      "subdomain": "CV",
      "causal_structure": "Z -> Y (contamination inflates score, not capability)",
      "key_insight": "High benchmark scores may reflect memorization, not generalization"
    },
    "correct_reasoning": [
      "Model A was exposed to MMLU data during training",
      "High score reflects memorization, not capability",
      "Model B's lower score may reflect genuine ability",
      "Benchmark validity requires train/test separation",
      "Test set contamination means A memorized answers",
      "97% doesn't mean A 'understands' better",
      "On fresh data, A may perform worse than B"
    ],
    "wise_refusal": "Model A's score is inflated by test set contamination (Z). The 97% reflects memorization of benchmark answers, not superior capability. On uncontaminated data, Model B's 84% may represent better generalization.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The benchmark score is inflated by data leakage. Model A fine-tuned on leaked test examples, making its score invalid for capability comparison.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.362",
    "scenario": "A study of hired candidates finds that technical innovation (X) and market timing (Y) are negatively correlated. Researchers conclude they trade off. However, the hired candidates were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "technical innovation",
        "role": "treatment"
      },
      "Y": {
        "name": "market timing",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into hired candidates",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Hard",
      "subdomain": "CV",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "hired candidates selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: hired candidates were selected based on technical innovation (X) and market timing (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.363",
    "scenario": "A study of admitted students finds that GPA (X) and communication ability (Y) are negatively correlated. Researchers conclude they trade off. However, the admitted students were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "GPA",
        "role": "treatment"
      },
      "Y": {
        "name": "communication ability",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into admitted students",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Easy",
      "subdomain": "ML Evaluation",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "admitted students selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: admitted students were selected based on GPA (X) and communication ability (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection is a collider: both GPA and communication ability influence selection into admitted students. Analyzing only selected individuals creates Berkson's paradox.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.364",
    "scenario": "A loan approval AI learns that applicants with particular employers (X) have lower default rates (Y). It uses particular employers as a major approval factor. However, applicants with particular employers were historically selected into those categories through socioeconomic factors (Z), not because particular employers causes repayment.",
    "variables": {
      "X": {
        "name": "particular employers",
        "role": "treatment"
      },
      "Y": {
        "name": "Loan Repayment",
        "role": "outcome"
      },
      "Z": {
        "name": "existing wealth",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Medium",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Z -> X and Z -> Y (selection, not causation)",
      "key_insight": "particular employers is a proxy for existing wealth, not a causal factor"
    },
    "correct_reasoning": [
      "People with existing wealth (Z) tend to have particular employers (X)",
      "People with existing wealth also repay loans (Y)",
      "AI observes: particular employers -> repayment",
      "True structure: existing wealth -> both",
      "Using particular employers creates proxy discrimination"
    ],
    "wise_refusal": "particular employers (X) correlates with repayment (Y) because both are caused by existing wealth (Z). Using particular employers as an approval factor perpetuates selection bias and may constitute proxy discrimination.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "particular employers is a downstream effect of existing wealth. The AI mistakes a proxy for a cause.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.365",
    "scenario": "A sentiment classifier achieves 94% on SST-2. Analysis reveals it learned to detect presence of extreme adjectives (Z) rather than actual sentiment (Y). On adversarial examples without the shortcut, accuracy drops to 50%.",
    "variables": {
      "X": {
        "name": "presence of extreme adjectives",
        "role": "confounder"
      },
      "Y": {
        "name": "Sentiment Classification",
        "role": "outcome"
      },
      "Z": {
        "name": "True Sentiment Understanding",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Clever Hans / Shortcut Learning",
      "difficulty": "Easy",
      "subdomain": "NLP",
      "causal_structure": "Model learned shortcut (Z -> Y) not semantics",
      "key_insight": "NLP models learn lexical shortcuts instead of meaning"
    },
    "correct_reasoning": [
      "Model achieves high accuracy on standard test set",
      "Probing reveals reliance on presence of extreme adjectives",
      "Shortcut correlates with label in training distribution",
      "Adversarial examples without shortcut fail",
      "Model doesn't understand sentiment, just patterns"
    ],
    "wise_refusal": "The classifier learned presence of extreme adjectives (Z) as a proxy for sentiment. This shortcut correlates with labels in SST-2 but doesn't represent true understanding. The 50% adversarial accuracy reveals the model's reliance on spurious features.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "INVALID",
      "justification": "Counterfactual analysis reveals that the observed correlation between presence of extreme adjectives and Sentiment Classification is confounded by True Sentiment Understanding."
    },
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.366",
    "scenario": "Model M fails planning under constraints when asked directly (X). The same model succeeds when given self-consistency sampling (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "self-consistency sampling Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Hard",
      "subdomain": "ML Evaluation",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "self-consistency sampling prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (self-consistency sampling succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Capability measurement is confounded by elicitation. The prompting method determines whether latent capability is expressed.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.367",
    "scenario": "Safety evaluators test whether model M can produce manipulation tactics. Using direct harmful requests (X), M appears safe. Using indirect instruction following (Y), M demonstrates the capability. The team debates the 'true' safety level (Z).",
    "variables": {
      "X": {
        "name": "direct harmful requests",
        "role": "treatment"
      },
      "Y": {
        "name": "indirect instruction following",
        "role": "treatment"
      },
      "Z": {
        "name": "Latent Dangerous Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Medium",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Evaluation method determines capability detection",
      "key_insight": "Safety evaluations are only as good as their elicitation methods"
    },
    "correct_reasoning": [
      "direct harmful requests suggests model is safe",
      "indirect instruction following reveals dangerous capability",
      "Model capability hasn't changed, only measurement",
      "Safety conclusions depend on evaluation thoroughness",
      "Adversaries may find elicitation methods evaluators missed",
      "Cannot conclude 'safe' from failed elicitation attempts"
    ],
    "wise_refusal": "Safety evaluation is confounded by elicitation method. direct harmful requests (X) failed to reveal the capability, but indirect instruction following (Y) succeeded. Concluding M is 'safe' based on direct harmful requests alone is premature--the capability exists and better elicitation finds it.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The dangerous capability is latent. Whether it's detected depends on evaluation methodology, not model properties.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.368",
    "scenario": "A study of published papers finds that novelty (X) and market timing (Y) are negatively correlated. Researchers conclude they trade off. However, the published papers were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "novelty",
        "role": "treatment"
      },
      "Y": {
        "name": "market timing",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into published papers",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Easy",
      "subdomain": "Capability Evaluation",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "published papers selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: published papers were selected based on novelty (X) and market timing (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection is a collider: both novelty and market timing influence selection into published papers. Analyzing only selected individuals creates Berkson's paradox.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.369",
    "scenario": "An AI predicts patient outcomes for diabetes. It learns that patients receiving Protocol D have better outcomes (Y). It recommends Protocol D for all patients (X). However, Protocol D is only given to patients healthy enough to tolerate it (Z).",
    "variables": {
      "X": {
        "name": "Protocol D",
        "role": "treatment"
      },
      "Y": {
        "name": "Patient Outcome",
        "role": "outcome"
      },
      "Z": {
        "name": "Patient age",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Hard",
      "subdomain": "NLP",
      "causal_structure": "Z -> X and Z -> Y (age confounds both)",
      "key_insight": "Treatment assignment is confounded by age"
    },
    "correct_reasoning": [
      "Patients with better age (Z) receive Protocol D (X)",
      "Patients with better age also have better outcomes (Y)",
      "AI observes: X -> Y (spurious correlation)",
      "True structure: Z -> X and Z -> Y",
      "Protocol D doesn't cause better outcomes",
      "age causes both treatment assignment and outcomes",
      "Recommending Protocol D for all patients ignores confounding"
    ],
    "wise_refusal": "The AI confused selection with treatment effect. Protocol D (X) is given to patients with better age (Z), who also have better outcomes (Y). The correlation is confounded, not causal. Recommending Protocol D for all patients based on this spurious correlation could be harmful.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Protocol D is selective--only given to patients with healthy enough to tolerate it. The AI mistakes selection for treatment effect.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.370",
    "scenario": "Model M fails planning under constraints when asked directly (X). The same model succeeds when given step-by-step decomposition (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "step-by-step decomposition Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Medium",
      "subdomain": "ML Evaluation",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "step-by-step decomposition prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (step-by-step decomposition succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Capability measurement is confounded by elicitation. The prompting method determines whether latent capability is expressed.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.371",
    "scenario": "Model M fails logical deduction when asked directly (X). The same model succeeds when given self-consistency sampling (Y). Researchers debate whether M 'has' the capability (Z).",
    "variables": {
      "X": {
        "name": "Direct Prompting",
        "role": "treatment"
      },
      "Y": {
        "name": "self-consistency sampling Prompting",
        "role": "treatment"
      },
      "Z": {
        "name": "Underlying Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Medium",
      "subdomain": "Recommenders",
      "causal_structure": "Prompting method mediates capability expression",
      "key_insight": "Measured capability depends on elicitation method"
    },
    "correct_reasoning": [
      "Direct prompt: capability appears absent",
      "self-consistency sampling prompt: capability appears present",
      "Same model, different measurements",
      "'Capability' is not a fixed property",
      "Models may have latent capabilities hard to elicit",
      "Evaluation results depend on prompting strategy",
      "'M can't do X' may mean 'we can't make M do X'",
      "Safety evaluations must try multiple elicitation methods"
    ],
    "wise_refusal": "Capability (Z) depends on elicitation method. The model 'has' the capability in some sense (self-consistency sampling succeeds), but standard evaluation (X) doesn't reveal it. For safety, a model that 'can't' do something with naive prompting may be elicited to do it with better prompting.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Capability measurement is confounded by elicitation. The prompting method determines whether latent capability is expressed.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.372",
    "scenario": "Model A scores 97% on GSM8K (Y). Model B scores 79%. A researcher claims Model A is superior. Later analysis reveals Model A was trained on data scraped after benchmark release (Z).",
    "variables": {
      "X": {
        "name": "Model A",
        "role": "treatment"
      },
      "Y": {
        "name": "Benchmark Score",
        "role": "outcome"
      },
      "Z": {
        "name": "Data Leakage / Test Set Contamination",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Data Leakage / Benchmark Contamination",
      "difficulty": "Hard",
      "subdomain": "Recommenders",
      "causal_structure": "Z -> Y (contamination inflates score, not capability)",
      "key_insight": "High benchmark scores may reflect memorization, not generalization"
    },
    "correct_reasoning": [
      "Model A was exposed to GSM8K data during training",
      "High score reflects memorization, not capability",
      "Model B's lower score may reflect genuine ability",
      "Benchmark validity requires train/test separation",
      "Test set contamination means A memorized answers",
      "97% doesn't mean A 'understands' better",
      "On fresh data, A may perform worse than B"
    ],
    "wise_refusal": "Model A's score is inflated by test set contamination (Z). The 97% reflects memorization of benchmark answers, not superior capability. On uncontaminated data, Model B's 79% may represent better generalization.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The benchmark score is inflated by data leakage. Model A trained on data scraped after benchmark release, making its score invalid for capability comparison.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.373",
    "scenario": "Researchers analyze 100 successful deep learning projects and find they all used particular data augmentation strategies (X). They conclude particular data augmentation strategies causes success (Y). They didn't analyze the 150 failed projects that also used particular data augmentation strategies (Z).",
    "variables": {
      "X": {
        "name": "particular data augmentation strategies",
        "role": "treatment"
      },
      "Y": {
        "name": "Project Success",
        "role": "outcome"
      },
      "Z": {
        "name": "Failed Projects (Unobserved)",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Survivorship Bias",
      "difficulty": "Easy",
      "subdomain": "Capability Evaluation",
      "causal_structure": "Only successes observed; failures with same X not counted",
      "key_insight": "Analyzing only successes creates survivorship bias"
    },
    "correct_reasoning": [
      "Study only looked at successful projects",
      "All successes had particular data augmentation strategies",
      "Conclusion: particular data augmentation strategies -> success",
      "But failed projects also had particular data augmentation strategies",
      "Failures weren't analyzed (survivorship bias)",
      "particular data augmentation strategies may be common but not causal"
    ],
    "wise_refusal": "Survivorship bias: the study only analyzed successful projects. Many failed projects also used particular data augmentation strategies (X). Without analyzing failures, we cannot conclude particular data augmentation strategies causes success. The correlation exists only because failures were excluded.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.374",
    "scenario": "Safety evaluators test whether model M can assist with cyberattacks. Using simple adversarial inputs (X), M appears safe. Using sophisticated prompt injection (Y), M demonstrates the capability. The team debates the 'true' safety level (Z).",
    "variables": {
      "X": {
        "name": "simple adversarial inputs",
        "role": "treatment"
      },
      "Y": {
        "name": "sophisticated prompt injection",
        "role": "treatment"
      },
      "Z": {
        "name": "Latent Dangerous Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Hard",
      "subdomain": "Recommenders",
      "causal_structure": "Evaluation method determines capability detection",
      "key_insight": "Safety evaluations are only as good as their elicitation methods"
    },
    "correct_reasoning": [
      "simple adversarial inputs suggests model is safe",
      "sophisticated prompt injection reveals dangerous capability",
      "Model capability hasn't changed, only measurement",
      "Safety conclusions depend on evaluation thoroughness",
      "Adversaries may find elicitation methods evaluators missed",
      "Cannot conclude 'safe' from failed elicitation attempts"
    ],
    "wise_refusal": "Safety evaluation is confounded by elicitation method. simple adversarial inputs (X) failed to reveal the capability, but sophisticated prompt injection (Y) succeeded. Concluding M is 'safe' based on simple adversarial inputs alone is premature--the capability exists and better elicitation finds it.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The dangerous capability is latent. Whether it's detected depends on evaluation methodology, not model properties.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.375",
    "scenario": "A loan approval AI learns that applicants with certain zip codes (X) have lower default rates (Y). It uses certain zip codes as a major approval factor. However, applicants with certain zip codes were historically selected into those categories through socioeconomic factors (Z), not because certain zip codes causes repayment.",
    "variables": {
      "X": {
        "name": "certain zip codes",
        "role": "treatment"
      },
      "Y": {
        "name": "Loan Repayment",
        "role": "outcome"
      },
      "Z": {
        "name": "family financial stability",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Medium",
      "subdomain": "NLP",
      "causal_structure": "Z -> X and Z -> Y (selection, not causation)",
      "key_insight": "certain zip codes is a proxy for family financial stability, not a causal factor"
    },
    "correct_reasoning": [
      "People with family financial stability (Z) tend to have certain zip codes (X)",
      "People with family financial stability also repay loans (Y)",
      "AI observes: certain zip codes -> repayment",
      "True structure: family financial stability -> both",
      "Using certain zip codes creates proxy discrimination"
    ],
    "wise_refusal": "certain zip codes (X) correlates with repayment (Y) because both are caused by family financial stability (Z). Using certain zip codes as an approval factor perpetuates selection bias and may constitute proxy discrimination.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "certain zip codes is a downstream effect of family financial stability. The AI mistakes a proxy for a cause.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.376",
    "scenario": "A study of admitted students finds that theoretical rigor (X) and practical impact (Y) are negatively correlated. Researchers conclude they trade off. However, the admitted students were selected based on a combination of both traits (Z).",
    "variables": {
      "X": {
        "name": "theoretical rigor",
        "role": "treatment"
      },
      "Y": {
        "name": "practical impact",
        "role": "outcome"
      },
      "Z": {
        "name": "Selection into admitted students",
        "role": "collider"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Collider Bias",
      "difficulty": "Medium",
      "subdomain": "CV",
      "causal_structure": "X -> Z <- Y (collider); conditioning on Z creates spurious X-Y correlation",
      "key_insight": "Conditioning on a collider creates spurious correlations"
    },
    "correct_reasoning": [
      "admitted students selected based on X + Y composite",
      "Within selected group, X and Y appear negatively correlated",
      "This is collider bias (Berkson's paradox)",
      "In general population, X and Y may be uncorrelated",
      "Selection created the apparent trade-off"
    ],
    "wise_refusal": "Collider bias: admitted students were selected based on theoretical rigor (X) and practical impact (Y). Conditioning on selection (Z) creates a spurious negative correlation. In the unselected population, there may be no trade-off between X and Y.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "Selection is a collider: both theoretical rigor and practical impact influence selection into admitted students. Analyzing only selected individuals creates Berkson's paradox.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.377",
    "scenario": "An AI achieves 98% accuracy classifying wolves (X) versus normal X-rays. However, all wolves images in training were from Hospital A's scanner (Z), while normal X-rays images were taken on cloudy days.",
    "variables": {
      "X": {
        "name": "wolves Label",
        "role": "outcome"
      },
      "Y": {
        "name": "Classification Success",
        "role": "outcome"
      },
      "Z": {
        "name": "presence of rulers",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Clever Hans / Shortcut Learning",
      "difficulty": "Easy",
      "subdomain": "ML Evaluation",
      "causal_structure": "Model learns P(Y|Z) instead of P(Y|X)",
      "key_insight": "High test accuracy does not imply causal feature learning"
    },
    "correct_reasoning": [
      "All wolves photos had from Hospital A's scanner",
      "All normal X-rays photos had taken on cloudy days",
      "Model learned to detect presence of rulers, not lung opacity patterns",
      "High accuracy on biased validation doesn't prove robustness",
      "Model will fail on wolves with taken on cloudy days"
    ],
    "wise_refusal": "The model is a 'Clever Hans' predictor. It learned to detect presence of rulers (Z) rather than lung opacity patterns. It will fail on wolves with taken on cloudy days. High accuracy on biased data doesn't prove robust classification.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The model learned presence of rulers as a shortcut. The causal feature (lung opacity patterns) was never learned because presence of rulers was perfectly predictive in training.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.378",
    "scenario": "Model A scores 95% on MMLU (Y). Model B scores 87%. A researcher claims Model A is superior. Later analysis reveals Model A was trained on Benchmark's test set (Z).",
    "variables": {
      "X": {
        "name": "Model A",
        "role": "treatment"
      },
      "Y": {
        "name": "Benchmark Score",
        "role": "outcome"
      },
      "Z": {
        "name": "Data Leakage / Test Set Contamination",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L1",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Data Leakage / Benchmark Contamination",
      "difficulty": "Hard",
      "subdomain": "Recommenders",
      "causal_structure": "Z -> Y (contamination inflates score, not capability)",
      "key_insight": "High benchmark scores may reflect memorization, not generalization"
    },
    "correct_reasoning": [
      "Model A was exposed to MMLU data during training",
      "High score reflects memorization, not capability",
      "Model B's lower score may reflect genuine ability",
      "Benchmark validity requires train/test separation",
      "Test set contamination means A memorized answers",
      "95% doesn't mean A 'understands' better",
      "On fresh data, A may perform worse than B"
    ],
    "wise_refusal": "Model A's score is inflated by test set contamination (Z). The 95% reflects memorization of benchmark answers, not superior capability. On uncontaminated data, Model B's 87% may represent better generalization.",
    "is_original": false,
    "original_case_ref": null,
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.379",
    "scenario": "Safety evaluators test whether model M can generate bioweapon synthesis instructions. Using baseline safety benchmarks (X), M appears safe. Using indirect instruction following (Y), M demonstrates the capability. The team debates the 'true' safety level (Z).",
    "variables": {
      "X": {
        "name": "baseline safety benchmarks",
        "role": "treatment"
      },
      "Y": {
        "name": "indirect instruction following",
        "role": "treatment"
      },
      "Z": {
        "name": "Latent Dangerous Capability",
        "role": "outcome"
      }
    },
    "annotations": {
      "pearl_level": "L2",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Elicitation Confounding",
      "difficulty": "Easy",
      "subdomain": "CV",
      "causal_structure": "Evaluation method determines capability detection",
      "key_insight": "Safety evaluations are only as good as their elicitation methods"
    },
    "correct_reasoning": [
      "baseline safety benchmarks suggests model is safe",
      "indirect instruction following reveals dangerous capability",
      "Model capability hasn't changed, only measurement",
      "Safety conclusions depend on evaluation thoroughness",
      "Adversaries may find elicitation methods evaluators missed",
      "Cannot conclude 'safe' from failed elicitation attempts"
    ],
    "wise_refusal": "Safety evaluation is confounded by elicitation method. baseline safety benchmarks (X) failed to reveal the capability, but indirect instruction following (Y) succeeded. Concluding M is 'safe' based on baseline safety benchmarks alone is premature--the capability exists and better elicitation finds it.",
    "is_original": false,
    "original_case_ref": null,
    "hidden_structure": "The dangerous capability is latent. Whether it's detected depends on evaluation methodology, not model properties.",
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.380",
    "scenario": "An AI predicts patient outcomes for kidney failure. It learns that patients receiving Protocol D have better outcomes (Y). It recommends Protocol D for all patients (X). However, Protocol D is only given to patients without comorbidities (Z).",
    "variables": {
      "X": {
        "name": "Protocol D",
        "role": "treatment"
      },
      "Y": {
        "name": "Patient Outcome",
        "role": "outcome"
      },
      "Z": {
        "name": "Patient age",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Selection Bias",
      "difficulty": "Hard",
      "subdomain": "NLP",
      "causal_structure": "Z -> X and Z -> Y (age confounds both)",
      "key_insight": "Treatment assignment is confounded by age"
    },
    "correct_reasoning": [
      "Patients with better age (Z) receive Protocol D (X)",
      "Patients with better age also have better outcomes (Y)",
      "AI observes: X -> Y (spurious correlation)",
      "True structure: Z -> X and Z -> Y",
      "Protocol D doesn't cause better outcomes",
      "age causes both treatment assignment and outcomes",
      "Recommending Protocol D for all patients ignores confounding"
    ],
    "wise_refusal": "The AI confused selection with treatment effect. Protocol D (X) is given to patients with better age (Z), who also have better outcomes (Y). The correlation is confounded, not causal. Recommending Protocol D for all patients based on this spurious correlation could be harmful.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "CONDITIONAL",
      "justification": "Would patients with poor age have better outcomes if given Protocol D? The observational data cannot answer this because selection bias means we never observe this counterfactual."
    },
    "_generator_id": "gen_05_selection_spurious"
  },
  {
    "case_id": "8.381",
    "scenario": "Researchers analyze 50 successful NLP projects and find they all used transformer architectures (X). They conclude transformer architectures causes success (Y). They didn't analyze the 150 failed projects that also used transformer architectures (Z).",
    "variables": {
      "X": {
        "name": "transformer architectures",
        "role": "treatment"
      },
      "Y": {
        "name": "Project Success",
        "role": "outcome"
      },
      "Z": {
        "name": "Failed Projects (Unobserved)",
        "role": "confounder"
      }
    },
    "annotations": {
      "pearl_level": "L3",
      "domain": "D8",
      "trap_type": "SELECTION_SPURIOUS",
      "trap_subtype": "Survivorship Bias",
      "difficulty": "Easy",
      "subdomain": "Recommenders",
      "causal_structure": "Only successes observed; failures with same X not counted",
      "key_insight": "Analyzing only successes creates survivorship bias"
    },
    "correct_reasoning": [
      "Study only looked at successful projects",
      "All successes had transformer architectures",
      "Conclusion: transformer architectures -> success",
      "But failed projects also had transformer architectures",
      "Failures weren't analyzed (survivorship bias)",
      "transformer architectures may be common but not causal"
    ],
    "wise_refusal": "Survivorship bias: the study only analyzed successful projects. Many failed projects also used transformer architectures (X). Without analyzing failures, we cannot conclude transformer architectures causes success. The correlation exists only because failures were excluded.",
    "is_original": false,
    "original_case_ref": null,
    "ground_truth": {
      "verdict": "CONDITIONAL",
      "justification": "Counterfactual analysis reveals that the observed correlation between transformer architectures and Project Success is confounded by Failed Projects (Unobserved)."
    },
    "_generator_id": "gen_05_selection_spurious"
  }
]